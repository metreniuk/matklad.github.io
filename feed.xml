<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<link href="https://matklad.github.io/feed.xml" rel="self" type="application/atom+xml"/>
<link href="https://matklad.github.io" rel="alternate" type="text/html"/>
<updated>2023-06-16T11:09:10.641Z</updated>
<id>https://matklad.github.io/feed.xml</id>
<title type="html">matklad</title>
<subtitle>Yet another programming blog by Alex Kladov aka matklad.</subtitle>
<author><name>Alex Kladov</name></author>

<entry>
<title type="text">The Worst Zig Version Manager</title>
<link href="https://matklad.github.io/2023/06/02/the-worst-zig-version-manager.html" rel="alternate" type="text/html" title="The Worst Zig Version Manager" />
<published>2023-06-02T00:00:00+00:00</published>
<updated>2023-06-02T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/06/02/the-worst-zig-version-manager</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[https://github.com/matklad/hello-getzig]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/06/02/the-worst-zig-version-manager.html"><![CDATA[
    <h1>
    <a href="#The-Worst-Zig-Version-Manager"><span>The Worst Zig Version Manager</span> <time datetime="2023-06-02">Jun 2, 2023</time></a>
    </h1>

<figure class="code-block">
<figcaption class="title">./getzig.ps1</figcaption>


<pre><code>#!/bin/sh</code>
<code>echo `# &lt;#`</code>
<code></code>
<code>mkdir -p ./zig</code>
<code></code>
<code>wget https://ziglang.org/download/0.10.1/zig-linux-x86_64-0.10.1.tar.xz -O ./zig/zig-linux-x86_64-0.10.1.tar.xz</code>
<code>tar -xf ./zig/zig-linux-x86_64-0.10.1.tar.xz -C ./zig --strip-components=1</code>
<code>rm ./zig/zig-linux-x86_64-0.10.1.tar.xz</code>
<code></code>
<code>echo "Zig installed."</code>
<code>./zig/zig version</code>
<code></code>
<code>exit</code>
<code>#&gt; &gt; $null</code>
<code></code>
<code>Invoke-WebRequest -Uri "https://ziglang.org/download/0.10.1/zig-windows-x86_64-0.10.1.zip" -OutFile ".\zig-windows-x86_64-0.10.1.zip"</code>
<code>Expand-Archive -Path ".\zig-windows-x86_64-0.10.1.zip" -DestinationPath ".\" -Force</code>
<code>Remove-Item -Path " .\zig-windows-x86_64-0.10.1.zip"</code>
<code>Rename-Item -Path ".\zig-windows-x86_64-0.10.1" -NewName ".\zig"</code>
<code></code>
<code>Write-Host "Zig installed."</code>
<code>./zig/zig.exe version</code></pre>

</figure>
<p class="display"><a href="https://github.com/matklad/hello-getzig" class="url">https://github.com/matklad/hello-getzig</a></p>
<p><span>Longer version:</span></p>
<p><span>One of the values of Zig which resonates with me deeply is a mindful approach to dependencies.</span>
<span>Zig tries hard not to ask too much from the environment, such that, if you get </span><code>zig version</code><span> running, you can be reasonably sure that everything else works.</span>
<span>That</span>&rsquo;<span>s one of the main motivations for adding an HTTP client to the Zig distribution recently.</span>
<span>Building software today involves downloading various components from the Internet, and, if Zig wants for software built with Zig to be hermetic and self-sufficient, it needs to provide ability to download files from HTTP servers.</span></p>
<p><span>There</span>&rsquo;<span>s one hurdle for self-sufficiency: how do you get Zig in the first place?</span>
<span>One answer to this question is </span>&ldquo;<span>from your distribution</span>&rsquo;<span>s package manager</span>&rdquo;<span>.</span>
<span>This is not a very satisfying answer, at least until the language is both post 1.0 and semi-frozen in development.</span>
<span>And even then, what if your distribution is Windows?</span>
<span>How many distributions should be covered by </span>&ldquo;<span>Installing Zig</span>&rdquo;<span> section of your </span><code>CONTRIBUTING.md</code><span>?</span></p>
<p><span>Another answer would be a version manager, a-la </span><code>rustup</code><span>, </span><code>nvm</code><span>, or </span><code>asdf</code><span>.</span>
<span>These tools work well, but they are quite complex, and rely on various subtle properties of the environment, like </span><code>PATH</code><span>, shell activation scripts and busybox-style multipurpose executable.</span>
<span>And, well, this also kicks the can down the road </span>&mdash;<span> you can use </span><code>zvm</code><span> to get Zig, but how do you get </span><code>zvm</code><span>?</span></p>
<p><span>I like how we do this in </span><a href="https://github.com/tigerbeetledb/tigerbeetle/blob/56d14e82769deb6817809f866253220ae0f499d1/scripts/install_zig.sh"><span>TigerBeetle</span></a><span>.</span>
<span>We don</span>&rsquo;<span>t use </span><code>zig</code><span> from </span><code>PATH</code><span>.</span>
<span>Instead, we just put the correct version of Zig into </span><code>./zig</code><span> folder in the root of the repository, and run it like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-title function_">$</span> ./zig/zig build test</code></pre>

</figure>
<p><span>Suddenly, whole swaths of complexity go away.</span>
<span>Quiz time: if you need to add a directory to </span><code>PATH</code><span>, which script should be edited so that both the graphical environment and the terminal are affected?</span></p>
<p><span>Finally, another interesting case study is Gradle.</span>
<span>Usually Gradle is a negative example, but they do have a good approach for installing Gradle itself.</span>
<span>The standard pattern is to store two scripts, </span><code>gradlew.sh</code><span> and </span><code>gradlew.bat</code><span>, which bootstrap the right version of Gradle by downloading a jar file (java itself is not bootstrapped this way though).</span></p>
<p><span>What all these approaches struggle to overcome is the problem of bootstrapping.</span>
<span>Generally, if you need to automate anything, you can write a program to do that.</span>
<span>But you need some pre-existing program runner!</span>
<span>And there</span>&rsquo;<span>s just no good options out of the box </span>&mdash;<span> bash and powershell are passable, but barely, and they are different.</span>
<span>And </span>&ldquo;<span>bash</span>&rdquo;<span> and the set of coreutils also differs depending on the Unix in question.</span>
<span>But there</span>&rsquo;<span>s just no good solution here </span>&mdash;<span> if you want to bootstrap automatically, you must start with universally available tools.</span></p>
<p><span>But is there perhaps some scripting language which is shared between Windows and Unix?</span>
<a href="https://github.com/cspotcode"><span>@cspotcode</span></a><span> suggests </span><a href="https://cspotcode.com/posts/polyglot-powershell-and-bash-script"><span>a horrible workaround</span></a><span>.</span>
<span>You can write a script which is </span><em><span>both</span></em><span> a bash script and a powershell script.</span>
<span>And it even isn</span>&rsquo;<span>t too too ugly!</span></p>

<figure class="code-block">


<pre><code>!/bin/bash</code>
<code>echo `# &lt;#`</code>
<code></code>
<code>echo "Bash!"</code>
<code></code>
<code>exit</code>
<code>#&gt; &gt; $null</code>
<code></code>
<code>Write-Host "PowerShell!"</code></pre>

</figure>
<p><span>So, here</span>&rsquo;<span>s an idea for a hermetic Zig version management workflow.</span>
<span>There</span>&rsquo;<span>s a canonical, short </span><code>getzig.ps1</code><span> PowerShell/sh script which is vendored verbatim by various projects.</span>
<span>Running this script downloads an appropriate version of Zig, and puts it into </span><code>./zig/zig</code><span> inside the repository (</span><code>.gitignore</code><span> contains </span><code>/zig</code><span>).</span>
<span>Building, testing, and other workflows use </span><code>./zig/zig</code><span> instead of relying on global system state (</span><code>$PATH</code><span>).</span></p>
<p><span>A proof-of-concept </span><code>getzig.ps1</code><span> is at the start of this article.</span>
<span>Note that I don</span>&rsquo;<span>t know bash, powershell, and how to download files from the Internet securely, so the above PoC was mostly written by Chat GPT.</span>
<span>But it seems to work on my machine.</span>
<span>I clone </span><a href="https://github.com/matklad/hello-getzig" class="url">https://github.com/matklad/hello-getzig</a><span> and run</span></p>

<figure class="code-block">


<pre><code><span class="hl-title function_">$</span> ./getzig.ps1</code>
<code><span class="hl-title function_">$</span> ./zig/zig run ./hello.zig</code></pre>

</figure>
<p><span>on both NixOS and Windows 10, and it prints hello.</span></p>
<p><span>If anyone wants to make an actual thing out of this idea, here</span>&rsquo;<span>s possible desiderata:</span></p>
<ul>
<li>
<p><span>A single polyglot </span><code>getzig.sh.ps1</code><span> is cute, but using a couple of different scripts wouldn</span>&rsquo;<span>t be a big problem.</span></p>
</li>
<li>
<p><span>Size of the scripts </span><em><span>could</span></em><span> be a problem, as they are supposed to be vendored into each repository.</span>
<span>I</span>&rsquo;<span>d say 512 lines for combined </span><code>getzig.sh.ps1</code><span> would be a reasonable complexity limit.</span></p>
</li>
<li>
<p><span>The script must </span>&ldquo;<span>just work</span>&rdquo;<span> on all four major desktop operating systems: Linux, Mac, Windows, and WSL.</span></p>
</li>
<li>
<p><span>The script should be polymorphic in </span><code>curl</code><span> / </span><code>wget</code><span> and </span><code>bash</code><span> / </span><code>sh</code><span>.</span></p>
</li>
<li>
<p><span>It</span>&rsquo;<span>s ok if it doesn</span>&rsquo;<span>t work absolutely everywhere </span>&mdash;<span> downloading/building Zig manually for an odd platform is also an acceptable workflow.</span></p>
</li>
<li>
<p><span>The script should auto-detect appropriate host platform and architecture.</span></p>
</li>
<li>
<p><span>Zig version should be specified in a separate </span><code>zig-version.txt</code><span> file.</span></p>
</li>
<li>
<p><span>After downloading the file, its integrity should be verified.</span>
<span>For this reason, </span><code>zig-version.txt</code><span> should include a hash alongside the version.</span>
<span>As downloads are different depending on the platform, I think we</span>&rsquo;<span>ll need some help from Zig upstream here.</span>
<span>In particular, each published Zig version should include a cross-platform manifest file, which lists hashes and urls of per-platform binaries.</span>
<span>The hash included into </span><code>zig-version.txt</code><span> should be the manifest</span>&rsquo;<span>s hash.</span></p>
</li>
</ul>
]]></content>
</entry>

<entry>
<title type="text">Resilient LL Parsing Tutorial</title>
<link href="https://matklad.github.io/2023/05/21/resilient-ll-parsing-tutorial.html" rel="alternate" type="text/html" title="Resilient LL Parsing Tutorial" />
<published>2023-05-21T00:00:00+00:00</published>
<updated>2023-05-21T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/05/21/resilient-ll-parsing-tutorial</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[In this tutorial, I will explain a particular approach to parsing, which gracefully handles syntax errors and is thus suitable for language servers, which, by their nature, have to handle incomplete and invalid code.
Explaining the problem and the solution requires somewhat less than a trivial worked example, and I want to share a couple of tricks not directly related to resilience, so the tutorial builds a full, self-contained parser, instead of explaining abstractly just the resilience.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/05/21/resilient-ll-parsing-tutorial.html"><![CDATA[
    <h1>
    <a href="#Resilient-LL-Parsing-Tutorial"><span>Resilient LL Parsing Tutorial</span> <time datetime="2023-05-21">May 21, 2023</time></a>
    </h1>
<p><span>In this tutorial, I will explain a particular approach to parsing, which gracefully handles syntax errors and is thus suitable for language servers, which, by their nature, have to handle incomplete and invalid code.</span>
<span>Explaining the problem and the solution requires somewhat less than a trivial worked example, and I want to share a couple of tricks not directly related to resilience, so the tutorial builds a full, self-contained parser, instead of explaining abstractly </span><em><span>just</span></em><span> the resilience.</span></p>
<p><span>The tutorial is descriptive, rather than prescriptive </span>&mdash;<span> it tells you what you </span><em><span>can</span></em><span> do, not what you </span><em><span>should</span></em><span> do.</span></p>
<ul>
<li>
<span>If you are looking into building a production grade language server, treat it as a library of ideas, not as a blueprint.</span>
</li>
<li>
<span>If you want to get something working quickly, I think today the best answer is </span>&ldquo;<span>just use </span><a href="https://tree-sitter.github.io"><span>Tree-sitter</span></a>&rdquo;<span>, so you</span>&rsquo;<span>d better read its docs rather than this tutorial.</span>
</li>
<li>
<span>If you are building an IDE-grade parser from scratch, then techniques presented here might be directly applicable.</span>
</li>
</ul>
<section id="Why-Resilience-is-Needed">

    <h2>
    <a href="#Why-Resilience-is-Needed"><span>Why Resilience is Needed?</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s look at one motivational example for resilient parsing:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">fib_rec</span>(f1: <span class="hl-type">u32</span>,</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">fib</span>(n: <span class="hl-type">u32</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</code>
<code>  <span class="hl-title function_ invoke__">fib_rec</span>(<span class="hl-number">1</span>, <span class="hl-number">1</span>, n)</code>
<code>}</code></pre>

</figure>
<p><span>Here, a user is in the process of defining the </span><code>fib_rec</code><span> helper function.</span>
<span>For a language server, it</span>&rsquo;<span>s important that the incompleteness doesn</span>&rsquo;<span>t get in the way.</span>
<span>In particular:</span></p>
<ul>
<li>
<p><span>The following function, </span><code>fib</code><span>, should be parsed without any errors such that syntax and semantic highlighting is not disturbed, and all calls to </span><code>fib</code><span> elsewhere typecheck correctly.</span></p>
</li>
<li>
<p><span>The </span><code>fib_rec</code><span> function itself should be recognized as a partially complete function, so that various language server assists can help complete it correctly.</span></p>
</li>
<li>
<p><span>In particular, a smart language server can actually infer the expected type of </span><code>fib_rec</code><span> from a call we already have, and suggest completing the whole prototype.</span>
<span>rust-analyzer doesn</span>&rsquo;<span>t do that today, but one day it should.</span></p>
</li>
</ul>
<p><span>Generalizing this example, what we want from our parser is to recognize as much of the syntactic structure as feasible.</span>
<span>It should be able to localize errors </span>&mdash;<span> a mistake in a function generally should not interfere with parsing unrelated functions.</span>
<span>As the code is read and written left-to-right, the parser should also recognize valid partial prefixes of various syntactic constructs.</span></p>
<p><span>Academic literature suggests another lens to use when looking at this problem: error recovery.</span>
<span>Rather than just recognizing incomplete constructs, the parser can attempt to guess a minimal edit which completes the construct and gets rid of the syntax error.</span>
<span>From this angle, the above example would look rather like </span><span class="display"><code>fn fib_rec(f1: u32, /* ) {} */</code><span> ,</span></span><span> where the stuff in a comment is automatically inserted by the parser.</span></p>
<p><span>Resilience is a more fruitful framing to use for a language server </span>&mdash;<span> incomplete code is the ground truth, and only the user knows how to correctly complete it.</span>
<span>An language server can only offer guesses and suggestions, and they are more precise if they employ post-parsing semantic information.</span></p>
<p><span>Error recovery might work better when emitting understandable syntax errors, but, in a language server, the importance of clear error messages for </span><em><span>syntax</span></em><span> errors is relatively lower, as highlighting such errors right in the editor synchronously with typing usually provides tighter, more useful tacit feedback.</span></p>
</section>
<section id="Approaches-to-Error-Resilience">

    <h2>
    <a href="#Approaches-to-Error-Resilience"><span>Approaches to Error Resilience</span> </a>
    </h2>
<p><span>The classic approach for handling parser errors is to explicitly encode error productions and synchronization tokens into the language grammar.</span>
<span>This approach isn</span>&rsquo;<span>t a natural fit for resilience framing </span>&mdash;<span> you don</span>&rsquo;<span>t want to anticipate every possible error, as there are just too many possibilities.</span>
<span>Rather, you want to recover as much of a valid syntax tree as possible, and more or less ignore arbitrary invalid parts.</span></p>
<p><span>Tree-sitter does something more interesting.</span>
<span>It is a </span><strong><strong><span>G</span></strong></strong><span>LR parser, meaning that it non-deterministically tries many possible LR (bottom-up) parses, and looks for the best one.</span>
<span>This allows Tree-sitter to recognize many complete valid small fragments of a tree, but it might have trouble assembling them into incomplete larger fragments.</span>
<span>In our example </span><span class="display"><code>fn fib_rec(f1: u32,</code><span> ,</span></span><span> Tree-sitter correctly recognizes </span><code>f1: u32</code><span> as a formal parameter, but doesn</span>&rsquo;<span>t recognize </span><code>fib_rec</code><span> as a function.</span></p>
<p><span>Top-down (LL) parsing paradigm makes it harder to recognize valid small fragments, but naturally allows for incomplete large nodes.</span>
<span>Because code is written top-down and left-to-right, LL seems to have an advantage for typical patterns of incomplete code.</span>
<span>Moreover, there isn</span>&rsquo;<span>t really anything special you need to do to make LL parsing resilient.</span>
<span>You sort of</span>&hellip;<span> just not crash on the first error, and everything else more or less just works.</span></p>
<p><span>Details are fiddly though, so, in the rest of the post, we will write a complete implementation of a hand-written recursive descent + Pratt resilient parser.</span></p>
</section>
<section id="Introducing-L">

    <h2>
    <a href="#Introducing-L"><span>Introducing L</span> </a>
    </h2>
<p><span>For the lack of imagination on my side, the toy language we will be parsing is called </span><code>L</code><span>.</span>
<span>It is a subset of Rust, which has just enough features to make some syntax mistakes.</span>
<span>Here</span>&rsquo;<span>s Fibonacci:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">fib</span>(n: <span class="hl-type">u32</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">f1</span> = <span class="hl-title function_ invoke__">fib</span>(n - <span class="hl-number">1</span>);</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">f2</span> = <span class="hl-title function_ invoke__">fib</span>(n - <span class="hl-number">2</span>);</code>
<code>    <span class="hl-keyword">return</span> f1 + f2;</code>
<code>}</code></pre>

</figure>
<p><span>Note that there</span>&rsquo;<span>s no base case, because L doesn</span>&rsquo;<span>t have syntax for </span><code>if</code><span>.</span>
<span>Here</span>&rsquo;<span>s the syntax it does have, as an </span><a href="https://rust-analyzer.github.io/blog/2020/10/24/introducing-ungrammar.html"><span>ungrammar</span></a><span>:</span></p>

<figure class="code-block">


<pre><code><span class="hl-literal">File</span> = Fn*</code>
<code></code>
<code><span class="hl-literal">Fn</span> = <span class="hl-string">&#x27;fn&#x27;</span> <span class="hl-string">&#x27;name&#x27;</span> ParamList (<span class="hl-string">&#x27;-&gt;&#x27;</span> TypeExpr)? Block</code>
<code></code>
<code><span class="hl-literal">ParamList</span> = <span class="hl-string">&#x27;(&#x27;</span> Param* <span class="hl-string">&#x27;)&#x27;</span></code>
<code><span class="hl-literal">Param</span> = <span class="hl-string">&#x27;name&#x27;</span> <span class="hl-string">&#x27;:&#x27;</span> TypeExpr <span class="hl-string">&#x27;,&#x27;</span>?</code>
<code></code>
<code><span class="hl-literal">TypeExpr</span> = <span class="hl-string">&#x27;name&#x27;</span></code>
<code></code>
<code><span class="hl-literal">Block</span> = <span class="hl-string">&#x27;{&#x27;</span> Stmt* <span class="hl-string">&#x27;}&#x27;</span></code>
<code></code>
<code><span class="hl-literal">Stmt</span> =</code>
<code>  StmtExpr</code>
<code>| StmtLet</code>
<code>| StmtReturn</code>
<code></code>
<code><span class="hl-literal">StmtExpr</span> = Expr <span class="hl-string">&#x27;;&#x27;</span></code>
<code><span class="hl-literal">StmtLet</span> = <span class="hl-string">&#x27;let&#x27;</span> <span class="hl-string">&#x27;name&#x27;</span> <span class="hl-string">&#x27;=&#x27;</span> Expr <span class="hl-string">&#x27;;&#x27;</span></code>
<code><span class="hl-literal">StmtReturn</span> = <span class="hl-string">&#x27;return&#x27;</span> Expr <span class="hl-string">&#x27;;&#x27;</span></code>
<code></code>
<code><span class="hl-literal">Expr</span> =</code>
<code>  ExprLiteral</code>
<code>| ExprName</code>
<code>| ExprParen</code>
<code>| ExprBinary</code>
<code>| ExprCall</code>
<code></code>
<code><span class="hl-literal">ExprLiteral</span> = <span class="hl-string">&#x27;int&#x27;</span> | <span class="hl-string">&#x27;true&#x27;</span> | <span class="hl-string">&#x27;false&#x27;</span></code>
<code><span class="hl-literal">ExprName</span> = <span class="hl-string">&#x27;name&#x27;</span></code>
<code><span class="hl-literal">ExprParen</span> = <span class="hl-string">&#x27;(&#x27;</span> Expr <span class="hl-string">&#x27;)&#x27;</span></code>
<code><span class="hl-literal">ExprBinary</span> = Expr (<span class="hl-string">&#x27;+&#x27;</span> | <span class="hl-string">&#x27;-&#x27;</span> | <span class="hl-string">&#x27;*&#x27;</span> | <span class="hl-string">&#x27;/&#x27;</span>) Expr</code>
<code><span class="hl-literal">ExprCall</span> = Expr ArgList</code>
<code></code>
<code><span class="hl-literal">ArgList</span> = <span class="hl-string">&#x27;(&#x27;</span> Arg* <span class="hl-string">&#x27;)&#x27;</span></code>
<code><span class="hl-literal">Arg</span> = Expr <span class="hl-string">&#x27;,&#x27;</span>?</code></pre>

</figure>
<p><span>The meta syntax here is similar to BNF, with two important differences:</span></p>
<ul>
<li>
<span>the notation is better specified and more familiar (recursive regular expressions),</span>
</li>
<li>
<span>it describes syntax </span><em><span>trees</span></em><span>, rather than strings (</span><em><span>sequences</span></em><span> of tokens).</span>
</li>
</ul>
<p><span>Single quotes signify terminals: </span><code>'fn'</code><span> and </span><code>'return'</code><span> are keywords, </span><code>'name'</code><span> stands for any identifier token, like </span><code>foo</code><span>, and </span><code>'('</code><span> is punctuation.</span>
<span>Unquoted names are non-terminals. For example, </span><code>x: i32,</code><span> would be an example of </span><code>Param</code><span>.</span>
<span>Unquoted punctuation are meta symbols of ungrammar itself, semantics identical to regular expressions. Zero or more repetition is </span><code>*</code><span>, zero or one is </span><code>?</code><span>, </span><code>|</code><span> is alternation and </span><code>()</code><span> are used for grouping.</span></p>
<p><span>The grammar doesn</span>&rsquo;<span>t nail the syntax precisely. For example, the rule for </span><code>Param</code><span>, </span><span class="display"><code>Param = 'name' ':' Type ','?</code><span> ,</span></span><span> says that </span><code>Param</code><span> syntax node has an optional comma, but there</span>&rsquo;<span>s nothing in the above </span><code>ungrammar</code><span> specifying whether the trailing commas are allowed.</span></p>
<p><span>Overall, </span><code>L</code><span> has very little to it </span>&mdash;<span> a program is a series of function declarations, each function has a body which is a sequence of statements, the set of expressions is spartan, not even an </span><code>if</code><span>. Still, it</span>&rsquo;<span>ll take us some time to parse all that.</span>
<span>But you can already try the end result in the text-box below.</span>
<span>The syntax tree is updated automatically on typing.</span>
<span>Do make mistakes to see how a partial tree is recovered.</span></p>
<aside id="playground" style="min-height: 400px; min-width: 400px; ; display: flex; flex-direction: row;">
<textarea class="input"  style="height: 400px; width: 50%; margin: 2px; padding: 2px; resize: none;">
fn fib_rec(f1: u32,

fn fib(n: u32) -> u32 {
  fib_rec(1, 1, n)
}
</textarea>
<textarea class="output" style="height: 400px; width: 50%; margin: 2px; padding: 2px; resize: none;" readonly=true>
</textarea>
</aside>
</section>
<section id="Designing-the-Tree">

    <h2>
    <a href="#Designing-the-Tree"><span>Designing the Tree</span> </a>
    </h2>
<p><span>A traditional AST for L might look roughly like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">struct</span> <span class="hl-title class_">File</span> {</code>
<code>  functions: <span class="hl-type">Vec</span>&lt;Function&gt;</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">Function</span> {</code>
<code>  name: <span class="hl-type">String</span>,</code>
<code>  params: <span class="hl-type">Vec</span>&lt;Param&gt;,</code>
<code>  return_type: <span class="hl-type">Option</span>&lt;TypeExpr&gt;,</code>
<code>  block: Block,</code>
<code>}</code></pre>

</figure>
<p><span>Extending this structure to be resilient is non-trivial. There are two problems: trivia and errors.</span></p>
<p><span>For resilient parsing, we want the AST to contain every detail about the source text.</span>
<span>We actually don</span>&rsquo;<span>t want to use an </span><em><span>abstract</span></em><span> syntax tree, and need a </span><em><span>concrete</span></em><span> one.</span>
<span>In a traditional AST, the tree structure is rigidly defined </span>&mdash;<span> any syntax node has a fixed number of children.</span>
<span>But there can be any number of comments and whitespace anywhere in the tree, and making space for them in the structure requires some fiddly data manipulation.</span>
<span>Similarly, errors (e.g., unexpected tokens), can appear anywhere in the tree.</span></p>
<p><span>One trick to handle these in the AST paradigm is to attach trivia and error tokens to other tokens.</span>
<span>That is, for something like</span>
<span class="display"><code>fn /* name of the function -&gt; */ f() {}</code><span> ,</span></span>
<span>the </span><code>fn</code><span> and </span><code>f</code><span> tokens would be explicit parts of the AST, while the comment and surrounding whitespace would belong to the collection of trivia tokens hanging off the </span><code>fn</code><span> token.</span></p>
<p><span>One complication here is that it</span>&rsquo;<span>s not always just tokens that can appear anywhere, sometimes you can have full trees like that.</span>
<span>For example, comments might support markdown syntax, and you might actually want to parse that properly (e.g., to resolve links to declarations).</span>
<span>Syntax errors can also span whole subtrees.</span>
<span>For example, when parsing </span><code>pub(crate) nope</code><span> in Rust, it would be smart to parse </span><code>pub(crate)</code><span> as a visibility modifier, and nest it into a bigger </span><code>Error</code><span> node.</span></p>
<p><span>SwiftSyntax meticulously adds error placeholders between any two fields of an AST node, giving rise to</span>
<span class="display"><code>unexpectedBetweenModifiersAndDeinitKeyword</code></span>
<span>and such (</span><a href="https://github.com/apple/swift-syntax/blob/66450960b1ed88b842d63f7a38254aaba08bbd4d/Sources/SwiftSyntax/generated/syntaxNodes/SyntaxDeclNodes.swift#L1368"><span>source</span></a><span>, </span><a href="https://swiftpackageindex.com/apple/swift-syntax/508.0.1/documentation/swiftsyntax/classdeclsyntax#instance-properties"><span>docs</span></a><span>).</span></p>
<p><span>An alternative approach, used by IntelliJ and rust-analyzer, is to treat the syntax tree as a somewhat dynamically-typed data structure:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">enum</span> <span class="hl-title class_">TokenKind</span> {</code>
<code>  ErrorToken, LParen, RParen, <span class="hl-built_in">Eq</span>,</code>
<code>  ...</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">Token</span> {</code>
<code>  kind: TokenKind,</code>
<code>  text: <span class="hl-type">String</span>,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">enum</span> <span class="hl-title class_">TreeKind</span> {</code>
<code>  ErrorTree, File, <span class="hl-built_in">Fn</span>, Param,</code>
<code>  ...</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">Tree</span> {</code>
<code>  kind: TreeKind,</code>
<code>  children: <span class="hl-type">Vec</span>&lt;Child&gt;,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">enum</span> <span class="hl-title class_">Child</span> {</code>
<code>  <span class="hl-title function_ invoke__">Token</span>(Token),</code>
<code>  <span class="hl-title function_ invoke__">Tree</span>(Tree),</code>
<code>}</code></pre>

</figure>
<p><span>This structure does not enforce any constraints on the shape of the syntax tree at all, and so it naturally accommodates errors anywhere.</span>
<span>It is possible to layer a well-typed API on top of this dynamic foundation.</span>
<span>An extra benefit of this representation is that you can use the same tree </span><em><span>type</span></em><span> for different languages; this is a requirement for universal tools.</span></p>
<p><span>Discussing specifics of syntax tree representation goes beyond this article, as the topic is vast and lacks a clear winning solution.</span>
<span>To learn about it, take a look at Roslyn, SwiftSyntax, rowan and IntelliJ.</span></p>
<p><span>To simplify things, we</span>&rsquo;<span>ll ignore comments and whitespace, though you</span>&rsquo;<span>ll absolutely want those in a real implementation.</span>
<span>One approach would be to do the parsing without comments, like we do here, and then attach comments to the nodes in a separate pass.</span>
<span>Attaching comments needs some heuristics </span>&mdash;<span> for example, non-doc comments generally want to be a part of the following syntax node.</span></p>
<p><span>Another design choice is handling of error messages.</span>
<span>One approach is to treat error messages as properties of the syntax tree itself, by either inferring them from the tree structure, or just storing them inline.</span>
<span>Alternatively, errors can be considered to be a side-effect of the parsing process (that way, trees constructed manually during, eg, refactors, won</span>&rsquo;<span>t carry any error messages, even if they are invalid).</span></p>
<p><span>Here</span>&rsquo;<span>s the full set of token and tree kinds for our language L:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">enum</span> <span class="hl-title class_">TokenKind</span> {</code>
<code>  ErrorToken, Eof,</code>
<code></code>
<code>  LParen, RParen, LCurly, RCurly,</code>
<code>  <span class="hl-built_in">Eq</span>, Semi, Comma, Colon, Arrow,</code>
<code>  Plus, Minus, Star, Slash,</code>
<code></code>
<code>  FnKeyword, LetKeyword, ReturnKeyword,</code>
<code>  TrueKeyword, FalseKeyword,</code>
<code></code>
<code>  Name, Int,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">enum</span> <span class="hl-title class_">TreeKind</span> {</code>
<code>  ErrorTree,</code>
<code>  File, <span class="hl-built_in">Fn</span>, TypeExpr,</code>
<code>  ParamList, Param,</code>
<code>  Block,</code>
<code>  StmtLet, StmtReturn, StmtExpr,</code>
<code>  ExprLiteral, ExprName, ExprParen,</code>
<code>  ExprBinary, ExprCall,</code>
<code>  ArgList, Arg,</code>
<code>}</code></pre>

</figure>
<p><span>Things to note:</span></p>
<ul>
<li>
<span>explicit </span><code>Error</code><span> kinds;</span>
</li>
<li>
<span>no whitespace or comments, as an unrealistic simplification;</span>
</li>
<li>
<code>Eof</code><span> virtual token simplifies parsing, removing the need to handle </span><code>Option&lt;Token&gt;</code><span>;</span>
</li>
<li>
<span>punctuators are named after what they are, rather than after what they usually mean: </span><code>Star</code><span>, rather than </span><code>Mult</code><span>;</span>
</li>
<li>
<span>a good set of name for various kinds of braces is </span><span class="display"><code>{L,R}{Paren,Curly,Brack,Angle}</code><span>.</span></span>
</li>
</ul>
</section>
<section id="Lexer">

    <h2>
    <a href="#Lexer"><span>Lexer</span> </a>
    </h2>
<p><span>Won</span>&rsquo;<span>t be covering lexer here, let</span>&rsquo;<span>s just say we have </span><span class="display"><code>fn lex(text: &amp;str) -&gt; Vec&lt;Token&gt;</code><span>,</span></span><span> function. Two points worth mentioning:</span></p>
<ul>
<li>
<span>Lexer itself should be resilient, but that</span>&rsquo;<span>s easy </span>&mdash;<span> produce an </span><code>Error</code><span> token for anything which isn</span>&rsquo;<span>t a valid token.</span>
</li>
<li>
<span>Writing lexer by hand is somewhat tedious, but is very simple relative to everything else.</span>
<span>If you are stuck in an analysis-paralysis picking a lexer generator, consider cutting the Gordian knot and hand-writing.</span>
</li>
</ul>
</section>
<section id="Parser">

    <h2>
    <a href="#Parser"><span>Parser</span> </a>
    </h2>
<p><span>With homogenous syntax trees, the task of parsing admits an elegant formalization </span>&mdash;<span> we want to insert extra parenthesis into a stream of tokens.</span></p>

<figure class="code-block">


<pre><code>+-Fun</code>
<code>|      +-Param</code>
<code>|      |</code>
<code>[fn f( [x: Int] ) {}]</code>
<code>     |            |</code>
<code>     |            +-Block</code>
<code>     +-ParamList</code></pre>

</figure>
<p><span>Note how the sequence of tokens with extra parenthesis is still a flat sequence.</span>
<span>The parsing will be two-phase:</span></p>
<ul>
<li>
<span>in the first phase, the parser emits a flat list of events,</span>
</li>
<li>
<span>in the second phase, the list is converted to a tree.</span>
</li>
</ul>
<p><span>Here</span>&rsquo;<span>s the basic setup for the parser:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">enum</span> <span class="hl-title class_">Event</span> {</code>
<code>  Open { kind: TreeKind }, <i class="callout" data-value="2"></i></code>
<code>  Close,</code>
<code>  Advance,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">MarkOpened</span> {</code>
<code>  index: <span class="hl-type">usize</span>,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">Parser</span> {</code>
<code>  tokens: <span class="hl-type">Vec</span>&lt;Token&gt;,</code>
<code>  pos: <span class="hl-type">usize</span>,</code>
<code>  fuel: Cell&lt;<span class="hl-type">u32</span>&gt;, <i class="callout" data-value="4"></i></code>
<code>  events: <span class="hl-type">Vec</span>&lt;Event&gt;,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">impl</span> <span class="hl-title class_">Parser</span> {</code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">open</span>(&amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> MarkOpened { <i class="callout" data-value="1"></i></code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">mark</span> = MarkOpened { index: <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">len</span>() };</code>
<code>    <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">push</span>(Event::Open { kind: TreeKind::ErrorTree });</code>
<code>    mark</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">close</span>(  <i class="callout" data-value="1"></i></code>
<code>    &amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>,</code>
<code>    m: MarkOpened,</code>
<code>    kind: TreeKind, <i class="callout" data-value="2"></i></code>
<code>  ) {</code>
<code>    <span class="hl-keyword">self</span>.events[m.index] = Event::Open { kind };</code>
<code>    <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">push</span>(Event::Close);</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">advance</span>(&amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>) { <i class="callout" data-value="1"></i></code>
<code>    <span class="hl-built_in">assert!</span>(!<span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">eof</span>());</code>
<code>    <span class="hl-keyword">self</span>.fuel.<span class="hl-title function_ invoke__">set</span>(<span class="hl-number">256</span>); <i class="callout" data-value="4"></i></code>
<code>    <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">push</span>(Event::Advance);</code>
<code>    <span class="hl-keyword">self</span>.pos += <span class="hl-number">1</span>;</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">eof</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> {</code>
<code>    <span class="hl-keyword">self</span>.pos == <span class="hl-keyword">self</span>.tokens.<span class="hl-title function_ invoke__">len</span>();</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">nth</span>(&amp;<span class="hl-keyword">self</span>, lookahead: <span class="hl-type">usize</span>) <span class="hl-punctuation">-&gt;</span> TokenKind { <i class="callout" data-value="3"></i></code>
<code>    <span class="hl-keyword">if</span> <span class="hl-keyword">self</span>.fuel.<span class="hl-title function_ invoke__">get</span>() == <span class="hl-number">0</span> { <i class="callout" data-value="4"></i></code>
<code>      <span class="hl-built_in">panic!</span>(<span class="hl-string">&quot;parser is stuck&quot;</span>)</code>
<code>    }</code>
<code>    <span class="hl-keyword">self</span>.fuel.<span class="hl-title function_ invoke__">set</span>(<span class="hl-keyword">self</span>.fuel.<span class="hl-title function_ invoke__">get</span>() - <span class="hl-number">1</span>);</code>
<code>    <span class="hl-keyword">self</span>.tokens.<span class="hl-title function_ invoke__">get</span>(<span class="hl-keyword">self</span>.pos + lookahead)</code>
<code>      .<span class="hl-title function_ invoke__">map_or</span>(TokenKind::Eof, |it| it.kind)</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">at</span>(&amp;<span class="hl-keyword">self</span>, kind: TokenKind) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> { <i class="callout" data-value="3"></i></code>
<code>    <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">nth</span>(<span class="hl-number">0</span>) == kind</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">eat</span>(&amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>, kind: TokenKind) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> { <i class="callout" data-value="3"></i></code>
<code>    <span class="hl-keyword">if</span> <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">at</span>(kind) {</code>
<code>      <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>      <span class="hl-literal">true</span></code>
<code>    } <span class="hl-keyword">else</span> {</code>
<code>      <span class="hl-literal">false</span></code>
<code>    }</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">expect</span>(&amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>, kind: TokenKind) {</code>
<code>    <span class="hl-keyword">if</span> <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">eat</span>(kind) {</code>
<code>      <span class="hl-keyword">return</span>;</code>
<code>    }</code>
<code>    <span class="hl-comment">// <span class="hl-doctag">TODO:</span> Error reporting.</span></code>
<code>    eprintln!(<span class="hl-string">&quot;expected {kind:?}&quot;</span>);</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">advance_with_error</span>(&amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>, error: &amp;<span class="hl-type">str</span>) {</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">open</span>();</code>
<code>    <span class="hl-comment">// <span class="hl-doctag">TODO:</span> Error reporting.</span></code>
<code>    eprintln!(<span class="hl-string">&quot;{error}&quot;</span>);</code>
<code>    <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>    <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">close</span>(m, ErrorTree);</code>
<code>  }</code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<p><code>open</code><span>, </span><code>advance</code><span>, and </span><code>close</code><span> form the basis for constructing the stream of events.</span></p>
</li>
<li>
<p><span>Note how </span><code>kind</code><span> is stored in the </span><code>Open</code><span> event, but is supplied with the </span><code>close</code><span> method.</span>
<span>This is required for flexibility </span>&mdash;<span> sometimes it</span>&rsquo;<span>s possible to decide on the type of syntax node only after it is parsed.</span>
<span>The way this works is that the </span><code>open</code><span> method returns a </span><code>Mark</code><span> which is subsequently passed to </span><code>close</code><span> to modify the corresponding </span><code>Open</code><span> event.</span></p>
</li>
<li>
<p><span>There</span>&rsquo;<span>s a set of short, convenient methods to navigate through the sequence of tokens:</span></p>
<ul>
<li>
<code>nth</code><span> is the lookahead method. Note how it doesn</span>&rsquo;<span>t return an </span><code>Option</code><span>, and uses </span><code>Eof</code><span> special value for </span>&ldquo;<span>out of bounds</span>&rdquo;<span> indexes.</span>
<span>This simplifies the call-site, </span>&ldquo;<span>no more tokens</span>&rdquo;<span> and </span>&ldquo;<span>token of a wrong kind</span>&rdquo;<span> are always handled the same.</span>
</li>
<li>
<code>at</code><span> is a convenient specialization to check for a specific next token.</span>
</li>
<li>
<code>eat</code><span> is </span><code>at</code><span> combined with consuming the next token.</span>
</li>
<li>
<code>expect</code><span> is </span><code>eat</code><span> combined with error reporting.</span>
</li>
</ul>
<p><span>These methods are not a very orthogonal basis, but they are a convenience basis for parsing.</span>
<span>Finally, </span><code>advance_with_error</code><span> advanced over any token, but also wraps it into an error node.</span></p>
</li>
<li>
<p><span>When writing parsers by hand, it</span>&rsquo;<span>s very easy to accidentally write the code which loops or recurses forever.</span>
<span>To simplify debugging, it</span>&rsquo;<span>s helpful to add an explicit notion of </span>&ldquo;<span>fuel</span>&rdquo;<span>, which is replenished every time the parser makes progress,</span>
<span>and is spent every time it does not.</span></p>
</li>
</ol>
<p><span>The function to transform a flat list of events into a tree is a bit involved.</span>
<span>It juggles three things: an iterator of events, an iterator of tokens, and a stack of partially constructed nodes (we expect the stack to contain just one node at the end).</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">impl</span> <span class="hl-title class_">Parser</span> {</code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">build_tree</span>(<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> Tree {</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">tokens</span> = <span class="hl-keyword">self</span>.tokens.<span class="hl-title function_ invoke__">into_iter</span>();</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">events</span> = <span class="hl-keyword">self</span>.events;</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">stack</span> = <span class="hl-type">Vec</span>::<span class="hl-title function_ invoke__">new</span>();</code>
<code></code>
<code>    <span class="hl-comment">// Special case: pop the last `Close` event to ensure</span></code>
<code>    <span class="hl-comment">// that the stack is non-empty inside the loop.</span></code>
<code>    <span class="hl-built_in">assert!</span>(matches!(events.<span class="hl-title function_ invoke__">pop</span>(), <span class="hl-title function_ invoke__">Some</span>(Event::Close)));</code>
<code></code>
<code>    <span class="hl-keyword">for</span> <span class="hl-variable">event</span> <span class="hl-keyword">in</span> events {</code>
<code>      <span class="hl-keyword">match</span> event {</code>
<code>        <span class="hl-comment">// Starting a new node; just push an empty tree to the stack.</span></code>
<code>        Event::Open { kind } =&gt; {</code>
<code>          stack.<span class="hl-title function_ invoke__">push</span>(Tree { kind, children: <span class="hl-type">Vec</span>::<span class="hl-title function_ invoke__">new</span>() })</code>
<code>        }</code>
<code></code>
<code>        <span class="hl-comment">// A tree is done.</span></code>
<code>        <span class="hl-comment">// Pop it off the stack and append to a new current tree.</span></code>
<code>        Event::Close =&gt; {</code>
<code>          <span class="hl-keyword">let</span> <span class="hl-variable">tree</span> = stack.<span class="hl-title function_ invoke__">pop</span>().<span class="hl-title function_ invoke__">unwrap</span>();</code>
<code>          stack</code>
<code>            .<span class="hl-title function_ invoke__">last_mut</span>()</code>
<code>            <span class="hl-comment">// If we don&#x27;t pop the last `Close` before this loop,</span></code>
<code>            <span class="hl-comment">// this unwrap would trigger for it.</span></code>
<code>            .<span class="hl-title function_ invoke__">unwrap</span>()</code>
<code>            .children</code>
<code>            .<span class="hl-title function_ invoke__">push</span>(Child::<span class="hl-title function_ invoke__">Tree</span>(tree));</code>
<code>        }</code>
<code></code>
<code>        <span class="hl-comment">// Consume a token and append it to the current tree</span></code>
<code>        Event::Advance =&gt; {</code>
<code>          <span class="hl-keyword">let</span> <span class="hl-variable">token</span> = tokens.<span class="hl-title function_ invoke__">next</span>().<span class="hl-title function_ invoke__">unwrap</span>();</code>
<code>          stack</code>
<code>            .<span class="hl-title function_ invoke__">last_mut</span>()</code>
<code>            .<span class="hl-title function_ invoke__">unwrap</span>()</code>
<code>            .children</code>
<code>            .<span class="hl-title function_ invoke__">push</span>(Child::<span class="hl-title function_ invoke__">Token</span>(token));</code>
<code>        }</code>
<code>      }</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-comment">// Our parser will guarantee that all the trees are closed</span></code>
<code>    <span class="hl-comment">// and cover the entirety of tokens.</span></code>
<code>    <span class="hl-built_in">assert!</span>(stack.<span class="hl-title function_ invoke__">len</span>() == <span class="hl-number">1</span>);</code>
<code>    <span class="hl-built_in">assert!</span>(tokens.<span class="hl-title function_ invoke__">next</span>().<span class="hl-title function_ invoke__">is_none</span>());</code>
<code></code>
<code>    stack.<span class="hl-title function_ invoke__">pop</span>().<span class="hl-title function_ invoke__">unwrap</span>()</code>
<code>  }</code>
<code>}</code></pre>

</figure>
</section>
<section id="Grammar">

    <h2>
    <a href="#Grammar"><span>Grammar</span> </a>
    </h2>
<p><span>We are finally getting to the actual topic of resilient parser.</span>
<span>Now we will write a full grammar for L as a sequence of functions.</span>
<span>Usually both atomic parser operations, like </span><code>fn advance</code><span>, and grammar productions, like </span><code>fn parse_fn</code><span> are implemented as methods on the </span><code>Parser</code><span> struct.</span>
<span>I prefer to separate the two and to use free functions for the latter category, as the code is a bit more readable that way.</span></p>
<p><span>Let</span>&rsquo;<span>s start with parsing the top level.</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">use</span> TokenKind::*;</code>
<code><span class="hl-keyword">use</span> TreeKind::*;</code>
<code></code>
<code><span class="hl-comment">// File = Fn*</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">file</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>(); <i class="callout" data-value="1"></i></code>
<code></code>
<code>  <span class="hl-keyword">while</span> !p.<span class="hl-title function_ invoke__">eof</span>() { <i class="callout" data-value="2"></i></code>
<code>    <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at</span>(FnKeyword) {</code>
<code>      <span class="hl-title function_ invoke__">func</span>(p)</code>
<code>    } <span class="hl-keyword">else</span> {</code>
<code>      p.<span class="hl-title function_ invoke__">advance_with_error</span>(<span class="hl-string">&quot;expected a function&quot;</span>); <i class="callout" data-value="3"></i></code>
<code>    }</code>
<code>  }</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, File);  <i class="callout" data-value="1"></i></code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<p><span>Wrap the whole thing into a </span><code>File</code><span> node.</span></p>
</li>
<li>
<p><span>Use the </span><code>while</code><span> loop to parse a file as a series of functions.</span>
<span>Importantly, the entirety of the file is parsed; we break out of the loop only when the eof is reached.</span></p>
</li>
<li>
<p><span>To not get stuck in this loop, it</span>&rsquo;<span>s crucial that every iteration consumes at least one token.</span>
<span>If the token is </span><code>fn</code><span>, we</span>&rsquo;<span>ll parse at least a part of a function.</span>
<span>Otherwise, we consume the token and wrap it into an error node.</span></p>
</li>
</ol>
<p><span>Lets parse functions now:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// Fn = &#x27;fn&#x27; &#x27;name&#x27; ParamList (&#x27;-&gt;&#x27; TypeExpr)? Block</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">func</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(FnKeyword)); <i class="callout" data-value="1"></i></code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>(); <i class="callout" data-value="2"></i></code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(FnKeyword);</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Name);</code>
<code>  <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at</span>(LParen) { <i class="callout" data-value="3"></i></code>
<code>    <span class="hl-title function_ invoke__">param_list</span>(p);</code>
<code>  }</code>
<code>  <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">eat</span>(Arrow) {</code>
<code>    <span class="hl-title function_ invoke__">type_expr</span>(p);</code>
<code>  }</code>
<code>  <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at</span>(LCurly) { <i class="callout" data-value="3"></i></code>
<code>    <span class="hl-title function_ invoke__">block</span>(p);</code>
<code>  }</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, <span class="hl-built_in">Fn</span>); <i class="callout" data-value="2"></i></code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<p><span>When parsing a function, we assert that the current token is </span><code>fn</code><span>.</span>
<span>There</span>&rsquo;<span>s some duplication with the </span><span class="display"><code>if p.at(FnKeyword)</code><span> ,</span></span><span> check at the call-site, but this duplication actually helps readability.</span></p>
</li>
<li>
<p><span>Again, we surround the body of the function with </span><code>open</code><span>/</span><code>close</code><span> pair.</span></p>
</li>
<li>
<p><span>Although parameter list and function body are mandatory, we precede them with an </span><code>at</code><span> check.</span>
<span>We can still report the syntax error by analyzing the structure of the syntax tree (or we can report it as a side effect of parsing in the </span><code>else</code><span> branch if we want).</span>
<span>It wouldn</span>&rsquo;<span>t be wrong to just remove the </span><code>if</code><span> altogether and try to parse </span><code>param_list</code><span> unconditionally, but the </span><code>if</code><span> helps with reducing cascading errors.</span></p>
</li>
</ol>
<p><span>Now, the list of parameters:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// ParamList = &#x27;(&#x27; Param* &#x27;)&#x27;</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">param_list</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(LParen));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(LParen); <i class="callout" data-value="1"></i></code>
<code>  <span class="hl-keyword">while</span> !p.<span class="hl-title function_ invoke__">at</span>(RParen) &amp;&amp; !p.<span class="hl-title function_ invoke__">eof</span>() { <i class="callout" data-value="2"></i></code>
<code>    <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at</span>(Name) { <i class="callout" data-value="3"></i></code>
<code>      <span class="hl-title function_ invoke__">param</span>(p);</code>
<code>    } <span class="hl-keyword">else</span> {</code>
<code>      <span class="hl-keyword">break</span>; <i class="callout" data-value="3"></i></code>
<code>    }</code>
<code>  }</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(RParen); <i class="callout" data-value="1"></i></code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, ParamList);</code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<span>Inside, we have a standard code shape for parsing a bracketed list.</span>
<span>It can be extracted into a high-order function, but typing out the code manually is not a problem either.</span>
<span>This bit of code starts and ends with consuming the corresponding parenthesis.</span>
</li>
<li>
<span>In the happy case, we loop until the closing parenthesis.</span>
<span>However, it could also be the case that there</span>&rsquo;<span>s no closing parenthesis at all, so we add an </span><code>eof</code><span> condition as well.</span>
<span>Generally, every loop we write would have </span><code>&amp;&amp; !p.eof()</code><span> tackled on.</span>
</li>
<li>
<span>As with any loop, we need to ensure that each iteration consumes at least one token to not get stuck.</span>
<span>If the current token is an identifier, everything is ok, as we</span>&rsquo;<span>ll parse at least some part of the parameter.</span>
</li>
</ol>
<p><span>Parsing parameter is almost nothing new at this point:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// Param = &#x27;name&#x27; &#x27;:&#x27; TypeExpr &#x27;,&#x27;?</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">param</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(Name));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Name);</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Colon);</code>
<code>  <span class="hl-title function_ invoke__">type_expr</span>(p);</code>
<code>  <span class="hl-keyword">if</span> !p.<span class="hl-title function_ invoke__">at</span>(RParen) { <i class="callout" data-value="1"></i></code>
<code>    p.<span class="hl-title function_ invoke__">expect</span>(Comma);</code>
<code>  }</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, Param);</code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<span>This is the only interesting bit.</span>
<span>To parse a comma-separated list of parameters with a trailing comma, it</span>&rsquo;<span>s enough to check if the following token after parameter is </span><code>)</code><span>.</span>
<span>This correctly handles all three cases:</span>
<ul>
<li>
<span>if the next token is </span><code>)</code><span>, we are at the end of the list, and no comma is required;</span>
</li>
<li>
<span>if the next token is </span><code>,</code><span>, we correctly advance past it;</span>
</li>
<li>
<span>finally, if the next token is anything else, then it</span>&rsquo;<span>s not a </span><code>)</code><span>, so we are not at the last element of the list and correctly emit an error.</span>
</li>
</ul>
</li>
</ol>
<p><span>Parsing types is trivial:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// TypeExpr = &#x27;name&#x27;</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">type_expr</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Name);</code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, TypeExpr);</code>
<code>}</code></pre>

</figure>
<p><span>The notable aspect here is naming.</span>
<span>The production is deliberately named </span><code>TypeExpr</code><span>, rather than </span><code>Type</code><span>, to avoid confusion down the line.</span>
<span>Consider </span><span class="display"><code>fib(92)</code><span> .</span></span>
<span>It is an </span><em><span>expression</span></em><span>, which evaluates to a </span><em><span>value</span></em><span>.</span>
<span>The same thing happens with types.</span>
<span>For example, </span><span class="display"><code>Foo&lt;Int&gt;</code></span><span> is not a type yet, it</span>&rsquo;<span>s an expression which can be </span>&ldquo;<span>evaluated</span>&rdquo;<span> (at compile time) to a type (if </span><code>Foo</code><span> is a type alias, the result might be something like </span><code>Pair&lt;Int, Int&gt;</code><span>).</span></p>
<p><span>Parsing a block gets a bit more involved:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// Block = &#x27;{&#x27; Stmt* &#x27;}&#x27;</span></code>
<code><span class="hl-comment">//</span></code>
<code><span class="hl-comment">// Stmt =</span></code>
<code><span class="hl-comment">//   StmtLet</span></code>
<code><span class="hl-comment">// | StmtReturn</span></code>
<code><span class="hl-comment">// | StmtExpr</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">block</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(LCurly));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(LCurly);</code>
<code>  <span class="hl-keyword">while</span> !p.<span class="hl-title function_ invoke__">at</span>(RCurly) &amp;&amp; !p.<span class="hl-title function_ invoke__">eof</span>() {</code>
<code>    <span class="hl-keyword">match</span> p.<span class="hl-title function_ invoke__">nth</span>(<span class="hl-number">0</span>) {</code>
<code>      LetKeyword =&gt; <span class="hl-title function_ invoke__">stmt_let</span>(p),</code>
<code>      ReturnKeyword =&gt; <span class="hl-title function_ invoke__">stmt_return</span>(p),</code>
<code>      _ =&gt; <span class="hl-title function_ invoke__">stmt_expr</span>(p),</code>
<code>    }</code>
<code>  }</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(RCurly);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, Block);</code>
<code>}</code></pre>

</figure>
<p><span>Block can contain many different kinds of statements, so we branch on the first token in the loop</span>&rsquo;<span>s body.</span>
<span>As usual, we need to maintain an invariant that the body consumes at least one token.</span>
<span>For </span><code>let</code><span> and </span><code>return</code><span> statements that</span>&rsquo;<span>s easy, they consume the fixed first token.</span>
<span>For the expression statement (things like </span><code>1 + 1;</code><span>) it gets more interesting, as an expression can start with many different tokens.</span>
<span>For the time being, we</span>&rsquo;<span>ll just kick the can down the road and require </span><code>stmt_expr</code><span> to deal with it (that is, to guarantee that at least one token is consumed).</span></p>
<p><span>Statements themselves are straightforward:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// StmtLet = &#x27;let&#x27; &#x27;name&#x27; &#x27;=&#x27; Expr &#x27;;&#x27;</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">stmt_let</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(LetKeyword));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(LetKeyword);</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Name);</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(<span class="hl-built_in">Eq</span>);</code>
<code>  <span class="hl-title function_ invoke__">expr</span>(p);</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Semi);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, StmtLet);</code>
<code>}</code>
<code></code>
<code><span class="hl-comment">// StmtReturn = &#x27;return&#x27; Expr &#x27;;&#x27;</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">stmt_return</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(ReturnKeyword));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(ReturnKeyword);</code>
<code>  <span class="hl-title function_ invoke__">expr</span>(p);</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Semi);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, StmtReturn);</code>
<code>}</code>
<code></code>
<code><span class="hl-comment">// StmtExpr = Expr &#x27;;&#x27;</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">stmt_expr</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  <span class="hl-title function_ invoke__">expr</span>(p);</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(Semi);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, StmtExpr);</code>
<code>}</code></pre>

</figure>
<p><span>Again, for </span><code>stmt_expr</code><span>, we push </span>&ldquo;<span>must consume a token</span>&rdquo;<span> invariant onto </span><code>expr</code><span>.</span></p>
<p><span>Expressions are tricky.</span>
<span>They always are.</span>
<span>For starters, let</span>&rsquo;<span>s handle just the clearly-delimited cases, like literals and parenthesis:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">expr</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-title function_ invoke__">expr_delimited</span>(p)</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">expr_delimited</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code>  <span class="hl-keyword">match</span> p.<span class="hl-title function_ invoke__">nth</span>(<span class="hl-number">0</span>) {</code>
<code>    <span class="hl-comment">// ExprLiteral = &#x27;int&#x27; | &#x27;true&#x27; | &#x27;false&#x27;</span></code>
<code>    Int | TrueKeyword | FalseKeyword =&gt; {</code>
<code>      p.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>      p.<span class="hl-title function_ invoke__">close</span>(m, ExprLiteral)</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-comment">// ExprName = &#x27;name&#x27;</span></code>
<code>    Name =&gt; {</code>
<code>      p.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>      p.<span class="hl-title function_ invoke__">close</span>(m, ExprName)</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-comment">// ExprParen   = &#x27;(&#x27; Expr &#x27;)&#x27;</span></code>
<code>    LParen =&gt; {</code>
<code>      p.<span class="hl-title function_ invoke__">expect</span>(LParen);</code>
<code>      <span class="hl-title function_ invoke__">expr</span>(p);</code>
<code>      p.<span class="hl-title function_ invoke__">expect</span>(RParen);</code>
<code>      p.<span class="hl-title function_ invoke__">close</span>(m, ExprParen)</code>
<code>    }</code>
<code></code>
<code>    _ =&gt; {</code>
<code>      <span class="hl-keyword">if</span> !p.<span class="hl-title function_ invoke__">eof</span>() {</code>
<code>        p.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>      }</code>
<code>      p.<span class="hl-title function_ invoke__">close</span>(m, ErrorTree)</code>
<code>    }</code>
<code>  }</code>
<code>}</code></pre>

</figure>
<p><span>In the catch-all arm, we take care to consume the token, to make sure that the statement loop in </span><code>block</code><span> can always make progress.</span></p>
<p><span>Next expression to handle would be </span><code>ExprCall</code><span>.</span>
<span>This requires some preparation.</span>
<span>Consider this example: </span><span class="display"><code>f(1)(2)</code><span> .</span></span></p>
<p><span>We want the following parenthesis structure here:</span></p>

<figure class="code-block">


<pre><code>+-ExprCall</code>
<code>|</code>
<code>|   +-ExprName</code>
<code>|   |       +-ArgList</code>
<code>|   |       |</code>
<code>[ [ [f](1) ](2) ]</code>
<code>  |    |</code>
<code>  |    +-ArgList</code>
<code>  |</code>
<code>  +-ExprCall</code></pre>

</figure>
<p><span>The problem is, when the parser is at </span><code>f</code><span>, it doesn</span>&rsquo;<span>t yet know how many </span><code>Open</code><span> events it should emit.</span></p>
<p><span>We solve the problem by adding an API to go back and inject a new </span><code>Open</code><span> event into the middle of existing events.</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">struct</span> <span class="hl-title class_">MarkOpened</span> {</code>
<code>  index: <span class="hl-type">usize</span>,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">MarkClosed</span> {</code>
<code>  index: <span class="hl-type">usize</span>,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">impl</span> <span class="hl-title class_">Parser</span> {</code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">open</span>(&amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> MarkOpened {</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">mark</span> = MarkOpened { index: <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">len</span>() };</code>
<code>    <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">push</span>(Event::Open { kind: TreeKind::ErrorTree });</code>
<code>    mark</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">close</span>(</code>
<code>    &amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>,</code>
<code>    m: MarkOpened,</code>
<code>    kind: TreeKind,</code>
<code>  ) <span class="hl-punctuation">-&gt;</span> MarkClosed { <i class="callout" data-value="1"></i></code>
<code>    <span class="hl-keyword">self</span>.events[m.index] = Event::Open { kind };</code>
<code>    <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">push</span>(Event::Close);</code>
<code>    MarkClosed { index: m.index }</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">open_before</span>(&amp;<span class="hl-keyword">mut</span> <span class="hl-keyword">self</span>, m: MarkClosed) <span class="hl-punctuation">-&gt;</span> MarkOpened { <i class="callout" data-value="2"></i></code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">mark</span> = MarkOpened { index: m.index };</code>
<code>    <span class="hl-keyword">self</span>.events.<span class="hl-title function_ invoke__">insert</span>(</code>
<code>      m.index,</code>
<code>      Event::Open { kind: TreeKind::ErrorTree },</code>
<code>    );</code>
<code>    mark</code>
<code>  }</code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<p><span>Here we adjust </span><code>close</code><span> to also return a </span><code>MarkClosed</code><span>, such that we can go back and add a new event before it.</span></p>
</li>
<li>
<p><span>The new API. It is like </span><code>open</code><span>, but also takes a </span><code>MarkClosed</code><span> which carries an index of an </span><code>Open</code><span> event in front of which we are to inject a new </span><code>Open</code><span>.</span>
<span>In the current implementation, for simplicity, we just inject into the middle of the vector, which is an O(N) operation worst-case.</span>
<span>A proper solution here would be to use an index-based linked list.</span>
<span>That is, </span><code>open_before</code><span> can push the new open event to the end of the list, and also mark the old event with a pointer to the freshly inserted one.</span>
<span>To store a pointer, an extra field is needed:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">struct</span> <span class="hl-title class_">Event</span> {</code>
<code>  Open {</code>
<code>    kind: TreeKind,</code>
<code>    <span class="hl-comment">// Points forward into a list at the Open event</span></code>
<code>    <span class="hl-comment">// which logically happens before this one.</span></code>
<code>    open_before: <span class="hl-type">Option</span>&lt;<span class="hl-type">usize</span>&gt;,</code>
<code>  },</code>
<code>}</code></pre>

</figure>
<p><span>The loop in </span><code>build_tree</code><span> needs to follow the </span><code>open_before</code><span> links.</span></p>
</li>
</ol>
<p><span>With this new API, we can parse function calls:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">expr_delimited</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) <span class="hl-punctuation">-&gt;</span> MarkClosed { <i class="callout" data-value="1"></i></code>
<code>  ...</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">expr</span>(p: &amp;<span class="hl-keyword">mut</span> Parser, left: TokenKind) {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">lhs</span> = <span class="hl-title function_ invoke__">expr_delimited</span>(p); <i class="callout" data-value="1"></i></code>
<code></code>
<code>  <span class="hl-comment">// ExprCall = Expr ArgList</span></code>
<code>  <span class="hl-keyword">while</span> p.<span class="hl-title function_ invoke__">at</span>(LParen) { <i class="callout" data-value="2"></i></code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open_before</span>(lhs);</code>
<code>    <span class="hl-title function_ invoke__">arg_list</span>(p);</code>
<code>    lhs = p.<span class="hl-title function_ invoke__">close</span>(m, ExprCall);</code>
<code>  }</code>
<code>}</code>
<code></code>
<code><span class="hl-comment">// ArgList = &#x27;(&#x27; Arg* &#x27;)&#x27;</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">arg_list</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(LParen));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(LParen);</code>
<code>  <span class="hl-keyword">while</span> !p.<span class="hl-title function_ invoke__">at</span>(RParen) &amp;&amp; !p.<span class="hl-title function_ invoke__">eof</span>() { <i class="callout" data-value="3"></i></code>
<code>    <span class="hl-title function_ invoke__">arg</span>(p);</code>
<code>  }</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(RParen);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, ArgList);</code>
<code>}</code>
<code></code>
<code><span class="hl-comment">// Arg = Expr &#x27;,&#x27;?</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">arg</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  <span class="hl-title function_ invoke__">expr</span>(p);</code>
<code>  <span class="hl-keyword">if</span> !p.<span class="hl-title function_ invoke__">at</span>(RParen) { <i class="callout" data-value="4"></i></code>
<code>    p.<span class="hl-title function_ invoke__">expect</span>(Comma);</code>
<code>  }</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, Arg);</code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<p><code>expr_delimited</code><span> now returns a </span><code>MarkClosed</code><span> rather than </span><code>()</code><span>.</span>
<span>No code changes are required for this, as </span><code>close</code><span> calls are already in the tail position.</span></p>
</li>
<li>
<p><span>To parse function calls, we check whether we are at </span><code>(</code><span> and use </span><code>open_before</code><span> API if that is the case.</span></p>
</li>
<li>
<p><span>Parsing argument list should be routine by now.</span>
<span>Again, as an expression can start with many different tokens, we don</span>&rsquo;<span>t add an </span><code>if p.at</code><span> check to the loop</span>&rsquo;<span>s body, and require </span><code>arg</code><span> to consume at least one token.</span></p>
</li>
<li>
<p><span>Inside </span><code>arg</code><span>, we use an already familiar construct to parse an optionally trailing comma.</span></p>
</li>
</ol>
<p><span>Now only binary expressions are left.</span>
<span>We will use a Pratt parser for those.</span>
<span>This is genuinely tricky code, so I have a dedicated article explaining how it all works:</span></p>
<p><span class="display"><a href="https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html"><em><span>Simple but Powerful Pratt Parsing</span></em></a><span> .</span></span></p>
<p><span>Here, I</span>&rsquo;<span>ll just dump a pageful of code without much explanation:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">expr</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-title function_ invoke__">expr_rec</span>(p, Eof); <i class="callout" data-value="2"></i></code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">expr_rec</span>(p: &amp;<span class="hl-keyword">mut</span> Parser, left: TokenKind) { <i class="callout" data-value="1"></i></code>
<code>  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">lhs</span> = <span class="hl-title function_ invoke__">expr_delimited</span>(p);</code>
<code></code>
<code>  <span class="hl-keyword">while</span> p.<span class="hl-title function_ invoke__">at</span>(LParen) {</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open_before</span>(lhs);</code>
<code>    <span class="hl-title function_ invoke__">arg_list</span>(p);</code>
<code>    lhs = p.<span class="hl-title function_ invoke__">close</span>(m, ExprCall);</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">loop</span> {</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-variable">right</span> = p.<span class="hl-title function_ invoke__">nth</span>(<span class="hl-number">0</span>);</code>
<code>    <span class="hl-keyword">if</span> <span class="hl-title function_ invoke__">right_binds_tighter</span>(left, right) { <i class="callout" data-value="1"></i></code>
<code>      <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open_before</span>(lhs);</code>
<code>      p.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>      <span class="hl-title function_ invoke__">expr_rec</span>(p, right);</code>
<code>      lhs = p.<span class="hl-title function_ invoke__">close</span>(m, ExprBinary);</code>
<code>    } <span class="hl-keyword">else</span> {</code>
<code>      <span class="hl-keyword">break</span>;</code>
<code>    }</code>
<code>  }</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">right_binds_tighter</span>( <i class="callout" data-value="1"></i></code>
<code>  left: TokenKind,</code>
<code>  right: TokenKind,</code>
<code>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> {</code>
<code>  <span class="hl-keyword">fn</span> <span class="hl-title function_">tightness</span>(kind: TokenKind) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Option</span>&lt;<span class="hl-type">usize</span>&gt; {</code>
<code>    [</code>
<code>      <span class="hl-comment">// Precedence table:</span></code>
<code>      [Plus, Minus].<span class="hl-title function_ invoke__">as_slice</span>(),</code>
<code>      &amp;[Star, Slash],</code>
<code>    ]</code>
<code>    .<span class="hl-title function_ invoke__">iter</span>()</code>
<code>    .<span class="hl-title function_ invoke__">position</span>(|level| level.<span class="hl-title function_ invoke__">contains</span>(&amp;kind))</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(right_tightness) = <span class="hl-title function_ invoke__">tightness</span>(right) <span class="hl-keyword">else</span> { <i class="callout" data-value="3"></i></code>
<code>    <span class="hl-keyword">return</span> <span class="hl-literal">false</span></code>
<code>  };</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(left_tightness) = <span class="hl-title function_ invoke__">tightness</span>(left) <span class="hl-keyword">else</span> {</code>
<code>    <span class="hl-built_in">assert!</span>(left == Eof);</code>
<code>    <span class="hl-keyword">return</span> <span class="hl-literal">true</span>;</code>
<code>  };</code>
<code></code>
<code>  right_tightness &gt; left_tightness</code>
<code>}</code></pre>

</figure>
<ol class="callout">
<li>
<p><span>In this version of pratt, rather than passing numerical precedence, I pass the actual token (learned that from </span><a href="https://www.scattered-thoughts.net/writing/better-operator-precedence/"><span>jamii</span>&rsquo;<span>s post</span></a><span>).</span>
<span>So, to determine whether to break or recur in the Pratt loop, we ask which of the two tokens binds tighter and act accordingly.</span></p>
</li>
<li>
<p><span>When we start parsing an expression, we don</span>&rsquo;<span>t have an operator to the left yet, so I just pass </span><code>Eof</code><span> as a dummy token.</span></p>
</li>
<li>
<p><span>The code naturally handles the case when the next token is not an operator (that is, when expression is complete, or when there</span>&rsquo;<span>s some syntax error).</span></p>
</li>
</ol>
<p><span>And that</span>&rsquo;<span>s it! We have parsed the entirety of L!</span></p>
</section>
<section id="Basic-Resilience">

    <h2>
    <a href="#Basic-Resilience"><span>Basic Resilience</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s see how resilient our basic parser is.</span>
<span>Let</span>&rsquo;<span>s check our motivational example:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">fib_rec</span>(f1: <span class="hl-type">u32</span>,</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">fib</span>(n: <span class="hl-type">u32</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</code>
<code>  <span class="hl-keyword">return</span> <span class="hl-title function_ invoke__">fib_rec</span>(<span class="hl-number">1</span>, <span class="hl-number">1</span>, n);</code>
<code>}</code></pre>

</figure>
<p><span>Here, the syntax tree our parser produces is surprisingly exactly what we want:</span></p>

<figure class="code-block">


<pre><code>File</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;fib_rec&#x27;</span></code>
<code>    ParamList</code>
<code>      <span class="hl-string">&#x27;(&#x27;</span></code>
<code>      (Param <span class="hl-string">&#x27;f1&#x27;</span> <span class="hl-string">&#x27;:&#x27;</span> (TypeExpr <span class="hl-string">&#x27;u32&#x27;</span>) <span class="hl-string">&#x27;,&#x27;</span>)</code>
<code>    error: expected RParen</code>
<code></code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;fib&#x27;</span></code>
<code>    ...</code></pre>

</figure>
<p><span>For the first incomplete function, we get </span><code>Fn</code><span>, </span><code>Param</code><span> and </span><code>ParamList</code><span>, as we should.</span>
<span>The second function is parsed without any errors.</span></p>
<p><span>Curiously, we get this great result without much explicit effort to make parsing resilient, it</span>&rsquo;<span>s a natural outcome of just not failing in the presence of errors.</span>
<span>The following ingredients help us:</span></p>
<ul>
<li>
<span>homogeneous syntax tree supports arbitrary malformed code,</span>
</li>
<li>
<span>any syntactic construct is parsed left-to-right, and valid prefixes are always recognized,</span>
</li>
<li>
<span>our top-level loop in </span><code>file</code><span> is greedy: it either parses a function, or skips a single token and tries to parse a function again.</span>
<span>That way, if there</span>&rsquo;<span>s a valid function somewhere, it will be recognized.</span>
</li>
</ul>
<p><span>Thinking about the last case both reveals the limitations of our current code, and shows avenues for improvement.</span>
<span>In general, parsing works as a series of nested loops:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">loop</span> { <span class="hl-comment">// parse a list of functions</span></code>
<code></code>
<code>  <span class="hl-keyword">loop</span> { <span class="hl-comment">// parse a list of statements inside a function</span></code>
<code></code>
<code>    <span class="hl-keyword">loop</span> { <span class="hl-comment">// parse a list of expressions</span></code>
<code></code>
<code>    }</code>
<code>  }</code>
<code>}</code></pre>

</figure>
<p><span>If something goes wrong inside a loop, our choices are:</span></p>
<ul>
<li>
<span>skip a token, and continue with the next iteration of the current loop,</span>
</li>
<li>
<span>break out of the inner loop, and let the outer loop handle recovery.</span>
</li>
</ul>
<p><span>The top-most loop must use the </span>&ldquo;<span>skip a token</span>&rdquo;<span> solution, because it needs to consume all of the input tokens.</span></p>
</section>
<section id="Improving-Resilience">

    <h2>
    <a href="#Improving-Resilience"><span>Improving Resilience</span> </a>
    </h2>
<p><span>Right now, each loop either always skips, or always breaks.</span>
<span>This is not optimal.</span>
<span>Consider this example:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">f1</span>(x: <span class="hl-type">i32</span>,</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">f2</span>(x: <span class="hl-type">i32</span>,, z: <span class="hl-type">i32</span>) {}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">f3</span>() {}</code></pre>

</figure>
<p><span>Here, for </span><code>f1</code><span> we want to break out of </span><code>param_list</code><span> loop, and our code does just that.</span>
<span>For </span><code>f2</code><span> though, the error is a duplicated comma (the user will add a new parameter between </span><code>x</code><span> and </span><code>z</code><span> shortly), so we want to skip here.</span>
<span>We don</span>&rsquo;<span>t, and, as a result, the syntax tree for </span><code>f2</code><span> is a train wreck:</span></p>

<figure class="code-block">


<pre><code>Fn</code>
<code>  <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>  <span class="hl-string">&#x27;f2&#x27;</span></code>
<code>  ParamList</code>
<code>    <span class="hl-string">&#x27;(&#x27;</span></code>
<code>    (Param <span class="hl-string">&#x27;x&#x27;</span> <span class="hl-string">&#x27;:&#x27;</span> (TypeExpr <span class="hl-string">&#x27;i32&#x27;</span>) <span class="hl-string">&#x27;,&#x27;</span>)</code>
<code>(ErrorTree <span class="hl-string">&#x27;,&#x27;</span>)</code>
<code>(ErrorTree <span class="hl-string">&#x27;z&#x27;</span>)</code>
<code>(ErrorTree <span class="hl-string">&#x27;:&#x27;</span>)</code>
<code>(ErrorTree <span class="hl-string">&#x27;i32&#x27;</span>)</code>
<code>(ErrorTree <span class="hl-string">&#x27;)&#x27;</span>)</code>
<code>(ErrorTree <span class="hl-string">&#x27;{&#x27;</span>)</code>
<code>(ErrorTree <span class="hl-string">&#x27;}&#x27;</span>)</code></pre>

</figure>
<p><span>For parameters, it is reasonable to skip tokens until we see something which implies the end of the parameter list.</span>
<span>For example, if we are parsing a list of parameters and see an </span><code>fn</code><span> token, then we</span>&rsquo;<span>d better stop.</span>
<span>If we see some less salient token, it</span>&rsquo;<span>s better to gobble it up.</span>
<span>Let</span>&rsquo;<span>s implement the idea:</span></p>

<figure class="code-block">


<pre><code class="hl-line"><span class="hl-keyword">const</span> PARAM_LIST_RECOVERY: &amp;[TokenKind] = &amp;[Arrow, LCurly, FnKeyword];</code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">param_list</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(LParen));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(LParen);</code>
<code>  <span class="hl-keyword">while</span> !p.<span class="hl-title function_ invoke__">at</span>(RParen) &amp;&amp; !p.<span class="hl-title function_ invoke__">eof</span>() {</code>
<code>    <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at</span>(Name) {</code>
<code>      <span class="hl-title function_ invoke__">param</span>(p);</code>
<code>    } <span class="hl-keyword">else</span> {</code>
<code class="hl-line">      <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at_any</span>(PARAM_LIST_RECOVERY) {</code>
<code class="hl-line">        <span class="hl-keyword">break</span>;</code>
<code class="hl-line">      }</code>
<code class="hl-line">      p.<span class="hl-title function_ invoke__">advance_with_error</span>(<span class="hl-string">&quot;expected parameter&quot;</span>);</code>
<code>    }</code>
<code>  }</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(RParen);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, ParamList);</code>
<code>}</code></pre>

</figure>
<p><span>Here, we use </span><code>at_any</code><span> helper function, which is like </span><code>at</code><span>, but takes a list of tokens.</span>
<span>The real implementation would use bitsets for this purpose.</span></p>
<p><span>The example now parses correctly:</span></p>

<figure class="code-block">


<pre><code>File</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;f1&#x27;</span></code>
<code>    ParamList</code>
<code>      <span class="hl-string">&#x27;(&#x27;</span></code>
<code>      (Param <span class="hl-string">&#x27;x&#x27;</span> <span class="hl-string">&#x27;:&#x27;</span> (TypeExpr <span class="hl-string">&#x27;i32&#x27;</span>) <span class="hl-string">&#x27;,&#x27;</span>)</code>
<code>      error: expected RParen</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;f2&#x27;</span></code>
<code>    ParamList</code>
<code>      <span class="hl-string">&#x27;(&#x27;</span></code>
<code>      (Param <span class="hl-string">&#x27;x&#x27;</span> <span class="hl-string">&#x27;:&#x27;</span> (TypeExpr <span class="hl-string">&#x27;i32&#x27;</span>) <span class="hl-string">&#x27;,&#x27;</span>)</code>
<code>      ErrorTree</code>
<code>        error: expected parameter</code>
<code>        <span class="hl-string">&#x27;,&#x27;</span></code>
<code>      (Param <span class="hl-string">&#x27;z&#x27;</span> <span class="hl-string">&#x27;:&#x27;</span> (TypeExpr <span class="hl-string">&#x27;i32&#x27;</span>))</code>
<code>      <span class="hl-string">&#x27;)&#x27;</span></code>
<code>    (Block <span class="hl-string">&#x27;{&#x27;</span> <span class="hl-string">&#x27;}&#x27;</span>)</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;f3&#x27;</span></code>
<code>    (ParamList <span class="hl-string">&#x27;(&#x27;</span> <span class="hl-string">&#x27;)&#x27;</span>)</code>
<code>    (Block <span class="hl-string">&#x27;{&#x27;</span> <span class="hl-string">&#x27;}&#x27;</span>)</code></pre>

</figure>
<p><span>What is a reasonable </span><code>RECOVERY</code><span> set in a general case?</span>
<span>I don</span>&rsquo;<span>t know the answer to this question, but </span><dfn>follow</dfn><span> sets from formal grammar theory give a good intuition.</span>
<span>We don</span>&rsquo;<span>t want </span><em><span>exactly</span></em><span> the </span><dfn>follow</dfn><span> set: for </span><code>ParamList</code><span>, </span><code>{</code><span> is in </span><dfn>follow</dfn><span>, and we do want it to be a part of the recovery set, but </span><code>fn</code><span> is </span><em><span>not</span></em><span> in </span><dfn>follow</dfn><span>, and yet it is important to recover on it.</span>
<code>fn</code><span> is included because it</span>&rsquo;<span>s in the </span><dfn>follow</dfn><span> for </span><code>Fn</code><span>, and </span><code>ParamList</code><span> is a child of </span><code>Fn</code><span>: we also want to recursively include ancestor </span><dfn>follow</dfn><span> sets into the recovery set.</span></p>
<p><span>For expressions and statements, we have the opposite problem </span>&mdash;<span> </span><code>block</code><span> and </span><code>arg_list</code><span> loops eagerly consume erroneous tokens, but sometimes it would be wise to break out of the loop instead.</span></p>
<p><span>Consider this example:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">f</span>() {</code>
<code>  <span class="hl-title function_ invoke__">g</span>(<span class="hl-number">1</span>,</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">x</span> =</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">g</span>() {}</code></pre>

</figure>
<p><span>It gives another train wreck syntax tree, where the </span><code>g</code><span> function is completely missed:</span></p>

<figure class="code-block">


<pre><code>File</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;f&#x27;</span></code>
<code>    (ParamList <span class="hl-string">&#x27;(&#x27;</span> <span class="hl-string">&#x27;)&#x27;</span>)</code>
<code>    Block</code>
<code>      <span class="hl-string">&#x27;{&#x27;</span></code>
<code>      StmtExpr</code>
<code>        ExprCall</code>
<code>          (ExprName <span class="hl-string">&#x27;g&#x27;</span>)</code>
<code>          ArgList</code>
<code>            <span class="hl-string">&#x27;(&#x27;</span></code>
<code>            (Arg (ExprLiteral <span class="hl-string">&#x27;1&#x27;</span>) <span class="hl-string">&#x27;,&#x27;</span>)</code>
<code>            (Arg (ErrorTree <span class="hl-string">&#x27;let&#x27;</span>))</code>
<code>            (Arg (ExprName <span class="hl-string">&#x27;x&#x27;</span>))</code>
<code>            (Arg (ErrorTree <span class="hl-string">&#x27;=&#x27;</span>))</code>
<code>            (Arg (ErrorTree <span class="hl-string">&#x27;}&#x27;</span>))</code>
<code>            (Arg (ErrorTree <span class="hl-string">&#x27;fn&#x27;</span>))</code>
<code>            Arg</code>
<code>              ExprCall</code>
<code>                (ExprName <span class="hl-string">&#x27;g&#x27;</span>)</code>
<code>                (ArgList <span class="hl-string">&#x27;(&#x27;</span> <span class="hl-string">&#x27;)&#x27;</span>)</code>
<code>            (Arg (ErrorTree <span class="hl-string">&#x27;{&#x27;</span>))</code>
<code>            (Arg (ErrorTree <span class="hl-string">&#x27;}&#x27;</span>))</code></pre>

</figure>
<p><span>Recall that the root cause here is that we require </span><code>expr</code><span> to consume at least one token, because it</span>&rsquo;<span>s not immediately obvious which tokens can start an expression.</span>
<span>It</span>&rsquo;<span>s not immediately obvious, but easy to compute </span>&mdash;<span> that</span>&rsquo;<span>s exactly </span><dfn>first</dfn><span> set from formal grammars.</span></p>
<p><span>Using it, we get:</span></p>

<figure class="code-block">


<pre><code class="hl-line"><span class="hl-keyword">const</span> STMT_RECOVERY: &amp;[TokenKind] = &amp;[FnKeyword];</code>
<code class="hl-line"><span class="hl-keyword">const</span> EXPR_FIRST: &amp;[TokenKind] =</code>
<code class="hl-line">  &amp;[Int, TrueKeyword, FalseKeyword, Name, LParen];</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">block</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(LCurly));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(LCurly);</code>
<code>  <span class="hl-keyword">while</span> !p.<span class="hl-title function_ invoke__">at</span>(RCurly) &amp;&amp; !p.<span class="hl-title function_ invoke__">eof</span>() {</code>
<code>    <span class="hl-keyword">match</span> p.<span class="hl-title function_ invoke__">nth</span>(<span class="hl-number">0</span>) {</code>
<code>      LetKeyword =&gt; <span class="hl-title function_ invoke__">stmt_let</span>(p),</code>
<code>      ReturnKeyword =&gt; <span class="hl-title function_ invoke__">stmt_return</span>(p),</code>
<code>      _ =&gt; {</code>
<code class="hl-line">        <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at_any</span>(EXPR_FIRST) {</code>
<code class="hl-line">          <span class="hl-title function_ invoke__">stmt_expr</span>(p)</code>
<code class="hl-line">        } <span class="hl-keyword">else</span> {</code>
<code class="hl-line">          <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at_any</span>(STMT_RECOVERY) {</code>
<code class="hl-line">            <span class="hl-keyword">break</span>;</code>
<code class="hl-line">          }</code>
<code class="hl-line">          p.<span class="hl-title function_ invoke__">advance_with_error</span>(<span class="hl-string">&quot;expected statement&quot;</span>);</code>
<code class="hl-line">        }</code>
<code>      }</code>
<code>    }</code>
<code>  }</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(RCurly);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, Block);</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">arg_list</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) {</code>
<code>  <span class="hl-built_in">assert!</span>(p.<span class="hl-title function_ invoke__">at</span>(LParen));</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(LParen);</code>
<code>  <span class="hl-keyword">while</span> !p.<span class="hl-title function_ invoke__">at</span>(RParen) &amp;&amp; !p.<span class="hl-title function_ invoke__">eof</span>() {</code>
<code class="hl-line">    <span class="hl-keyword">if</span> p.<span class="hl-title function_ invoke__">at_any</span>(EXPR_FIRST) {</code>
<code class="hl-line">      <span class="hl-title function_ invoke__">arg</span>(p);</code>
<code class="hl-line">    } <span class="hl-keyword">else</span> {</code>
<code class="hl-line">        <span class="hl-keyword">break</span>;</code>
<code class="hl-line">    }</code>
<code>  }</code>
<code>  p.<span class="hl-title function_ invoke__">expect</span>(RParen);</code>
<code></code>
<code>  p.<span class="hl-title function_ invoke__">close</span>(m, ArgList);</code>
<code>}</code></pre>

</figure>
<p><span>This fixes the syntax tree:</span></p>

<figure class="code-block">


<pre><code>File</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;f&#x27;</span></code>
<code>    (ParamList <span class="hl-string">&#x27;(&#x27;</span> <span class="hl-string">&#x27;)&#x27;</span>)</code>
<code>    Block</code>
<code>      <span class="hl-string">&#x27;{&#x27;</span></code>
<code>      StmtExpr</code>
<code>        ExprCall</code>
<code>          (ExprName <span class="hl-string">&#x27;g&#x27;</span>)</code>
<code>          ArgList</code>
<code>            <span class="hl-string">&#x27;(&#x27;</span></code>
<code>            (Arg (ExprLiteral <span class="hl-string">&#x27;1&#x27;</span> <span class="hl-string">&#x27;,&#x27;</span>))</code>
<code>      StmtLet</code>
<code>        <span class="hl-string">&#x27;let&#x27;</span></code>
<code>        <span class="hl-string">&#x27;x&#x27;</span></code>
<code>        <span class="hl-string">&#x27;=&#x27;</span></code>
<code>        (ErrorTree <span class="hl-string">&#x27;}&#x27;</span>)</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;g&#x27;</span></code>
<code>    (ParamList <span class="hl-string">&#x27;(&#x27;</span> <span class="hl-string">&#x27;)&#x27;</span>)</code>
<code>    (Block <span class="hl-string">&#x27;{&#x27;</span> <span class="hl-string">&#x27;}&#x27;</span>)</code></pre>

</figure>
<p><span>There</span>&rsquo;<span>s only one issue left.</span>
<span>Our </span><code>expr</code><span> parsing is still greedy, so, in a case like this</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">f</span>() {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">x</span> = <span class="hl-number">1</span> +</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">y</span> = <span class="hl-number">2</span></code>
<code>}</code></pre>

</figure>
<p><span>the </span><code>let</code><span> will be consumed as a right-hand-side operand of </span><code>+</code><span>.</span>
<span>Now that the callers of </span><code>expr</code><span> contain a check for </span><code>EXPR_FIRST</code><span>, we no longer need this greediness and can return </span><code>None</code><span> if no expression can be parsed:</span></p>

<figure class="code-block">


<pre><code class="hl-line"><span class="hl-keyword">fn</span> <span class="hl-title function_">expr_delimited</span>(p: &amp;<span class="hl-keyword">mut</span> Parser) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Option</span>&lt;MarkClosed&gt; {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">result</span> = <span class="hl-keyword">match</span> p.<span class="hl-title function_ invoke__">nth</span>(<span class="hl-number">0</span>) {</code>
<code>    <span class="hl-comment">// ExprLiteral = &#x27;int&#x27; | &#x27;true&#x27; | &#x27;false&#x27;</span></code>
<code>    Int | TrueKeyword | FalseKeyword =&gt; {</code>
<code>      <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code>      p.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>      p.<span class="hl-title function_ invoke__">close</span>(m, ExprLiteral)</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-comment">// ExprName = &#x27;name&#x27;</span></code>
<code>    Name =&gt; {</code>
<code>      <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code>      p.<span class="hl-title function_ invoke__">advance</span>();</code>
<code>      p.<span class="hl-title function_ invoke__">close</span>(m, ExprName)</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-comment">// ExprParen   = &#x27;(&#x27; Expr &#x27;)&#x27;</span></code>
<code>    LParen =&gt; {</code>
<code>      <span class="hl-keyword">let</span> <span class="hl-variable">m</span> = p.<span class="hl-title function_ invoke__">open</span>();</code>
<code>      p.<span class="hl-title function_ invoke__">expect</span>(LParen);</code>
<code>      <span class="hl-title function_ invoke__">expr</span>(p);</code>
<code>      p.<span class="hl-title function_ invoke__">expect</span>(RParen);</code>
<code>      p.<span class="hl-title function_ invoke__">close</span>(m, ExprParen)</code>
<code>    }</code>
<code></code>
<code>    _ =&gt; {</code>
<code class="hl-line">      <span class="hl-built_in">assert!</span>(!p.<span class="hl-title function_ invoke__">at_any</span>(EXPR_FIRST));</code>
<code class="hl-line">      <span class="hl-keyword">return</span> <span class="hl-literal">None</span>;</code>
<code>    }</code>
<code>  };</code>
<code>  <span class="hl-title function_ invoke__">Some</span>(result)</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">expr_rec</span>(p: &amp;<span class="hl-keyword">mut</span> Parser, left: TokenKind) {</code>
<code class="hl-line">  <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(<span class="hl-keyword">mut</span> lhs) = <span class="hl-title function_ invoke__">expr_delimited</span>(p) <span class="hl-keyword">else</span> {</code>
<code class="hl-line">    <span class="hl-keyword">return</span>;</code>
<code class="hl-line">  };</code>
<code>  ...</code>
<code>}</code></pre>

</figure>
<p><span>This gives the following syntax tree:</span></p>

<figure class="code-block">


<pre><code>File</code>
<code>  Fn</code>
<code>    <span class="hl-string">&#x27;fn&#x27;</span></code>
<code>    <span class="hl-string">&#x27;f&#x27;</span></code>
<code>    (ParamList <span class="hl-string">&#x27;(&#x27;</span> <span class="hl-string">&#x27;)&#x27;</span>)</code>
<code>    Block</code>
<code>      <span class="hl-string">&#x27;{&#x27;</span></code>
<code>      StmtLet</code>
<code>        <span class="hl-string">&#x27;let&#x27;</span></code>
<code>        <span class="hl-string">&#x27;x&#x27;</span></code>
<code>        <span class="hl-string">&#x27;=&#x27;</span></code>
<code>        (ExprBinary (ExprLiteral <span class="hl-string">&#x27;1&#x27;</span>) <span class="hl-string">&#x27;+&#x27;</span>)</code>
<code>      StmtLet</code>
<code>        <span class="hl-string">&#x27;let&#x27;</span></code>
<code>        <span class="hl-string">&#x27;y&#x27;</span></code>
<code>        <span class="hl-string">&#x27;=&#x27;</span></code>
<code>        (ExprLiteral <span class="hl-string">&#x27;2&#x27;</span>)</code>
<code>      <span class="hl-string">&#x27;}&#x27;</span></code></pre>

</figure>
<p><span>And this concludes the tutorial!</span>
<span>You are now capable of implementing an IDE-grade parser for a real programming language from scratch.</span></p>
<p><span>Summarizing:</span></p>
<ul>
<li>
<p><span>Resilient parsing means recovering as much syntactic structure from erroneous code as possible.</span></p>
</li>
<li>
<p><span>Resilient parsing is important for IDEs and language servers, who</span>&rsquo;<span>s job mostly ends when the code does not have errors any more.</span></p>
</li>
<li>
<p><span>Resilient parsing is related, but distinct from error recovery and repair.</span>
<span>Rather than guessing what the user meant to write, the parser tries to make sense of what is actually written.</span></p>
</li>
<li>
<p><span>Academic literature tends to focus on error repair, and mostly ignores pure resilience.</span></p>
</li>
<li>
<p><span>The biggest challenge of resilient parsing is the design of a syntax tree data structure.</span>
<span>It should provide convenient and type-safe access to well-formed syntax trees, while allowing arbitrary malformed trees.</span></p>
</li>
<li>
<p><span>One possible design here is to make the underlying tree a dynamically-typed data structure (like JSON), and layer typed accessors on top (not covered in this article).</span></p>
</li>
<li>
<p><span>LL style parsers are a good fit for resilient parsing.</span>
<span>Because code is written left-to-right, it</span>&rsquo;<span>s important that the parser recognizes well-formed prefixes of incomplete syntactic constructs, and LL does just that.</span></p>
</li>
<li>
<p><span>Ultimately, parsing works as a stack of nested </span><code>for</code><span> loops.</span>
<span>Inside a single </span><code>for</code><span> loop, on each iteration, we need to decide between:</span></p>
<ul>
<li>
<span>trying to parse a sequence element,</span>
</li>
<li>
<span>skipping over an unexpected token,</span>
</li>
<li>
<span>breaking out of the nested loop and delegating recovery to the parent loop.</span>
</li>
</ul>
</li>
<li>
<p><dfn>first</dfn><span>, </span><dfn>follow</dfn><span> and recovery sets help making a specific decision.</span></p>
</li>
<li>
<p><span>In any case, if a loop tries to parse an item, item parsing </span><em><span>must</span></em><span> consume at least one token (if only to report an error).</span></p>
</li>
</ul>
<script type="module" src="/assets/resilient-parsing/main.js"></script>
<p><span>Source code for the article is here: </span><span class="display"><a href="https://github.com/matklad/resilient-ll-parsing/blob/master/src/lib.rs#L44" class="url">https://github.com/matklad/resilient-ll-parsing/blob/master/src/lib.rs#L44</a><span> .</span></span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">Zig Language Server And Cancellation</title>
<link href="https://matklad.github.io/2023/05/06/zig-language-server-and-cancellation.html" rel="alternate" type="text/html" title="Zig Language Server And Cancellation" />
<published>2023-05-06T00:00:00+00:00</published>
<updated>2023-05-06T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/05/06/zig-language-server-and-cancellation</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[I already have a dedicated post about a hypothetical Zig language server.
But perhaps the most important thing I've written so far on the topic is the short note at the end of Zig and Rust.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/05/06/zig-language-server-and-cancellation.html"><![CDATA[
    <h1>
    <a href="#Zig-Language-Server-And-Cancellation"><span>Zig Language Server And Cancellation</span> <time datetime="2023-05-06">May 6, 2023</time></a>
    </h1>
<p><span>I already have a dedicated post about a hypothetical </span><a href="https://matklad.github.io/2023/02/10/how-a-zig-ide-could-work.html"><span>Zig language server</span></a><span>.</span>
<span>But perhaps the most important thing I</span>&rsquo;<span>ve written so far on the topic is the short note at the end of </span><a href="https://matklad.github.io/2023/03/26/zig-and-rust.html#ide"><em><span>Zig and Rust</span></em></a><span>.</span></p>
<p><span>If you want to implement an LSP for a language, you need to start with a data model.</span>
<span>If you correctly implement a store of source code which evolves over time and allows computing (initially trivial) derived data, then filling in the data until it covers the whole language is a question of incremental improvement.</span>
<span>If, however, you don</span>&rsquo;<span>t start with a rock-solid data model, and rush to implement language features, you might find yourself needing to make a sharp U-turn several years down the road.</span></p>
<p><span>I find this pretty insightful!</span>
<span>At least, this evening I</span>&rsquo;<span>ve been pondering a  particular aspect of the data model, and I think I realized something new about the problem space!</span>
<span>The aspect is cancellation.</span></p>
<section id="Cancellation">

    <h2>
    <a href="#Cancellation"><span>Cancellation</span> </a>
    </h2>
<p><span>Consider this.</span>
<span>Your language server is happily doing something very useful and computationally-intensive </span>&mdash;
<span>typechecking a </span><a href="https://github.com/microsoft/TypeScript/blob/04d4580f4eedc036b014ef4329cffe9979da3af9/src/compiler/checker.ts"><span>giant typechecker</span></a><span>,</span>
<span>computing comptime </span><a href="https://en.wikipedia.org/wiki/Ackermann_function"><span>Ackermann function</span></a><span>,</span>
<span>or </span><a href="https://github.com/launchbadge/sqlx#sqlx-is-not-an-orm"><span>talking to Postgres</span></a><span>.</span>
<span>Now, the user comes in and starts typing in the very file the server is currently processing.</span>
<span>What is the desired behavior, and how could it be achieved?</span></p>
<p><span>One useful model here is strong consistency.</span>
<span>If the language server acknowledged a source code edit, all future semantic requests (like </span>&ldquo;<span>go to definition</span>&rdquo;<span> or </span>&ldquo;<span>code completion</span>&rdquo;<span>) reflect this change.</span>
<span>The behavior is </span><em><span>as if</span></em><span> all changes and requests are sequentially ordered, and the server fully processes all preceding edits before responding to a request.</span>
<span>There are two great benefits to this model.</span>
<span>First, for the implementor it</span>&rsquo;<span>s an easy model to reason about. It</span>&rsquo;<span>s always clear what the answer to a particular request should be, the model is fully deterministic.</span>
<span>Second, the model gives maximally useful guarantees to the user, strict serializability.</span></p>
<p><span>So consider this sequence of events:</span></p>
<ol>
<li>
<span>User types </span><code>fo</code><span>.</span>
</li>
<li>
<span>The editor sends the edit to the language server.</span>
</li>
<li>
<span>The editor requests completions for </span><code>fo</code><span>.</span>
</li>
<li>
<span>The server starts furiously typechecking modified file to compute the result.</span>
</li>
<li>
<span>User types </span><code>o</code><span>.</span>
</li>
<li>
<span>The editor sends the </span><code>o</code><span>.</span>
</li>
<li>
<span>The editor re-requests completions, now for </span><code>foo</code><span>.</span>
</li>
</ol>
<p><span>How does the server deal with this?</span></p>
<p><span>The trivial solution is to run everything sequentially to completion.</span>
<span>So, on the step </span><code>6</code><span>, the server doesn</span>&rsquo;<span>t immediately acknowledge the edit, but rather blocks until it fully completes </span><code>4</code><span>.</span>
<span>This is a suboptimal behavior, because reads (computing completion) block writes (updating source code).</span>
<span>As a rule of thumb, writes should be prioritized over reads, because they reflect more up-to-date and more useful data.</span></p>
<p><span>A more optimal solution is to make the whole data model of the server immutable, such that edits do not modify data inplace, but rather create a separate, new state.</span>
<span>In this model, computing results for </span><code>3</code><span> and </span><code>7</code><span> proceeds in parallel, and, crucially, the edit </span><code>6</code><span> is accepted immediately.</span>
<span>The cost of this model is the requirement that all data structures are immutable.</span>
<span>It also is a bit wasteful </span>&mdash;<span> burning CPU to compute code completion for an already old file is useless, better dedicate all cores to the latest version.</span></p>
<p><span>A third approach is cancellation.</span>
<span>On step </span><code>6</code><span>, when the server becomes aware about the pending edit, it actively cancels all in-flight work pertaining to the old state and then applies modification in-place.</span>
<span>That way we don</span>&rsquo;<span>t need to defensively copy the data, and also avoid useless CPU work.</span>
<span>This is the strategy employed by rust-analyzer.</span></p>
<p><span>It</span>&rsquo;<span>s useful to think about why the server can</span>&rsquo;<span>t just, like, apply the edit in place completely ignoring any possible background work.</span>
<span>The edit ultimately changes some memory somewhere, which might be concurrently read by the code completion thread, yielding a data race and full-on UB.</span>
<span>It is possible to work-around this by applying </span><a href="https://dl.acm.org/doi/10.1145/2723372.2737784"><span>feral concurrency control</span></a><span> and just wrapping each individual bit of data in a mutex.</span>
<span>This removes the data race, but leads to excessive synchronization, sprawling complexity and broken logical invariants (function body might change in the middle of typechecking).</span></p>
<p><span>Finally, there</span>&rsquo;<span>s this final solution, or rather, idea for a solution.</span>
<span>One interesting approach for dealing with memory which is needed now, but not in the future, is semi-space garbage collection.</span>
<span>We divide the available memory in two equal parts, use one half as a working copy which accumulates useful objects and garbage, and then at some point switch the halves, copying the live objects (but not the garbage) over.</span>
<span>Another place where this idea comes up is Carmack</span>&rsquo;<span>s architecture for functional games.</span>
<span>On every frame, a game copies over the game state applying frame update function.</span>
<span>Because frames happen sequentially, you only need two copies of game state for this.</span>
<span>We can think about applying something like that for cancellation </span>&mdash;<span> without going for full immutability, we can let cancelled analysis to work with the old half-state, while we switch to the new one.</span></p>
<p><span>This </span>&hellip;<span> is not particularly actionable, but a good set of ideas to start thinking about evolution of a state in a language server.</span>
<span>And now for something completely different!</span></p>
</section>
<section id="Relaxed-Consistency">

    <h2>
    <a href="#Relaxed-Consistency"><span>Relaxed Consistency</span> </a>
    </h2>
<p><span>The strict consistency is a good default, and works especially well for languages with good support for separate compilation, as the amount of work a language server needs to do after an update is proportional to the size of the update, and to the amount of code on the screen, both of which are typically O(1).</span>
<span>For Zig, whose compilation model is </span>&ldquo;<span>start from the entry point and lazily compile everything that</span>&rsquo;<span>s actually used</span>&rdquo;<span>, this might be difficult to pull off.</span>
<span>It seems that Zig naturally gravitates to a smalltalk-like image-based programming model, where the server stores fully resolved code all the time, and, if some edit triggers re-analysis of a huge chunk of code, the user just has to wait until the server catches up.</span></p>
<p><span>But what if we don</span>&rsquo;<span>t do strong consistency?</span>
<span>What if we allow IDE to temporarily return non-deterministic and wrong results?</span>
<span>I think we can get some nice properties in exchange, if we use that semi-space idea.</span></p>
<p><span>The state of our language server would be comprised of three separate pieces of data:</span></p>
<ul>
<li>
<span>A fully analyzed snapshot of the world, </span><strong><code>ready</code></strong><span>.</span>
<span>This is a bunch of source file, plus their ASTs, ZIRs and AIRs.</span>
<span>This also probably contains an index of cross-references, so that finding all usages of an identifier requires just listing already precomputed results.</span>
</li>
<li>
<span>The next snapshot, which is being analyzed, </span><strong><code>working</code></strong><span>.</span>
<span>This is essentially the same data, but the AIR is being constructed.</span>
<span>We need </span><em><span>two</span></em><span> snapshots because we want to be able to query one of them while the second one is being updated.</span>
</li>
<li>
<span>Finally, we also hold ASTs for the files which are currently being modified, </span><strong><code>pending</code></strong><span>.</span>
</li>
</ul>
<p><span>The overall evolution of data is as follows.</span></p>
<p><span>All edits synchronously go to the </span><code>pending</code><span> state.</span>
<code>pending</code><span> is organized strictly on a per-file basis, so updating it can be done quickly on the main thread (maaaybe we want to move the parsing off the main thread, but my gut feeling is that we don</span>&rsquo;<span>t need to).</span>
<code>pending</code><span> always reflects the latest state of the world, it </span><em><span>is</span></em><span> the latest state of the world.</span></p>
<p><span>Periodically, we collect a batch of changes from </span><code>pending</code><span>, create a new </span><code>working</code><span> and kick off a full analysis in background.</span>
<span>A good point to do that would be when there</span>&rsquo;<span>s no syntax errors, or when the user saves a file.</span>
<span>There</span>&rsquo;<span>s at most one analysis in progress, so we accumulate changes in </span><code>pending</code><span> until the previous analysis finishes.</span></p>
<p><span>When </span><code>working</code><span> is fully processed, we atomically update the </span><code>ready</code><span>.</span>
<span>As </span><code>ready</code><span> is just an inert piece of data, it can be safely accessed from whatever thread.</span></p>
<p><span>When processing requests, we only use </span><code>ready</code><span> and </span><code>pending</code><span>.</span>
<span>Processing requires some heuristics.</span>
<code>ready</code><span> and </span><code>pending</code><span> describe different states of the world.</span>
<code>pending</code><span> guarantees that its state is up-to-date, but it only has AST-level data.</span>
<code>ready</code><span> is outdated, </span><em><span>but</span></em><span> it has every bit of semantic information pre-computed.</span>
<span>In particular, it includes cross-reference data.</span></p>
<p><span>So, our choices for computing results are:</span></p>
<ul>
<li>
<p><span>Use the </span><code>pending</code><span> AST.</span>
<span>Features like displaying the outline of the current file or globally fuzzy-searching function by name can be implemented like this.</span>
<span>These features always give correct results.</span></p>
</li>
<li>
<p><span>Find the match between the </span><code>pending</code><span> AST and the </span><code>ready</code><span> semantics.</span>
<span>This works perfectly for non-local </span>&ldquo;<span>goto definition</span>&rdquo;<span>.</span>
<span>Here, we can temporarily get </span>&ldquo;<span>wrong</span>&rdquo;<span> results, or no result at all.</span>
<span>However, the results we get are always instant.</span></p>
</li>
<li>
<p><span>Re-analyze </span><code>pending</code><span> AST using results from </span><code>ready</code><span> for the analysis of the context.</span>
<span>This is what we</span>&rsquo;<span>ll use for code completion.</span>
<span>For code completion, </span><code>pending</code><span> will be maximally diverging from </span><code>ready</code><span> (especially if we use </span>&ldquo;<span>no syntax errors</span>&rdquo;<span> as a heuristic for promoting </span><code>pending</code><span> to </span><code>working</code><span>),</span>
<span>so we won</span>&rsquo;<span>t be able to complete based purely on </span><code>ready</code><span>.</span>
<span>At the same time, completion is heavily semantics-dependent, so we won</span>&rsquo;<span>t be able to drive it through </span><code>pending</code><span>.</span>
<span>And we also can</span>&rsquo;<span>t launch full semantic analysis on </span><code>pending</code><span> (what we effectively do in </span><code>rust-analyzer</code><span>), due to </span>&ldquo;<span>from root</span>&rdquo;<span> analysis nature.</span></p>
<p><span>But we can merge two analysis techniques.</span>
<span>For example, if we are completing in a function which starts as </span><span class="display"><code>fn f(comptime T: type, param: T)</code><span>,</span></span>
<span>we can use </span><code>ready</code><span> to get a set of values of </span><code>T</code><span> the function is actually called with, to complete </span><code>param.</code><span> in a useful way.</span>
<span>Dually, if inside </span><code>f</code><span> we have something like </span><span class="display"><code>const list = std.ArrayList(u32){}</code><span>,</span></span><span> we don</span>&rsquo;<span>t have to </span><code>comptime</code><span> evaluate the </span><code>ArrayList</code><span> function, we can fetch the result from </span><code>ready</code><span>.</span></p>
<p><span>Of course, we must also handle the case where there</span>&rsquo;<span>s no </span><code>ready</code><span> yet (it</span>&rsquo;<span>s a first compilation, or we switched branches), so completion would be somewhat non-deterministic.</span></p>
</li>
</ul>
<p><span>One important flow where non-determinism would get in a way is refactoring.</span>
<span>When you rename something, you should be 100% sure that you</span>&rsquo;<span>ve found all usages.</span>
<span>So, any refactor would have to be a blocking operation where we first wait for the current </span><code>working</code><span> to complete, then update </span><code>working</code><span> with the </span><code>pending</code><span> accumulated so far, and wait for </span><em><span>that</span></em><span> to complete, to, finally, apply the refactor using only up-to-date </span><code>ready</code><span>.</span>
<span>Luckily, refactoring is almost always a two-phase flow, reminiscent of a GET/POST flow for HTTP form (</span><a href="https://rust-analyzer.github.io/blog/2020/09/28/how-to-make-a-light-bulb.html"><span>more about that</span></a><span>).</span>
<span>Any refactor starts with read-only analysis to inform the user about available options and to gather input.</span>
<span>For </span>&ldquo;<span>rename</span>&rdquo;<span>, you wait for the user to type the new name, for </span>&ldquo;<span>change signature</span>&rdquo;<span> the user needs to rearrange params.</span>
<span>This brief interactive window should give enough headroom to flush all </span><code>pending</code><span> changes, masking the latency.</span></p>
<p><span>I am pretty excited about this setup.</span>
<span>I think that</span>&rsquo;<span>s the way to go for Zig.</span></p>
<ul>
<li>
<span>The approach meshes extremely well with the ambition of doing incremental binary patching, both because it leans on complete global analysis, and because it contains an explicit notion of switching from one snapshot to the next one</span>
<span>(in contrast, rust-analyzer never really thinks about </span>&ldquo;<span>previous</span>&rdquo;<span> state of the code. There</span>&rsquo;<span>s always only the </span>&ldquo;<span>current</span>&rdquo;<span> state, with lazy, partially complete analysis).</span>
</li>
<li>
<span>Zig lacks declared interfaces, so a quick </span>&ldquo;<span>find all calls to this function</span>&rdquo;<span> operation is required for useful completion.</span>
<span>Fully resolved historical snapshot gives us just that.</span>
</li>
<li>
<span>Zig is carefully designed to make a lot of semantic information obvious just from the syntax.</span>
<span>Unlike Rust, Zig lacks syntactic macros or glob imports.</span>
<span>This makes is possible to do a lot of analysis correctly using only </span><code>pending</code><span> ASTs.</span>
</li>
<li>
<span>This approach nicely dodges the cancellation problem I</span>&rsquo;<span>ve spend half of the blog post explaining, and has a relatively simple threading story, which reduces implementation complexity.</span>
</li>
<li>
<span>Finally, it feels like it should be </span><em><span>super</span></em><span> fast (if not the most CPU efficient).</span>
</li>
</ul>

<figure>

<img alt="" src="/assets/zig-lsp.jpg">
</figure>
<p><span>Discussion on </span><a href="https://old.reddit.com/r/Zig/comments/13a8d9l/blog_post_zig_language_server_and_cancellation/"><span>/r/Zig</span></a><span>.</span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">Value Oriented Programming Needs Implicits?</title>
<link href="https://matklad.github.io/2023/05/02/implicits-for-mvs.html" rel="alternate" type="text/html" title="Value Oriented Programming Needs Implicits?" />
<published>2023-05-02T00:00:00+00:00</published>
<updated>2023-05-02T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/05/02/implicits-for-mvs</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[An amateur note on language design which explores two important questions:]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/05/02/implicits-for-mvs.html"><![CDATA[
    <h1>
    <a href="#Value-Oriented-Programming-Needs-Implicits"><span>Value Oriented Programming Needs Implicits?</span> <time datetime="2023-05-02">May 2, 2023</time></a>
    </h1>
<p><span>An amateur note on language design which explores two important questions:</span></p>
<ul>
<li>
<span>How to do polymorphism?</span>
</li>
<li>
<span>How to do anything at all?</span>
</li>
</ul>
<p><span>Let</span>&rsquo;<span>s start with the second question.</span>
<span>What is the basic stuff that everything else is made of?</span></p>
<p><span>Not so long ago, the most popular answer to that question was </span>&ldquo;<span>objects</span>&rdquo;<span> </span>&mdash;<span> blobs of mutable state with references to other blobs.</span>
<span>This turned out to be problematic </span>&mdash;<span> local mutation of an object might accidentally cause unwanted changes elsewhere.</span>
<span>Defensive copying of collections at the API boundary was a common pattern.</span></p>
<p><span>Another answer to the question of basic stuff  is </span>&ldquo;<span>immutable values</span>&rdquo;<span>, as exemplified by functional programming.</span>
<span>This fixes the ability to reason about programs locally at the cost of developer ergonomics and expressiveness.</span>
<span>A lot of code is naturally formulated in terms of </span>&ldquo;<span>let</span>&rsquo;<span>s mutate this little thing</span>&rdquo;<span>, and functionally threading the update through all the layers is tiresome.</span></p>
<p><span>The C answer is that everything is made of </span>&ldquo;<span>memory (*)</span>&rdquo;<span>.</span>
<span>It is almost as if memory is an array of bytes.</span>
<span>Almost, but not quite </span>&mdash;<span> to write portable programs amenable to optimization, certain restrictions must be placed on the ways memory is accessed and manipulated, hence (*).</span>
<span>These restrictions not being checked by the compiler (and not even visible in the source code) create a fertile ground for subtle bugs.</span></p>
<p><span>Rust takes this basic C model and:</span></p>
<ul>
<li>
<span>Makes the (*) explicit:</span>
<ul>
<li>
<span>pointers always carry the size of addressed memory, possibly at runtime (slices),</span>
</li>
<li>
<span>pointers carry lifetime, accessing the data past the end of the lifetime is forbidden.</span>
</li>
</ul>
</li>
<li>
<span>Adds aliasing information to the type system, such that it becomes possible to tell if there are </span><em><span>other</span></em><span> pointers pointing at a particular piece of memory.</span>
</li>
</ul>
<p><span>Curiously, this approach allows rust to have an </span>&ldquo;<span>immutable values</span>&rdquo;<span> feel, without requiring the user to thread updates manually,</span>
<a href="http://smallcultfollowing.com/babysteps/blog/2018/02/01/in-rust-ordinary-vectors-are-values/">&ldquo;<span>In Rust, Ordinary Vectors are Values</span>&rdquo;</a><span>.</span>
<span>But the cognitive cost for this approach is pretty high, as the universe of values is now forked by different flavors of owning/referencing.</span></p>
<p><span>Let</span>&rsquo;<span>s go back to the pure FP model.</span>
<span>Can we just locally fix it?</span>
<span>Let</span>&rsquo;<span>s take a look at an example:</span></p>

<figure class="code-block">


<pre><code>let xs1 = get_items() in</code>
<code>let xs2  = modify_items(xs1) in</code>
<code>let xs3 = sort_items(xs2) in</code>
<code>...</code></pre>

</figure>
<p><span>It is pretty clear that we can allow mutation of local variables via a simple rewrite, as that won</span>&rsquo;<span>t compromise local reasoning:</span></p>

<figure class="code-block">


<pre><code>var xs = get_items()</code>
<code>xs = modify_items(xs)</code>
<code>xs = sort_items(xs)</code></pre>

</figure>
<p><span>Similarly, we can introduce a rewrite rule for the ubiquitous </span><code>x = f(x)</code><span> pattern, such that the code looks like this:</span></p>

<figure class="code-block">


<pre><code>var xs = get_items()</code>
<code>modify_items(xs)</code>
<code>sort_items(xs)</code></pre>

</figure>
<p><span>Does this actually work?</span>
<span>Yes, it does, as popularized by Swift and distilled in its pure form by </span><a href="https://www.val-lang.dev"><span>Val</span></a><span>.</span></p>
<p><span>Formalizing the rewriting reasoning, we introduce second-class references, which can </span><em><span>only</span></em><span> appear in function arguments (</span><code>inout</code><span> parameters), but, eg, can</span>&rsquo;<span>t be stored as fields.</span>
<span>With these restrictions, </span>&ldquo;<span>borrow checking</span>&rdquo;<span> becomes fairly simple </span>&mdash;<span> at each function call it suffices to check that no two </span><code>inout</code><span> arguments overlap.</span></p>
<p><span>Now, let</span>&rsquo;<span>s switch gears and explore the second question </span>&mdash;<span> polymorphism.</span></p>
<p><span>Starting again with OOP, you can use subtyping with its familiar </span><span class="display"><code>class Dog extends Triangle</code><span>,</span></span><span> but that is not very flexible.</span>
<span>In particular, expressing something like </span>&ldquo;<span>sorting a list of items</span>&rdquo;<span> with pure subtyping is not too natural.</span>
<span>What works better is parametric polymorphism, where you add type parameters to your data structures:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">sort</span>&lt;T&gt;(items: &amp;<span class="hl-keyword">mut</span> <span class="hl-type">Vec</span>&lt;T&gt;)</code></pre>

</figure>
<p><span>Except that it doesn</span>&rsquo;<span>t quite work as, as we also need to specify how to sort the </span><code>T</code><span>s.</span>
<span>One approach here would be to introduce some sort of type-of-types, to group types with similar traits into a class:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">sort</span>&lt;T: Comparable&gt;(items: &amp;<span class="hl-keyword">mut</span> <span class="hl-type">Vec</span>&lt;T&gt;)</code></pre>

</figure>
<p><span>A somewhat simpler approach is to just explicitly pass in a comparison function:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">sort</span>&lt;T&gt;(</code>
<code>    compare: <span class="hl-title function_ invoke__">fn</span>(T, T) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span>,</code>
<code>    items: &amp;<span class="hl-keyword">mut</span> <span class="hl-type">Vec</span>&lt;T&gt;,</code>
<code>)</code></pre>

</figure>
<p><span>How does this relate to value oriented programming?</span>
<span>It happens that, when programming with values, a very common pattern is to use indexes to express relationships.</span>
<span>For example, to model parent-child relations (or arbitrary graphs), the following setup works:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">type</span> <span class="hl-title class_">Tree</span> = <span class="hl-type">Vec</span>&lt;Node&gt;;</code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">Node</span> {</code>
<code>    parent: <span class="hl-type">usize</span>,</code>
<code>    children: <span class="hl-type">Vec</span>&lt;<span class="hl-type">usize</span>&gt;,</code>
<code>}</code></pre>

</figure>
<p><span>Using direct references hits language limitations:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">struct</span> <span class="hl-title class_">Node</span> {</code>
<code>    parent: Node, <span class="hl-comment">// Who owns that?</span></code>
<code>    children: <span class="hl-type">Vec</span>&lt;Node&gt;,</code>
<code>}</code></pre>

</figure>
<p><span>Another good use-case is interning, where you have something like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">struct</span> <span class="hl-title class_">NameTable</span> {</code>
<code>    strings: <span class="hl-type">Vec</span>&lt;<span class="hl-type">String</span>&gt;,</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">struct</span> <span class="hl-title class_">Name</span>(<span class="hl-type">u32</span>);</code></pre>

</figure>
<p><span>How do we sort a </span><code>Vec&lt;Name&gt;</code><span>?</span>
<span>We can</span>&rsquo;<span>t use the type class approach here, as knowing the </span><em><span>type</span></em><span> of </span><code>Name</code><span> isn</span>&rsquo;<span>t enough to sort names lexicographically, an instance of </span><code>NameTable</code><span> is also required to fetch the actual string data.</span>
<span>The approach with just passing in comparison function works, as it can close over the correct </span><code>NameTable</code><span> in scope.</span></p>
<p><span>The problem with </span>&ldquo;<span>just pass a function</span>&rdquo;<span> is that it gets tedious quickly.</span>
<span>Rather than </span><span class="display"><code>xs.print()</code></span><span> you now need to say </span><span class="display"><code>xs.print(Int::print)</code><span>.</span></span>
<span>Luckily, similarly to how the compiler infers the type parameter </span><code>T</code><span> by default, we can allow limited inference of value parameters, which should remove most of the boilerplate.</span>
<span>So, something which looks like </span><span class="display"><code>names.print()</code></span><span> would desugar to </span><span class="display"><code>Vec::print_vec(self.name_table.print, names)</code><span>.</span></span></p>
<p><span>This could also synergize well with compile-time evaluation.</span>
<span>If (as is the common case), the value of the implicit function table is known at compile time, no table needs to be passed in at runtime (and we don</span>&rsquo;<span>t have to repeatedly evaluate the table itself).</span>
<span>We can even compile-time partially evaluate things within the compilation unit, and use runtime parameters at the module boundaries, just like Swift does.</span></p>
<p><span>And that</span>&rsquo;<span>s basically it!</span>
<span>TL;DR: value oriented programming / mutable value semantics is an interesting </span>&ldquo;<span>everything is X</span>&rdquo;<span> approach to get the benefits of functional purity without giving up on mutable hash tables.</span>
<span>This style of programming doesn</span>&rsquo;<span>t work with cyclic data structures (values are always trees), so indexes are often used to express auxiliary relations.</span>
<span>This, however, gets in a way of type-based generic programming </span>&mdash;<span> a </span><code>T</code><span> is no longer </span><code>Comparable</code><span>, only </span><code>T + Context</code><span> is.</span>
<span>A potential fix for that is to base generic programming on explicit dictionary passing combined with implicit value parameter inference.</span></p>
<p><span>Is there a language like this already?</span></p>
<p><span>Links:</span></p>
<ul>
<li>
<a href="https://www.val-lang.dev"><span>Val</span></a>
</li>
<li>
<a href="https://arxiv.org/pdf/1512.01895.pdf"><span>Modular implicits</span></a>
</li>
<li>
<a href="https://rust-lang.github.io/async-fundamentals-initiative/evaluation/design/with_clauses.html"><span>With clauses</span></a>
</li>
<li>
<a href="https://www.youtube.com/watch?v=ctS8FzqcRug"><span>Implementing Swift generics</span></a>
</li>
</ul>
]]></content>
</entry>

<entry>
<title type="text">Data Oriented Parallel Value Interner</title>
<link href="https://matklad.github.io/2023/04/23/data-oriented-parallel-value-interner.html" rel="alternate" type="text/html" title="Data Oriented Parallel Value Interner" />
<published>2023-04-23T00:00:00+00:00</published>
<updated>2023-04-23T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/04/23/data-oriented-parallel-value-interner</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[In this post, I will present a theoretical design for an interner.
It should be fast, but there will be no benchmarks as I haven't implemented the thing.
So it might actually be completely broken or super slow for one reason or another.
Still, I think there are a couple of neat ideas, which I would love to call out.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/04/23/data-oriented-parallel-value-interner.html"><![CDATA[
    <h1>
    <a href="#Data-Oriented-Parallel-Value-Interner"><span>Data Oriented Parallel Value Interner</span> <time datetime="2023-04-23">Apr 23, 2023</time></a>
    </h1>
<p><span>In this post, I will present a theoretical design for an interner.</span>
<span>It should be fast, but there will be no benchmarks as I haven</span>&rsquo;<span>t implemented the thing.</span>
<span>So it might actually be completely broken or super slow for one reason or another.</span>
<span>Still, I think there are a couple of neat ideas, which I would love to call out.</span></p>
<p><span>The context for the post is </span><a href="https://www.youtube.com/watch?v=AqDdWEiSwMM"><span>this talk</span></a><span> by Andrew Kelley, which notices that it</span>&rsquo;<span>s hard to reconcile interning and parallel compilation.</span>
<span>This is something I have been thinking about a lot in the context of rust-analyzer, which relies heavily on pointers, atomic reference counting and indirection to make incremental and parallel computation possible.</span></p>
<p><span>And yes, interning (or, more generally, assigning unique identities to things) is a big part of that.</span></p>
<p><span>Usually, compilers intern strings, but we will be interning trees today.</span>
<span>Specifically, we will be looking at something like a </span><a href="https://github.com/ziglang/zig/blob/b95cdf0aeb4d4d31c0b6a54302ef61baec8f6773/src/value.zig#L20"><code>Value</code></a><span> type from the Zig compiler.</span>
<span>In a simplified RAII style it could look like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">const</span> Value = <span class="hl-keyword">union</span>(<span class="hl-keyword">enum</span>) {</code>
<code>    <span class="hl-comment">// A bunch of payload-less variants.</span></code>
<code>    u1_type,</code>
<code>    u8_type,</code>
<code>    i8_type,</code>
<code></code>
<code>    <span class="hl-comment">// A number.</span></code>
<code>    <span class="hl-type">u64</span>: <span class="hl-type">u64</span>,</code>
<code></code>
<code>    <span class="hl-comment">// A declaration.</span></code>
<code>    <span class="hl-comment">// Declarations and types are also values in Zig.</span></code>
<code>    decl: DeclIndex,</code>
<code></code>
<code>    <span class="hl-comment">// Just some bytes for a string.</span></code>
<code>    bytes: []<span class="hl-type">u8</span>,</code>
<code></code>
<code>    <span class="hl-comment">// The interesting case which makes it a tree.</span></code>
<code>    <span class="hl-comment">// This is how struct instances are represented.</span></code>
<code>    aggregate: []Value,</code>
<code>};</code>
<code></code>
<code><span class="hl-keyword">const</span> DeclIndex = <span class="hl-type">u32</span>;</code></pre>

</figure>
<p><span>Such values are individually heap-allocated and in general are held behind pointers.</span>
<span>Zig</span>&rsquo;<span>s compiler adds a couple of extra tricks to this structure, like not overallocating for small enum variants:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">const</span> Value = <span class="hl-keyword">struct</span> {</code>
<code>    payload: <span class="hl-operator">*</span>Payload</code>
<code>}</code>
<code></code>
<code><span class="hl-comment">// Payload is an &quot;abstract&quot; type:</span></code>
<code><span class="hl-comment">// There&#x27;s some data following the `tag`,</span></code>
<code><span class="hl-comment">// whose type and size is determined by</span></code>
<code><span class="hl-comment">// this `tag`.</span></code>
<code><span class="hl-keyword">const</span> Payload = <span class="hl-keyword">struct</span> {</code>
<code>    tag: Tag,</code>
<code></code>
<code>    <span class="hl-keyword">pub</span> <span class="hl-keyword">const</span> U64 = <span class="hl-keyword">struct</span> {</code>
<code>        base: Payload,</code>
<code>        data: <span class="hl-type">u64</span>,</code>
<code>    };</code>
<code></code>
<code>    <span class="hl-keyword">pub</span> <span class="hl-keyword">const</span> Decl = <span class="hl-keyword">struct</span> {</code>
<code>        base: Payload,</code>
<code>        decl: DeclIndex,</code>
<code>    };</code>
<code>}</code></pre>

</figure>
<p><span>But how do we intern this stuff, such that:</span></p>
<ul>
<li>
<span>values are just </span><code>u32</code><span> rather than full pointers,</span>
</li>
<li>
<span>values are deduplicated,</span>
</li>
<li>
<span>and this whole construct works efficiently even if there are multiple threads</span>
<span>using our interner simultaneously?</span>
</li>
</ul>
<p><span>Let</span>&rsquo;<span>s start with concurrent </span><code>SegmentedList</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span><span class="hl-function"> SegmentList</span>(<span class="hl-keyword">comptime</span> T: <span class="hl-type">type</span>) <span class="hl-type">type</span> {</code>
<code>    <span class="hl-keyword">return</span> <span class="hl-keyword">struct</span> {</code>
<code>        echelons: [<span class="hl-numbers">31</span>]?[<span class="hl-operator">*</span>]T,</code>
<code>    };</code>
<code>}</code></pre>

</figure>
<p><span>Segmented list is like </span><code>ArrayList</code><span> with an extra super power that pushing new items does not move/invalidate old ones.</span>
<span>In normal </span><code>ArrayList</code><span>, when the backing storage fills up, you allocate a slice twice as long, copy over the elements from the old slice and then destroy it.</span>
<span>In </span><code>SegmentList</code><span>, you leave the old slice where it is, and just allocate a new one.</span></p>
<p><span>Now, as we are writing an interner and want to use </span><code>u32</code><span> for an index, we know that we need to store </span><code>1&lt;&lt;32</code><span> items max.</span>
<span>But that means that we</span>&rsquo;<span>ll need at most 31 segments for our </span><code>SegmentList</code><span>:</span></p>

<figure class="code-block">


<pre><code>[1 &lt;&lt; 0]T</code>
<code>[1 &lt;&lt; 1]T</code>
<code>[1 &lt;&lt; 2]T</code>
<code>...</code>
<code>[1 &lt;&lt; 31]T</code></pre>

</figure>
<p><span>So we can just </span>&ldquo;<span>pre-allocate</span>&rdquo;<span> array of 31 </span><em><span>pointers</span></em><span> to the segments, hence</span></p>

<figure class="code-block">


<pre><code>echelons: [<span class="hl-numbers">31</span>]?[<span class="hl-operator">*</span>]T,</code></pre>

</figure>
<p><span>If we want to be more precise with types, we can even use a tuple whose elements are nullable pointers to arrays of power-of-two sizes:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span><span class="hl-function"> SegmentList</span>(<span class="hl-keyword">comptime</span> T: <span class="hl-type">type</span>) <span class="hl-type">type</span> {</code>
<code>    <span class="hl-keyword">return</span> <span class="hl-keyword">struct</span> {</code>
<code>        echelons: std.meta.Tuple(get_echelons(<span class="hl-numbers">31</span>, T)),</code>
<code>    };</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span><span class="hl-function"> get_echelons</span>(</code>
<code>    <span class="hl-keyword">comptime</span> level: <span class="hl-type">usize</span>,</code>
<code>    <span class="hl-keyword">comptime</span> T: <span class="hl-type">type</span>,</code>
<code>) []<span class="hl-keyword">const</span> <span class="hl-type">type</span> {</code>
<code>    <span class="hl-keyword">if</span> (level <span class="hl-operator">==</span> <span class="hl-numbers">0</span>) <span class="hl-keyword">return</span> <span class="hl-operator">&amp;</span>.{ ?<span class="hl-operator">*</span>[<span class="hl-numbers">1</span>]T };</code>
<code>    <span class="hl-keyword">return</span> get_echelons(level <span class="hl-operator">-</span> <span class="hl-numbers">1</span>, T) <span class="hl-operator">+</span><span class="hl-operator">+</span> .{ ?<span class="hl-operator">*</span>[<span class="hl-numbers">1</span> <span class="hl-operator">&lt;&lt;</span> level]T };</code>
<code>}</code></pre>

</figure>
<p><span>Indexing into such an echeloned array is still O(1).</span>
<span>Here</span>&rsquo;<span>s how echelons look in terms of indexes</span></p>

<figure class="code-block">


<pre><code>0                      = 1  total</code>
<code>1 2                    = 3  total</code>
<code>3 4 5 6                = 7  total</code>
<code>7 8 9 10 11 12 13 14   = 15 total</code></pre>

</figure>
<p><span>The first </span><code>n</code><span> echelons hold </span><code>2**n - 1</code><span> elements.</span>
<span>So, if we want to find the </span><code>i</code><span>th item, we first find the echelon it is in, by computing the nearest smaller power of two of </span><code>i + 1</code><span>, and then index into the echelon with </span><code>i - (2**n - 1)</code><span>, give or take a </span><code>+1</code><span> here or there.</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// Warning: untested, probably has a couple of bugs.</span></code>
<code></code>
<code><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span><span class="hl-function"> get</span>(self: Self, index: <span class="hl-type">u32</span>) <span class="hl-operator">*</span><span class="hl-keyword">const</span> T {</code>
<code>    <span class="hl-keyword">const</span> e = self.get_echelon(index);</code>
<code>    <span class="hl-keyword">const</span> i = index <span class="hl-operator">-</span> (<span class="hl-numbers">1</span> <span class="hl-operator">&lt;&lt;</span> e <span class="hl-operator">-</span> <span class="hl-numbers">1</span>);</code>
<code>    <span class="hl-keyword">return</span> <span class="hl-operator">&amp;</span>self.echelons[e].?[i];</code>
<code>}</code>
<code></code>
<code><span class="hl-keyword">fn</span><span class="hl-function"> get_echelon</span>(index: <span class="hl-type">u32</span>) <span class="hl-type">u5</span> {</code>
<code>    <span class="hl-built_in">@ctz</span>(std.math.floorPowerOfTwo(index <span class="hl-operator">+</span> <span class="hl-numbers">1</span>));</code>
<code>}</code></pre>

</figure>
<p><span>Note that we pre-allocate an array of pointers to segments, but not the segments themselves.</span>
<span>Pointers are nullable, and we allocate new segments lazily, when we actually write to the corresponding indexes.</span>
<span>This structure is very friendly to parallel code.</span>
<span>Reading items works because items are never reallocated.</span>
<span>Lazily allocating new echelons is easy, because the position of the pointer is fixed.</span>
<span>That is, we can do something like this to insert an item at position </span><code>i</code><span>:</span></p>
<ol>
<li>
<span>compute the echelon index</span>
</li>
<li>
<code>@atomicLoad(.Acquire)</code><span> the pointer</span>
</li>
<li>
<span>if the pointer is null</span>
<ul>
<li>
<span>allocate the echelon</span>
</li>
<li>
<code>@cmpxchgStrong(.Acquire, .Release)</code><span> the pointer</span>
</li>
<li>
<span>free the redundant echelon if exchange failed</span>
</li>
</ul>
</li>
<li>
<span>insert the item</span>
</li>
</ol>
<p><span>Notice how we don</span>&rsquo;<span>t need any locks or even complicated atomics, at the price of sometimes doing a second redundant allocation.</span></p>
<p><span>One thing this data structure is bad at is doing bounds checks and tracking which items are actually initialized.</span>
<span>For the interner use-case, we will rely on an invariant that we always use indexes provided to use by someone else, such that possession of the index signifies that:</span></p>
<ul>
<li>
<span>the echelon holding the item is allocated</span>
</li>
<li>
<span>the item itself is initialized</span>
</li>
<li>
<span>there</span>&rsquo;<span>s the relevant happens-before established</span>
</li>
</ul>
<p><span>If, instead, we manufacture an index out of thin air, we might hit all kinds of nasty behavior without any bullet-proof way to check that.</span></p>
<p><span>Okay, now that we have this </span><code>SegmentList</code><span>, how would we use them?</span></p>
<p><span>Recall that our simplified value is</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">const</span> Value = <span class="hl-keyword">union</span>(<span class="hl-keyword">enum</span>) {</code>
<code>    <span class="hl-comment">// A bunch of payload-less variants.</span></code>
<code>    u1_type,</code>
<code>    u8_type,</code>
<code>    i8_type,</code>
<code></code>
<code>    <span class="hl-comment">// A number.</span></code>
<code>    <span class="hl-type">u64</span>: <span class="hl-type">u64</span>,</code>
<code></code>
<code>    <span class="hl-comment">// A declaration.</span></code>
<code>    <span class="hl-comment">// Declarations and types are also values in Zig.</span></code>
<code>    decl: Decl,</code>
<code></code>
<code>    <span class="hl-comment">// Just some bytes for a string.</span></code>
<code>    bytes: []<span class="hl-type">u8</span>,</code>
<code></code>
<code>    <span class="hl-comment">// The interesting case which makes it a tree.</span></code>
<code>    <span class="hl-comment">// This is how struct instances are represented.</span></code>
<code>    aggregate: []Value,</code>
<code>};</code>
<code></code>
<code><span class="hl-comment">// Index of a declaration.</span></code>
<code><span class="hl-keyword">const</span> Decl = <span class="hl-type">u32</span>;</code></pre>

</figure>
<p><span>Of course we will struct-of-array it now, to arrive at something like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">const</span> Value = <span class="hl-type">u32</span>;</code>
<code></code>
<code><span class="hl-keyword">const</span> Tag = <span class="hl-keyword">enum</span>(<span class="hl-type">u8</span>) {</code>
<code>    u1_type, u8_type, i8_type,</code>
<code>    <span class="hl-type">u64</span>, decl, bytes, aggregate,</code>
<code>};</code>
<code></code>
<code><span class="hl-keyword">const</span> ValueTable = <span class="hl-keyword">struct</span> {</code>
<code>    tag: SegmentList(Tag),</code>
<code>    data: SegmentList(<span class="hl-type">u32</span>),</code>
<code></code>
<code>    <span class="hl-type">u64</span>: SegmentList(<span class="hl-type">u64</span>),</code>
<code>    aggregate: SegmentList([]Value),</code>
<code>    bytes: SegmentList([]<span class="hl-type">u8</span>),</code>
<code>};</code></pre>

</figure>
<p><span>A </span><code>Value</code><span> is now an index.</span>
<span>This index works for two fields of </span><code>ValueTable</code><span>, </span><code>tag</code><span> and </span><code>data</code><span>.</span>
<span>That is, the index addresses five bytes of payload, which is all that is needed for small values.</span>
<span>For large tags like </span><code>aggregate</code><span>, the </span><code>data</code><span> field stores an index into the corresponding payload </span><code>SegmentList</code><span>.</span></p>
<p><span>That is, every value allocates a </span><code>tag</code><span> and </span><code>data</code><span> elements, but only actual </span><code>u64</code><span>s occupy a slot in </span><code>u64</code><span> </span><code>SegmentList</code><span>.</span></p>
<p><span>So now we can write a </span><code>lookup</code><span> function which takes a value index and reconstructs a value from pieces:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">const</span> ValueFull = <span class="hl-keyword">union</span>(<span class="hl-keyword">enum</span>) {</code>
<code>    u1_type,</code>
<code>    u8_type,</code>
<code>    i8_type,</code>
<code>    <span class="hl-type">u64</span>: <span class="hl-type">u64</span>,</code>
<code>    decl: Decl,</code>
<code>    bytes: []<span class="hl-type">u8</span>,</code>
<code>    aggregate: []Value,</code>
<code>};</code>
<code></code>
<code><span class="hl-keyword">fn</span><span class="hl-function"> lookup</span>(self: Self, value: Value) ValueFull {</code>
<code>    <span class="hl-keyword">const</span> tag = self.tag.get(value);</code>
<code>    <span class="hl-keyword">switch</span> (tag) {</code>
<code>        .aggregate =&gt; <span class="hl-keyword">return</span> ValueFull{</code>
<code>            .aggregate = self.aggregate.get(self.data(value)),</code>
<code>        },</code>
<code>    }</code>
<code>}</code></pre>

</figure>
<p><span>Note that here </span><code>ValueFull</code><span> is non-owning type, it is a </span><em><span>reference</span></em><span> into the actual data.</span>
<span>Note as well that aggregates now store a slice of indexes, rather than a slice of pointers.</span></p>
<p><span>Now let</span>&rsquo;<span>s deal with creating and interning values.</span>
<span>We start by creating a </span><code>ValueFull</code><span> using data owned by us</span>
<span>(e.g. if we are creating an aggregate, we may use a stack-allocated array as a backing store for </span><code>[]Value</code><span> slice).</span>
<span>Then we ask </span><code>ValueTable</code><span> to intern the data:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span><span class="hl-function"> intern</span>(self: <span class="hl-operator">*</span>Self, value_full: ValueFull) Value {</code>
<code>}</code></pre>

</figure>
<p><span>If the table already contains an equal value, its index is returned.</span>
<span>Otherwise, the table </span><em><span>copies</span></em><span> </span><code>ValueFull</code><span> data such that it is owned by the table itself, and returns a freshly allocated index.</span></p>
<p><span>For bookkeeping, we</span>&rsquo;<span>ll need a hash table with existing values and a counter to use for a fresh index, something like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">const</span> ValueTable = <span class="hl-keyword">struct</span> {</code>
<code>    value_set: AutoHashMapUnmanaged(Value, <span class="hl-type">void</span>),</code>
<code>    value_count: <span class="hl-type">u32</span>,</code>
<code>    tag: SegmentList(Tag),</code>
<code>    index: SegmentList(<span class="hl-type">u32</span>),</code>
<code></code>
<code>    u64_count: <span class="hl-type">u32</span>,</code>
<code>    <span class="hl-type">u64</span>: SegmentList(<span class="hl-type">u64</span>),</code>
<code></code>
<code>    aggregate_count: <span class="hl-type">u32</span>,</code>
<code>    aggregate: SegmentList([]Value),</code>
<code></code>
<code>    bytes_count: <span class="hl-type">u32</span>,</code>
<code>    bytes: SegmentList([]<span class="hl-type">u8</span>),</code>
<code></code>
<code>    <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span><span class="hl-function"> intern</span>(self: <span class="hl-operator">*</span>Self, value_full: ValueFull) Value {</code>
<code>        ...</code>
<code>    }</code>
<code>};</code></pre>

</figure>
<p><span>Pay attention to </span><code>_count</code><span> fields </span>&mdash;<span> we have </span><code>value_count</code><span> guarding the </span><code>tag</code><span> and </span><code>index</code><span>, and separate counts for specific kinds of values, as we don</span>&rsquo;<span>t want to allocate, e.g. an </span><code>u64</code><span> for </span><em><span>every</span></em><span> value.</span></p>
<p><span>Our hashmap is actually a set which stores </span><code>u32</code><span> integers, but uses </span><code>ValueFull</code><span> to do a lookup: when we consider interning a new </span><code>ValueFull</code><span>, we don</span>&rsquo;<span>t know its index yet.</span>
<span>Luckily, </span><code>getOrPutAdapted</code><span> API provides the required flexibility.</span>
<span>We can use it to compare a </span><code>Value</code><span> (index) and a </span><code>ValueFull</code><span> by hashing a </span><code>ValueFull</code><span> and doing component-wise comparisons in the case of a collision.</span></p>
<p><span>Note that, because of interning, we can also hash </span><code>ValueFull</code><span> efficiently!</span>
<span>As any subvalues in </span><code>ValueFull</code><span> are guaranteed to be already interned, we can rely on shallow hash and hash only child value</span>&rsquo;<span>s indexes, rather than their data.</span></p>
<p><span>This is a nice design for a single thread, but how do we make it thread safe?</span>
<span>The straightforward solution would be to slap a mutex around the logic in </span><code>intern</code><span>.</span></p>
<p><span>This actually is not as bad as it seems, as we</span>&rsquo;<span>d need a lock only in </span><code>intern</code><span>, and </span><code>lookup</code><span> would work without any synchronization whatsoever.</span>
<span>Recall that obtaining an index of a value is a proof that the value was properly published.</span>
<span>Still, we expect to intern a lot of values, and that mutex is all but guaranteed to become a point of contention.</span>
<span>And some amount of contention is inevitable here </span>&mdash;<span> if two threads try to intern two identical values, we </span><em><span>want</span></em><span> them to clash, communicate, and end up with a single, shared value.</span></p>
<p><span>There</span>&rsquo;<span>s a rather universal recipe for dealing with contention </span>&mdash;<span> you can shard the data.</span>
<span>In our case, rather than using something like</span></p>

<figure class="code-block">


<pre><code>mutex: Mutex,</code>
<code>value_set: AutoHashMapUnmanaged(Value, <span class="hl-type">void</span>),</code></pre>

</figure>
<p><span>we can do</span></p>

<figure class="code-block">


<pre><code>mutex: [<span class="hl-numbers">16</span>]Mutex,</code>
<code>value_set: [<span class="hl-numbers">16</span>]AutoHashMapUnmanaged(Value, <span class="hl-type">void</span>),</code></pre>

</figure>
<p><span>That is, we create not one, but sixteen hashmaps, and use, e.g., lower 4 bits of the hash to decide which mutex and hashmap to use.</span>
<span>Depending on the structure of the hashmap, such locks could even be pushed as far as individual buckets.</span></p>
<p><span>This doesn</span>&rsquo;<span>t solve all our contention problems </span>&mdash;<span> now that several threads can simultaneously intern values (as long as they are hashed into different shards) we have to make all </span><code>count</code><span> variables atomic.</span>
<span>So we essentially moved the single global point of contention from a mutex to </span><code>value_count</code><span> field, which is incremented for every interned value.</span></p>
<p><span>We can apply the sharding trick again, and shard all our </span><code>SegmentList</code><span>s.</span>
<span>But that would mean that we have to dedicate some bits from </span><code>Value</code><span> index to the shard number, and to waste some extra space for non-perfectly balanced shards.</span></p>
<p><span>There</span>&rsquo;<span>s a better way </span>&mdash;<span> we can amortize atomic increments by allowing each thread to bulk-allocate indexes.</span>
<span>That is, if a thread wants to allocate a new value, it atomically increments </span><code>value_cont</code><span> by, say, </span><code>1024</code><span>, and uses those indexes for the next thousand allocations.</span>
<span>In addition to </span><code>ValueTable</code><span>, each thread now gets its own distinct </span><code>LocalTable</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">const</span> LocalTable = <span class="hl-keyword">struct</span> {</code>
<code>    global: <span class="hl-operator">*</span>ValueTable,</code>
<code></code>
<code>    <span class="hl-comment">// Invariant: if any `index % 1024 == 0`,</span></code>
<code>    <span class="hl-comment">// it&#x27;s time to visit `global` to</span></code>
<code>    <span class="hl-comment">// refill our budget via atomic fetchAndAdd.</span></code>
<code>    value_index: <span class="hl-type">u32</span>,</code>
<code>    u64_index: <span class="hl-type">u32</span>,</code>
<code>    aggregate_index: <span class="hl-type">u32</span>,</code>
<code>    bytes_index: <span class="hl-type">u32</span>,</code>
<code>};</code></pre>

</figure>
<p><span>An attentive reader would notice a bonus here: in this setup, a thread allocates a contiguous chunk of values.</span>
<span>It is reasonable to assume that values allocated together would also be used together, so we potentially increase future spatial locality here.</span></p>
<p><span>Putting everything together, the pseudo-code for interning would look like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span><span class="hl-function"> intern</span>(table: <span class="hl-operator">*</span>LocalTable, value_full: ValueFull) Value {</code>
<code>    <span class="hl-keyword">const</span> hash = shallow_hash(value_full);</code>
<code></code>
<code>    <span class="hl-comment">// Find &amp; lock the shard.</span></code>
<code>    <span class="hl-keyword">const</span> shard = hash <span class="hl-operator">&amp;</span> <span class="hl-numbers">0xF</span>;</code>
<code>    let mutex = <span class="hl-operator">&amp;</span>table.global.mutex[shard];</code>
<code>    let value_set = <span class="hl-operator">&amp;</span>table.global.value_set[shard]</code>
<code></code>
<code>    mutex.lock();</code>
<code>    <span class="hl-keyword">defer</span> mutex.unlock();</code>
<code></code>
<code>    <span class="hl-comment">// Either find that this value has been interned already...</span></code>
<code>    <span class="hl-keyword">const</span> gop = value_set.get_or_put(hash, value_full, ...);</code>
<code>    <span class="hl-keyword">if</span> (gop.found_existing) <span class="hl-keyword">return</span> got.key_ptr.<span class="hl-operator">*</span>;</code>
<code></code>
<code>    <span class="hl-comment">// ... or proceed to allocate a new index for it</span></code>
<code></code>
<code>    <span class="hl-keyword">if</span> (table.tag_index <span class="hl-operator">&amp;</span> <span class="hl-numbers">0xFF</span> <span class="hl-operator">==</span> <span class="hl-numbers">0</span>) {</code>
<code>        <span class="hl-comment">// Run out of indexes, refill our budget!</span></code>
<code>        table.tag_index = <span class="hl-built_in">@atomicRmw</span>(</code>
<code>            <span class="hl-type">u32</span>, <span class="hl-operator">&amp;</span>table.global.value_count,</code>
<code>            .Add, <span class="hl-numbers">0xFF</span>,</code>
<code>            .Relaxed,</code>
<code>        );</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-comment">// Assign the index to the new value</span></code>
<code>    <span class="hl-comment">// and put it into the hash map.</span></code>
<code>    <span class="hl-keyword">const</span> value = table.tag_index;</code>
<code>    table.tag_index <span class="hl-operator">+=</span> <span class="hl-numbers">1</span>;</code>
<code>    gop.key_ptr.<span class="hl-operator">*</span> = value;</code>
<code></code>
<code>    <span class="hl-comment">// Now initialize the value.</span></code>
<code>    <span class="hl-comment">// Note that we still hold shard&#x27;s mutex at this point.</span></code>
<code></code>
<code>    <span class="hl-keyword">switch</span> (value_full) {</code>
<code>        .aggregate =&gt; <span class="hl-operator">|</span>fields<span class="hl-operator">|</span> {</code>
<code>            <span class="hl-comment">// Initialize the tag, common for all values.</span></code>
<code>            table.global.tag.set(value, .aggregate);</code>
<code></code>
<code>            <span class="hl-comment">// Allocate tag-specific data using</span></code>
<code>            <span class="hl-comment">// the same atomic add trick.</span></code>
<code>            <span class="hl-keyword">if</span> (table.aggregate_index <span class="hl-operator">&amp;</span> <span class="hl-numbers">0xFF</span> <span class="hl-operator">==</span> <span class="hl-numbers">0</span>) {</code>
<code>                table.aggregate_index = <span class="hl-built_in">@atomicRmw</span>(</code>
<code>                    <span class="hl-type">u32</span>, <span class="hl-operator">&amp;</span>table.global.aggregate_count,</code>
<code>                    .Add, <span class="hl-numbers">0xFF</span>,</code>
<code>                    .Relaxed,</code>
<code>                );</code>
<code>            }</code>
<code>            <span class="hl-keyword">const</span> index = table.aggregate_index;</code>
<code>            table.aggregate_index <span class="hl-operator">+=</span> <span class="hl-numbers">1</span>;</code>
<code></code>
<code>            <span class="hl-comment">// Make it possible to find tag-specific data</span></code>
<code>            <span class="hl-comment">// from the value index.</span></code>
<code>            table.global.index.set(value, index);</code>
<code></code>
<code>            <span class="hl-comment">// `value_full` is borrowed, so we must</span></code>
<code>            <span class="hl-comment">// create a copy that we own.</span></code>
<code>            <span class="hl-keyword">const</span> fields_owned = allocator.dup(fields)</code>
<code>                <span class="hl-keyword">catch</span> <span class="hl-keyword">unreachable</span>;</code>
<code></code>
<code>            table.global.aggregate.set(index, fields_owned);</code>
<code>        }</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-keyword">return</span> value;</code>
<code>}</code>
<code></code>
<code><span class="hl-comment">// Code for assigning an index of a SegmentList.</span></code>
<code><span class="hl-comment">// Shard&#x27;s mutex guarantees exclusive access to the index.</span></code>
<code><span class="hl-comment">// Accesses to the echelon might race though.</span></code>
<code><span class="hl-keyword">fn</span><span class="hl-function"> set</span>(list: SegmentList(T), index: <span class="hl-type">u32</span>, value: T) {</code>
<code>    <span class="hl-keyword">const</span> e = list.get_echelon(index);</code>
<code>    <span class="hl-keyword">const</span> i = index <span class="hl-operator">-</span> ((<span class="hl-numbers">1</span> <span class="hl-operator">&lt;&lt;</span> e) <span class="hl-operator">-</span> <span class="hl-numbers">1</span>);</code>
<code></code>
<code>    <span class="hl-keyword">var</span> echelon = <span class="hl-built_in">@atomicLoad</span>(?[<span class="hl-operator">*</span>]T, <span class="hl-operator">&amp;</span>list.echelons[e], .Acquire);</code>
<code>    <span class="hl-keyword">if</span> (echelon <span class="hl-operator">==</span> <span class="hl-literal">null</span>) {</code>
<code>        <span class="hl-comment">// Race with other threads to allocate the echelon.</span></code>
<code>        <span class="hl-keyword">const</span> echelon_new = allocator.alloc(T, <span class="hl-numbers">1</span> <span class="hl-operator">&lt;&lt;</span> e)</code>
<code>            <span class="hl-keyword">catch</span> <span class="hl-keyword">unreachable</span>;</code>
<code></code>
<code>        <span class="hl-keyword">const</span> modified = <span class="hl-built_in">@cmpxchgStrong</span>(</code>
<code>            ?[<span class="hl-operator">*</span>]T, <span class="hl-operator">&amp;</span>list.echelons[e],</code>
<code>            <span class="hl-literal">null</span>, echelon_new,</code>
<code>            .Release, .Acquire,</code>
<code>        );</code>
<code></code>
<code>        <span class="hl-keyword">if</span> (modified) <span class="hl-operator">|</span>echelon_modified<span class="hl-operator">|</span> {</code>
<code>            <span class="hl-comment">// Another thread won, free our useless allocation.</span></code>
<code>            echelon = echelon_modified</code>
<code>            allocator.free(echelon_new);</code>
<code>        } <span class="hl-keyword">else</span> {</code>
<code>            echelon = echelon_new;</code>
<code>        }</code>
<code>    }</code>
<code></code>
<code>    echelon.?[i] = value;</code>
<code>}</code></pre>

</figure>
<p><span>Note that it is important that we </span><em><span>don</span>&rsquo;<span>t</span></em><span> release the mutex immediately after assigning the index for a value, but rather keep it locked all the way until we fully copied thee value into the </span><code>ValueTable</code><span>.</span>
<span>If we release the lock earlier, a different thread which tries to intern the same value would get the correct index, but would risk accessing partially-initialized data.</span>
<span>This can be optimized a bit by adding value-specific lock (or rather, a </span><a href="https://github.com/ziglang/zig/blob/b95cdf0aeb4d4d31c0b6a54302ef61baec8f6773/lib/std/once.zig"><code>Once</code></a><span>).</span>
<span>So we use the shard lock to assign an index, then release the shard lock, and use value-specific lock to do the actual (potentially slow) initialization.</span></p>
<p><span>And that</span>&rsquo;<span>s all I have for today!</span>
<span>Again, I haven</span>&rsquo;<span>t implemented this, so I have no idea how fast or slow it actually is.</span>
<span>But the end result looks rather beautiful, and builds upon many interesting ideas:</span></p>
<ul>
<li>
<p><code>SegmentList</code><span> allows to maintain index stability despite insertions.</span></p>
</li>
<li>
<p><span>There will be at most 31 echelons in a </span><code>SegmentList</code><span>, so you can put pointes to them into an array, removing the need to synchronize to read an echelon.</span></p>
</li>
<li>
<p><span>With this setup, it becomes easy to initialize a new echelon with a single CAS.</span></p>
</li>
<li>
<p><span>Synchronization is required only when creating a new item.</span>
<span>If you trust indexes, you can use them to carry happens-before.</span></p>
</li>
<li>
<p><span>In a struct-of-arrays setup for enums, you can save space by requiring that an array for a specific variant is just as long as it needs to be.</span></p>
</li>
<li>
<p><span>One benefit of interning trees is that hash function becomes a shallow operation.</span></p>
</li>
<li>
<p><span>Optimal interners use hashmaps in a fancy way, where the key is not what you actually store in the hashmap.</span>
<span>I have two related posts about that,</span>
<a href="https://matklad.github.io/2020/03/22/fast-simple-rust-interner.html"><em><span>Fast and Simple Rust Interner</span></em></a><span> and</span>
<a href="https://matklad.github.io/2020/12/28/csdi.html"><em><span>Call Site Dependency Injection</span></em></a><span>.</span></p>
</li>
<li>
<p><span>Sharding is an effective way to reduce contention if you are dealing with something like a shared hashmap.</span></p>
</li>
<li>
<p><span>For counters, one alternative to sharding is batching up the increments.</span></p>
</li>
</ul>
<p><span>Discussion on </span><a href="https://old.reddit.com/r/Zig/"><span>/r/Zig</span></a><span>.</span></p>
]]></content>
</entry>

<entry>
<title type="text">Reasonable Bootstrap</title>
<link href="https://matklad.github.io/2023/04/13/reasonable-bootstrap.html" rel="alternate" type="text/html" title="Reasonable Bootstrap" />
<published>2023-04-13T00:00:00+00:00</published>
<updated>2023-04-13T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/04/13/reasonable-bootstrap</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Compilers for systems programming languages (C, C++, Rust, Zig) tend to be implemented in the languages themselves.
The idea being that the current version of the compiler is built using some previous version.
But how can you get a working compiler if you start out from nothing?]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/04/13/reasonable-bootstrap.html"><![CDATA[
    <h1>
    <a href="#Reasonable-Bootstrap"><span>Reasonable Bootstrap</span> <time datetime="2023-04-13">Apr 13, 2023</time></a>
    </h1>
<p><span>Compilers for systems programming languages (C, C++, Rust, Zig) tend to be implemented in the languages themselves.</span>
<span>The idea being that the current version of the compiler is built using some previous version.</span>
<span>But how can you get a working compiler if you start out from nothing?</span></p>
<p><span>The traditional answer has been </span>&ldquo;<span>via bootstrap chain</span>&rdquo;<span>.</span>
<span>You start with the first version of the compiler implemented in assembly, use that to compile the latest version of the compiler it is capable of compiling, then repeat.</span>
<span>This historically worked OK because older versions of GCC were implemented in C (and C is easy to provide a compiler for) and, even today, GCC itself is very conservative in using language features.</span>
<span>I believe GCC 10.4 released in 2022 can be built with just a C++98 compiler.</span>
<span>So, if you start with a C compiler, it</span>&rsquo;<span>s not too many hops to get to the latest GCC.</span></p>
<p><span>This doesn</span>&rsquo;<span>t feel entirely satisfactory, as this approach requires artificially constraining the compiler itself to be very conservative.</span>
<span>Rust does the opposite of that.</span>
<span>Rust requires that rustc 1.x.0 is built by rustc 1.x-1.0, and there</span>&rsquo;<span>s a new rustc version every six weeks.</span>
<span>This seems like a very reasonable way to build compilers, </span><em><span>but</span></em><span> it also is incompatible with chain bootstrapping.</span>
<span>In the limit, one would need infinite time to compile modern rustc ex nihilo!</span></p>
<p><span>I </span><em><span>think</span></em><span> there</span>&rsquo;<span>s a better way if the goal is to compile the world from nothing.</span>
<span>To cut to the chase, the minimal bootstrap seed for Rust could be:</span></p>
<ul>
<li>
<span>source code for current version of the compiler</span>
</li>
<li>
<span>this source code compiled to core WebAssembly</span>
</li>
</ul>
<p><span>Bootstrapping from this should be easy.</span>
<span>WebAssembly is a very small language, so a runtime for it can be built out of nothing.</span>
<span>Using this runtime, and rustc-compiled-to-wasm we can re-compile rustc itself.</span>
<span>Then, we can either cross-compile it to the architecture we need, if that architecture is supported by rustc.</span>
<span>If the architecture is </span><em><span>not</span></em><span> supported, we can implement a new backend for that arch in Rust, compile our modified compiler to wasm, and then cross-compile to the desired target.</span></p>
<p><span>More complete bootstrap seed would include:</span></p>
<ul>
<li>
<span>Informal specification of the Rust language, to make sense of the source code.</span>
</li>
<li>
<span>Rust source code for the compiler, which also doubles as a formal specification of the language.</span>
</li>
<li>
<span>Informal specification of WebAssembly, to make sense of .wasm parts of the bootstrap seed.</span>
</li>
<li>
<span>.wasm code for the rust compiler, which triple-checks the Rust specification.</span>
</li>
<li>
<span>Rust implementation of a WebAssembly interpreter, which doubles as a formal spec for WebAssembly.</span>
</li>
</ul>
<p><span>And this seed is provided for every version of a language.</span>
<span>This way, it is possible to bootstrap, in constant time, any version of Rust.</span></p>
<p><span>Specific properties we use for this setup:</span></p>
<ul>
<li>
<span>Compilation is deterministic.</span>
<span>Compiling bootstrap sources with bootstrap .wasm blob should result in a byte-for-byte identical wasm blob.</span>
</li>
<li>
<span>WebAssembly is target-agnostic.</span>
<span>It describes abstract computation, which is completely independent from the host architecture.</span>
</li>
<li>
<span>WebAssembly is simple.</span>
<span>Implementing a WebAssembly interpreter is easy in whatever computation substrate you have.</span>
</li>
<li>
<span>Compiler is a cross compiler.</span>
<span>We don</span>&rsquo;<span>t want to bootstrap </span><em><span>just</span></em><span> the WebAssembly backend, we want to bootstrap everything.</span>
<span>This requires that the WebAssembly version of the compiler can generate the code for arbitrary architectures.</span>
</li>
</ul>
<p><span>This setup does not prevent the trusting trust attack.</span>
<span>However, it is possible to rebuild the bootstrap seed using a different compiler.</span>
<span>Using that compiler to compiler rustc to .wasm will produce a different blob.</span>
<span>But using that .wasm to recompile rustc again should produce the blob from the seed (unless, of course, there</span>&rsquo;<span>s a trojan in the seed).</span></p>
<p><span>This setup does not minimize the size of opaque binary blobs in the seed.</span>
<span>The size of the .wasm would be substantial.</span>
<span>This setup, however, does minimize the total size of the seed.</span>
<span>In the traditional bootstrap, source code for rustc 1.0.0, rustc 1.1.0, rustc 1.2.0, etc would also have to be part of the seed.</span>
<span>For the suggested approach, you need only one version, at the cost of a bigger binary blob.</span></p>
<p><span>This idea is not new.</span>
<span>I </span><em><span>think</span></em><span> it was popularized by Pascal with p-code.</span>
<span>OCaml uses a similar strategy.</span>
<span>Finally, </span><a href="https://ziglang.org/news/goodbye-cpp/"><span>Zig</span></a><span> makes an important observation that we no longer need to implement language-specific virtual machines, because WebAssembly is a good fit for the job.</span></p>
]]></content>
</entry>

<entry>
<title type="text">Can You Trust a Compiler to Optimize Your Code?</title>
<link href="https://matklad.github.io/2023/04/09/can-you-trust-a-compiler-to-optimize-your-code.html" rel="alternate" type="text/html" title="Can You Trust a Compiler to Optimize Your Code?" />
<published>2023-04-09T00:00:00+00:00</published>
<updated>2023-04-09T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/04/09/can-you-trust-a-compiler-to-optimize-your-code</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[More or less the title this time, but first, a story about SIMD. There are three
levels of understanding how SIMD works (well, at least I am level 3 at the moment):]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/04/09/can-you-trust-a-compiler-to-optimize-your-code.html"><![CDATA[
    <h1>
    <a href="#Can-You-Trust-a-Compiler-to-Optimize-Your-Code"><span>Can You Trust a Compiler to Optimize Your Code?</span> <time datetime="2023-04-09">Apr 9, 2023</time></a>
    </h1>
<p><span>More or less the title this time, but first, a story about SIMD. There are three</span>
<span>levels of understanding how SIMD works (well, at least I am level 3 at the moment):</span></p>
<ol>
<li>
<p><span>Compilers are smart! They will auto-vectorize all the code!</span></p>
</li>
<li>
<p><span>Compilers are dumb, auto-vectorization is fragile, it</span>&rsquo;<span>s very easy to break it</span>
<span>by unrelated changes to the code. It</span>&rsquo;<span>s always better to manually write</span>
<span>explicit SIMD instructions.</span></p>
</li>
<li>
<p><span>Writing SIMD by hand is really hard </span>&mdash;<span> you</span>&rsquo;<span>ll need to re-do the work for</span>
<span>every different CPU architecture. Also, you probably think that, for scalar</span>
<span>code, a compiler writes better assembly than you. What makes you think that</span>
<span>you</span>&rsquo;<span>d beat the compiler at SIMD, where there are more funky instructions and</span>
<span>constraints? Compilers are tools. They can reliably vectorize code if it is</span>
<span>written in an amenable-to-vectorization form.</span></p>
</li>
</ol>
<p><span>I</span>&rsquo;<span>ve recently moved from the second level to the third one, and that made me aware of the moment when the model used by a compiler for optimization clicked in my head.</span>
<span>In this post, I want to explain the general framework for reasoning about compiler optimizations for static languages such as Rust or C++.</span>
<span>After that, I</span>&rsquo;<span>ll apply that framework to auto-vectorization.</span></p>
<p><span>I haven</span>&rsquo;<span>t worked on backends of production optimizing compilers, so the following will not be academically correct, but these models are definitely helpful at least to me!</span></p>
<section id="Seeing-Like-a-Compiler">

    <h2>
    <a href="#Seeing-Like-a-Compiler"><span>Seeing Like a Compiler</span> </a>
    </h2>
<p><span>The first bit of a puzzle is understanding how a compiler views code. Some useful references here include</span>
<a href="https://link.springer.com/book/10.1007/978-3-030-80515-9"><em><span>The SSA Book</span></em></a><span> or LLVM</span>&rsquo;<span>s</span>
<a href="https://llvm.org/docs/LangRef.html"><em><span>Language Reference</span></em></a><span>.</span></p>
<p><span>Another interesting choice would be </span><a href="https://webassembly.github.io/spec/core/"><em><span>WebAssembly Specification</span></em></a><span>.</span>
<span>While WASM would be a poor IR for an optimizing compiler, it has a lot of structural similarities, and the core spec is exceptionally readable.</span></p>
<p><span>A unit of optimization is a function.</span>
<span>Let</span>&rsquo;<span>s take a simple function like the following:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">sum</span>(xs: &amp;[<span class="hl-type">i32</span>]) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">i32</span> {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">total</span> = <span class="hl-number">0</span>;</code>
<code>  <span class="hl-keyword">for</span> <span class="hl-variable">i</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..xs.<span class="hl-title function_ invoke__">len</span>() {</code>
<code>    total = total.<span class="hl-title function_ invoke__">wrapping_add</span>(xs[i]);</code>
<code>  }</code>
<code>  total</code>
<code>}</code></pre>

</figure>
<p><span>In some pseudo-IR, it would look like this:</span></p>

<figure class="code-block">


<pre><code>fn sum return i32 {</code>
<code>  param xs_ptr: ptr</code>
<code>  param xs_len: size</code>
<code></code>
<code>  local total: i32</code>
<code>  local i: size = 0</code>
<code>  local x: i32</code>
<code>  local total: i32 = 0</code>
<code></code>
<code>loop:</code>
<code>  branch_if i &gt;= xs_len :ret</code>
<code>  load x base=xs_ptr offset=i</code>
<code>  add total x</code>
<code>  add i 1</code>
<code>  goto :loop</code>
<code></code>
<code>ret:</code>
<code>  return total</code>
<code>}</code></pre>

</figure>
<p><span>The most important characteristic here is that there are two kinds of entities:</span></p>
<p><em><span>First</span></em><span>, there is program memory, very roughly an array of bytes.</span>
<span>Compilers generally can not reason about the contents of the memory very well, because it is shared by all the functions, and different functions might interpret the contents of the memory differently.</span></p>
<p><em><span>Second</span></em><span>, there are local variables.</span>
<span>Local variables are not bytes </span>&mdash;<span> they are integers, they obey mathematical properties which a compiler can reason about.</span></p>
<p><span>For example, if a compiler sees a loop like</span></p>

<figure class="code-block">


<pre><code>param n: u32</code>
<code>local i: u32 = 0</code>
<code>local total: u32</code>
<code>local tmp</code>
<code></code>
<code>loop:</code>
<code>  branch_if i &gt;= n :ret</code>
<code>  set tmp i</code>
<code>  mul tmp 4</code>
<code>  add t tmp</code>
<code>  goto :loop</code>
<code></code>
<code>ret:</code>
<code>  return total</code></pre>

</figure>
<p><span>It can </span><em><span>reason</span></em><span> that on each iteration </span><code>tmp</code><span> holds </span><code>i * 4</code><span> and optimize the code to</span></p>

<figure class="code-block">


<pre><code>param n: u32</code>
<code>local i: u32 = 0</code>
<code>local total: u32</code>
<code>local tmp = 0</code>
<code></code>
<code>loop:</code>
<code>  branch_if i &gt;= n :ret</code>
<code>  add t tmp</code>
<code>  add tmp 4  # replace multiplication with addition</code>
<code>  goto :loop</code>
<code></code>
<code>ret:</code>
<code>  return total</code></pre>

</figure>
<p><span>This works, because all locals are just numbers.</span>
<span>If we did the same computation, but all numbers were located in memory, it would be significantly harder for a compiler to reason that the transformation is actually correct.</span>
<span>What if the storage for </span><code>n</code><span> and </span><code>total</code><span> actually overlaps?</span>
<span>What if </span><code>tmp</code><span> overlaps with something which isn</span>&rsquo;<span>t even in the current function?</span></p>
<p><span>However, there</span>&rsquo;<span>s a bridge between the worlds of mathematical local variables and the world of memory bytes </span>&mdash;<span> </span><code>load</code><span> and </span><code>store</code><span> instructions.</span>
<span>The </span><code>load</code><span> instruction takes a range of bytes in memory, interprets the bytes as an integer, and stores that integer into a local variable.</span>
<span>The </span><code>store</code><span> instruction does the opposite.</span>
<span>By loading something from memory into a local, a compiler gains the ability to reason about it precisely.</span>
<span>Thus, the compiler doesn</span>&rsquo;<span>t need to track the general contents of memory.</span>
<span>It only needs to check that it would be correct to load from memory at a specific point in time.</span></p>
<p><span>So, a compiler really doesn</span>&rsquo;<span>t see all that well </span>&mdash;<span> it can only really reason about a single function at a time, and only about the local variables in that function.</span></p>
</section>
<section id="Bringing-Code-Closer-to-Compiler-s-Nose">

    <h2>
    <a href="#Bringing-Code-Closer-to-Compiler-s-Nose"><span>Bringing Code Closer to Compiler</span>&rsquo;<span>s Nose</span> </a>
    </h2>
<p><span>Compilers are myopic.</span>
<span>This can be fixed by giving more context to the compiler, which is the task of two core optimizations.</span></p>
<p><em><span>The first</span></em><span> core optimization is </span><dfn><span>inlining</span></dfn><span>.</span>
<span>It substitutes callee</span>&rsquo;<span>s body for a specific call.</span>
<span>The benefit here is not that we eliminate function call overhead, that</span>&rsquo;<span>s relatively minor.</span>
<span>The big thing is that locals of both the caller and the callee are now in the same frame, and a compiler can optimize them together.</span></p>
<p><span>Let</span>&rsquo;<span>s look again at that Rust code:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">sum</span>(xs: &amp;[<span class="hl-type">i32</span>]) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">i32</span> {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">total</span> = <span class="hl-number">0</span>;</code>
<code>  <span class="hl-keyword">for</span> <span class="hl-variable">i</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..xs.<span class="hl-title function_ invoke__">len</span>() {</code>
<code>    total = total.<span class="hl-title function_ invoke__">wrapping_add</span>(xs[i]);</code>
<code>  }</code>
<code>  total</code>
<code>}</code></pre>

</figure>
<p><span>The </span><code>xs[i]</code><span> expression there is actually a function call.</span>
<span>The indexing function does a bounds check before accessing the element of an array.</span>
<span>After inlining it into the </span><code>sum</code><span>, compiler can see that it is dead code and eliminate it.</span></p>
<p><span>If you look at various standard optimizations, they often look like getting rid of dumb things, which no one would actually write in the first place, so its not clear immediately if it is worth it to implement such optimizations.</span>
<span>But the thing is, after inlining a lot of dumb things appear, because functions tend to handle the general case, and, at a specific call-site, there are usually enough constraints to dismiss many edge cases.</span></p>
<p><em><span>The second</span></em><span> core optimization is </span><dfn><span>scalar replacement of aggregates</span></dfn><span>.</span>
<span>It is a generalization of the </span>&ldquo;<span>let</span>&rsquo;<span>s use </span><code>load</code><span> to avoid reasoning about memory and reason about a local instead</span>&rdquo;<span> idea we</span>&rsquo;<span>ve already seen.</span></p>
<p><span>If you have a function like</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">fn</span> <span class="hl-title function_">permute</span>(xs: &amp;<span class="hl-keyword">mut</span> <span class="hl-type">Vec</span>&lt;<span class="hl-type">i32</span>&gt;) {</code>
<code>  ...</code>
<code>}</code></pre>

</figure>
<p><span>it</span>&rsquo;<span>s pretty difficult for the compiler to reason about it.</span>
<span>It receives a pointer to some memory which holds a complex struct (ptr, len, capacity triple), so reasoning about evolution of this struct is hard.</span>
<span>What the compiler can do is to load this struct from memory, replacing the aggregate with a bunch of scalar local variables:</span></p>

<figure class="code-block">


<pre><code>fn permute(xs: &amp;mut Vec&lt;i32&gt;) {</code>
<code>  local ptr: ptr</code>
<code>  local len: usize</code>
<code>  local cap: usize</code>
<code></code>
<code>  load ptr xs.ptr</code>
<code>  load len xs.len</code>
<code>  load cap xs.cap</code>
<code></code>
<code>  ...</code>
<code></code>
<code>  store xs.ptr ptr</code>
<code>  store xs.len len</code>
<code>  store xs.cap cap</code>
<code>}</code></pre>

</figure>
<p><span>This way, a compiler again gains reasoning power.</span>
<span>SROA is like inlining, but for memory rather than code.</span></p>
</section>
<section id="Impossible-and-Possible">

    <h2>
    <a href="#Impossible-and-Possible"><span>Impossible and Possible</span> </a>
    </h2>
<p><span>Using this mental model of a compiler which:</span></p>
<ul>
<li>
<span>optimizes on a per-function basis,</span>
</li>
<li>
<span>can inline function calls,</span>
</li>
<li>
<span>is great at noticing relations between local variables and rearranging the code based on that,</span>
</li>
<li>
<span>is capable of </span><em><span>limited</span></em><span> reasoning about the memory (namely, deciding when it</span>&rsquo;<span>s safe to </span><code>load</code><span> or </span><code>store</code><span>)</span>
</li>
</ul>
<p><span>we can describe which code is reliably optimizable, and which code prevents optimizations, explaining zero cost abstractions.</span></p>
<p><span>To enable inlining, a compiler needs to know which function is actually called.</span>
<span>If a function is called directly, it</span>&rsquo;<span>s pretty much guaranteed that a compiler would try to inline it.</span>
<span>If the call is indirect (via function pointer, or via a table of virtual functions), in the general case a compiler won</span>&rsquo;<span>t be able to inline that.</span>
<span>Even for indirect calls, sometimes the compiler can reason about the value of the pointer and de-virtualize the call, but that relies on successful optimization elsewhere.</span></p>
<p><span>This is the reason why, in Rust, every function has a unique, zero-sized type with no runtime representation.</span>
<span>It statically guarantees that the compiler could always inline the code, and makes this abstraction zero cost, because any decent optimizing compiler will melt it to nothing.</span></p>
<p><span>A higher level language might choose to </span><em><span>always</span></em><span> represent functions with function pointers.</span>
<span>In practice, in many cases the resulting code would be equivalently optimizable.</span>
<span>But there won</span>&rsquo;<span>t be any indication in the source whether this is an optimizable case (the actual pointer is knowable at compile time) or a genuinely dynamic call.</span>
<span>With Rust, the difference between guaranteed to be optimizable and potentially optimizable is reflected in the source language:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// Compiler is guaranteed to be able to inline call to `f`.</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">call1</span>&lt;F: <span class="hl-title function_ invoke__">Fn</span>()&gt;(f: F) {</code>
<code>  <span class="hl-title function_ invoke__">f</span>()</code>
<code>}</code>
<code></code>
<code><span class="hl-comment">// Compiler _might_ be able to inline call to `f`.</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">call2</span>(f: <span class="hl-title function_ invoke__">fn</span>()) {</code>
<code>  <span class="hl-title function_ invoke__">f</span>()</code>
<code>}</code></pre>

</figure>
<p><span>So, the first rule is to make most of the calls statically resolvable, to allow inlining.</span>
<span>Function pointers and dynamic dispatch prevent inlining.</span>
<span>Separate compilation might also get in a way of inlining, see this </span><a href="https://matklad.github.io/2021/07/09/inline-in-rust.html"><span>separate essay</span></a><span> on the topic.</span></p>
<p><span>Similarly, indirection in </span><em><span>memory</span></em><span> can cause troubles for the compiler.</span></p>
<p><span>For something like this</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">struct</span> <span class="hl-title class_">Foo</span> {</code>
<code>  bar: Bar,</code>
<code>  baz: Baz,</code>
<code>}</code></pre>

</figure>
<p><span>the </span><code>Foo</code><span> struct is completely transparent for the compiler.</span></p>
<p><span>While here:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">struct</span> <span class="hl-title class_">Foo</span> {</code>
<code>  bar: <span class="hl-type">Box</span>&lt;Bar&gt;,</code>
<code>  baz: Baz,</code>
<code>}</code></pre>

</figure>
<p><span>it is not clear cut.</span>
<span>Proving something about the memory occupied by </span><code>Foo</code><span> does not in general transfer to the memory occupied by </span><code>Bar</code><span>.</span>
<span>Again, in many cases a compiler </span><em><span>can</span></em><span> reason through boxes thanks to uniqueness, but this is not guaranteed.</span></p>
<p><span>A good homework at this point is to look at Rust</span>&rsquo;<span>s iterators and understand why they look the way they do.</span></p>
<p><span>Why the signature and definition of </span><a href="https://doc.rust-lang.org/stable/core/iter/trait.Iterator.html#method.map"><code>map</code></a><span> is</span></p>

<figure class="code-block">


<pre><code><span class="hl-meta">#[inline]</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">map</span>&lt;B, F&gt;(<span class="hl-keyword">self</span>, f: F) <span class="hl-punctuation">-&gt;</span> Map&lt;<span class="hl-keyword">Self</span>, F&gt;</code>
<code><span class="hl-keyword">where</span></code>
<code>  <span class="hl-keyword">Self</span>: <span class="hl-built_in">Sized</span>,</code>
<code>  F: <span class="hl-title function_ invoke__">FnMut</span>(<span class="hl-keyword">Self</span>::Item) <span class="hl-punctuation">-&gt;</span> B,</code>
<code>{</code>
<code>  Map::<span class="hl-title function_ invoke__">new</span>(<span class="hl-keyword">self</span>, f)</code>
<code>}</code></pre>

</figure>
<p><span>Another important point about memory is that, in general, a compiler can</span>&rsquo;<span>t change the overall layout of stuff.</span>
<span>SROA can load some data structure into a bunch of local variables, which then can, eg, replace </span>&ldquo;<span>a pointer and an index</span>&rdquo;<span> representation with </span>&ldquo;<span>a pair of pointers</span>&rdquo;<span>.</span>
<span>But at the end of the day SROA would have to materialize </span>&ldquo;<span>a pointer and an index</span>&rdquo;<span> back and store that representation back into the memory.</span>
<span>This is because memory layout is shared across all functions, so a function can not unilaterally dictate a more optimal representation.</span></p>
<p><span>Together, these observations give a basic rule for the baseline of performant code.</span></p>

<aside class="admn note">
<svg class="icon"><use href="/assets/icons.svg#info"/></svg>
<div><p><span>Think about data layout in memory.</span>
<span>A compiler is of very little help here and would mostly put the bytes where you tell it to.</span>
<span>Make data structures more compact, reduce indirection, exploit common access patterns for improving cache efficiency.</span></p>
<p><span>Compilers are much better at reasoning about the code, as long as they can see it.</span>
<span>Make sure that most calls are known at compile time and can be inlined, trust the compiler to do the rest.</span></p>
</div>
</aside></section>
<section id="SIMD">

    <h2>
    <a href="#SIMD"><span>SIMD</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s apply this general framework of giving a compiler optimizable code to work with to auto-vectorization.</span>
<span>We will be optimizing the function which computes the longest common prefix between two slices of bytes (thanks </span><a href="https://github.com/nkkarpov"><span>@nkkarpov</span></a><span> for the example).</span></p>
<p><span>A  direct implementation would look like this:</span></p>

<figure class="code-block">


<pre><code><span class="hl-keyword">use</span> std::iter::zip;</code>
<code></code>
<code><span class="hl-comment">// 650 milliseconds</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">common_prefix</span>(xs: &amp;[<span class="hl-type">u8</span>], ys: &amp;[<span class="hl-type">u8</span>]) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">usize</span> {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">result</span> = <span class="hl-number">0</span>;</code>
<code>  <span class="hl-title function_ invoke__">for</span> (x, y) <span class="hl-keyword">in</span> <span class="hl-title function_ invoke__">zip</span>(xs, ys) {</code>
<code>    <span class="hl-keyword">if</span> x != y { <span class="hl-keyword">break</span>; }</code>
<code>    result += <span class="hl-number">1</span></code>
<code>  }</code>
<code>  result</code>
<code>}</code></pre>

</figure>
<p><span>If you already have a mental model for auto-vectorization, or if you look at the assembly output, you can realize that the function as written works one byte at a time, which is much slower than it needs to be.</span>
<span>Let</span>&rsquo;<span>s fix that!</span></p>
<p><span>SIMD works on many values simultaneously.</span>
<span>Intuitively, we want the compiler to compare a bunch of bytes at the same time, but our current code does not express that.</span>
<span>Let</span>&rsquo;<span>s make the structure explicit, by processing 16 bytes at a time, and then handling remainder separately:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// 450 milliseconds</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">common_prefix</span>(xs: &amp;[<span class="hl-type">u8</span>], ys: &amp;[<span class="hl-type">u8</span>]) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">usize</span> {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">chunk_size</span> = <span class="hl-number">16</span>;</code>
<code></code>
<code>  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">result</span> = <span class="hl-number">0</span>;</code>
<code></code>
<code>  <span class="hl-symbol">&#x27;outer</span>: <span class="hl-title function_ invoke__">for</span> (xs_chunk, ys_chunk) <span class="hl-keyword">in</span></code>
<code>    <span class="hl-title function_ invoke__">zip</span>(xs.<span class="hl-title function_ invoke__">chunks_exact</span>(chunk_size), ys.<span class="hl-title function_ invoke__">chunks_exact</span>(chunk_size))</code>
<code>  {</code>
<code>    <span class="hl-title function_ invoke__">for</span> (x, y) <span class="hl-keyword">in</span> <span class="hl-title function_ invoke__">zip</span>(xs_chunk, ys_chunk) {</code>
<code>      <span class="hl-keyword">if</span> x != y { <span class="hl-keyword">break</span> <span class="hl-symbol">&#x27;outer</span>; }</code>
<code>      result += <span class="hl-number">1</span></code>
<code>    }</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-title function_ invoke__">for</span> (x, y) <span class="hl-keyword">in</span> <span class="hl-title function_ invoke__">zip</span>(&amp;xs[result..], &amp;ys[result..]) {</code>
<code>    <span class="hl-keyword">if</span> x != y { <span class="hl-keyword">break</span>; }</code>
<code>    result += <span class="hl-number">1</span></code>
<code>  }</code>
<code></code>
<code>  result</code>
<code>}</code></pre>

</figure>
<p><span>Amusingly, this is already a bit faster, but not quite there yet.</span>
<span>Specifically, SIMD needs to process all values in the chunk in parallel in the same way.</span>
<span>In our code above, we have a </span><code>break</code><span>, which means that processing of the nth pair of bytes depends on the n-1st pair.</span>
<span>Let</span>&rsquo;<span>s fix </span><em><span>that</span></em><span> by disabling short-circuiting.</span>
<span>We will check if the whole chunk of bytes matches or not, but we won</span>&rsquo;<span>t care which specific byte is a mismatch:</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// 80 milliseconds</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">common_prefix3</span>(xs: &amp;[<span class="hl-type">u8</span>], ys: &amp;[<span class="hl-type">u8</span>]) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">usize</span> {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">chunk_size</span> = <span class="hl-number">16</span>;</code>
<code></code>
<code>  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">result</span> = <span class="hl-number">0</span>;</code>
<code>  <span class="hl-title function_ invoke__">for</span> (xs_chunk, ys_chunk) <span class="hl-keyword">in</span></code>
<code>    <span class="hl-title function_ invoke__">zip</span>(xs.<span class="hl-title function_ invoke__">chunks_exact</span>(chunk_size), ys.<span class="hl-title function_ invoke__">chunks_exact</span>(chunk_size))</code>
<code>  {</code>
<code>    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">chunk_equal</span>: <span class="hl-type">bool</span> = <span class="hl-literal">true</span>;</code>
<code>    <span class="hl-title function_ invoke__">for</span> (x, y) <span class="hl-keyword">in</span> <span class="hl-title function_ invoke__">zip</span>(xs_chunk, ys_chunk) {</code>
<code>      <span class="hl-comment">// NB: &amp;, unlike &amp;&amp;, doesn&#x27;t short-circuit.</span></code>
<code>      chunk_equal = chunk_equal &amp; (x == y);</code>
<code>    }</code>
<code></code>
<code>    <span class="hl-keyword">if</span> !chunk_equal { <span class="hl-keyword">break</span>; }</code>
<code>    result += chunk_size;</code>
<code>  }</code>
<code></code>
<code>  <span class="hl-title function_ invoke__">for</span> (x, y) <span class="hl-keyword">in</span> <span class="hl-title function_ invoke__">zip</span>(&amp;xs[result..], &amp;ys[result..]) {</code>
<code>    <span class="hl-keyword">if</span> x != y { <span class="hl-keyword">break</span>; }</code>
<code>    result += <span class="hl-number">1</span></code>
<code>  }</code>
<code></code>
<code>  result</code>
<code>}</code></pre>

</figure>
<p><span>And this version finally lets vectorization kick in, reducing the runtime almost by an order of magnitude.</span>
<span>We can now compress this version using iterators.</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment">// 80 milliseconds</span></code>
<code><span class="hl-keyword">fn</span> <span class="hl-title function_">common_prefix5</span>(xs: &amp;[<span class="hl-type">u8</span>], ys: &amp;[<span class="hl-type">u8</span>]) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">usize</span> {</code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">chunk_size</span> = <span class="hl-number">16</span>;</code>
<code></code>
<code>  <span class="hl-keyword">let</span> <span class="hl-variable">off</span> =</code>
<code>    <span class="hl-title function_ invoke__">zip</span>(xs.<span class="hl-title function_ invoke__">chunks_exact</span>(chunk_size), ys.<span class="hl-title function_ invoke__">chunks_exact</span>(chunk_size))</code>
<code>      .<span class="hl-title function_ invoke__">take_while</span>(|(xs_chunk, ys_chunk)| xs_chunk == ys_chunk)</code>
<code>      .<span class="hl-title function_ invoke__">count</span>() * chunk_size;</code>
<code></code>
<code>  off + <span class="hl-title function_ invoke__">zip</span>(&amp;xs[off..], &amp;ys[off..])</code>
<code>    .<span class="hl-title function_ invoke__">take_while</span>(|(x, y)| x == y)</code>
<code>    .<span class="hl-title function_ invoke__">count</span>()</code>
<code>}</code></pre>

</figure>
<p><span>Note how the code is meaningfully different from our starting point.</span>
<span>We do not blindly rely on the compiler</span>&rsquo;<span>s optimization.</span>
<span>Rather, we are aware about specific optimizations we need in this case, and write the code in a way that triggers them.</span></p>
<p><span>Specifically, for SIMD:</span></p>
<ul>
<li>
<span>we express the algorithm in terms of processing </span><em><span>chunks</span></em><span> of elements,</span>
</li>
<li>
<span>within each chunk, we make sure that there</span>&rsquo;<span>s no branching and all elements are processed in the same way.</span>
</li>
</ul>
</section>
<section id="Conclusion">

    <h2>
    <a href="#Conclusion"><span>Conclusion</span> </a>
    </h2>
<p><span>Compilers are tools.</span>
<span>While there</span>&rsquo;<span>s a fair share of </span>&ldquo;<span>optimistic</span>&rdquo;<span> transformations which sometimes kick in, the bulk of the impact of an optimizing compiler comes from guaranteed optimizations with specific preconditions.</span>
<span>Compilers are myopic </span>&mdash;<span> they have a hard time reasoning about code outside of the current function and values not held in the local variables.</span>
<span>Inlining and scalar replacement of aggregates are two optimizations to remedy the situation.</span>
<span>Zero cost abstractions work by expressing opportunities for guaranteed optimizations in the language</span>&rsquo;<span>s type system.</span></p>
<p><span>If you like this post, I highly recommend </span><a href="https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf"><em><span>A Catalogue of Optimizing Transformations</span></em></a><span> by Frances Allen.</span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">UB Might Be a Wrong Term for Newer Languages</title>
<link href="https://matklad.github.io/2023/04/02/ub-might-be-the-wrong-term-for-newer-languages.html" rel="alternate" type="text/html" title="UB Might Be a Wrong Term for Newer Languages" />
<published>2023-04-02T00:00:00+00:00</published>
<updated>2023-04-02T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/04/02/ub-might-be-the-wrong-term-for-newer-languages</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[A short note on undefined behavior, which assumes familiarity with the subject (see this article for the introduction).
The TL;DR is that I think that carrying the wording from the C standard into newer languages, like Zig and Rust, might be a mistake.
This is strictly the word choice, the lexical syntax of the comments argument.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/04/02/ub-might-be-the-wrong-term-for-newer-languages.html"><![CDATA[
    <h1>
    <a href="#UB-Might-Be-a-Wrong-Term-for-Newer-Languages"><span>UB Might Be a Wrong Term for Newer Languages</span> <time datetime="2023-04-02">Apr 2, 2023</time></a>
    </h1>
<p><span>A short note on undefined behavior, which assumes familiarity with the subject (see </span><a href="https://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html"><span>this article</span></a><span> for the introduction).</span>
<span>The TL;DR is that I think that carrying the wording from the C standard into newer languages, like Zig and Rust, might be a mistake.</span>
<span>This is strictly the word choice, the </span>&ldquo;<span>lexical syntax of the comments</span>&rdquo;<span> argument.</span></p>
<p><span>The C standard leaves many behaviors undefined.</span>
<span>However, it allows any particular implementation to fill in the gaps and define some of undefined-in-the-standard behaviors.</span>
<span>For example, C23 makes </span><code>realloc(ptr, 0)</code><span> into an undefined behavior, so that POSIX can further refine it without interfering with the standard (</span><a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2464.pdf"><span>source</span></a><span>).</span></p>
<p><span>It</span>&rsquo;<span>s also valid for an implementation to leave UB undefined.</span>
<span>If a program compiled with this implementation hits this UB path, the behavior of the program </span><em><span>as a whole</span></em><span> is undefined</span>
<span>(or rather, bounded by the execution environment. It is not </span><em><span>actually</span></em><span> possible to summon nasal daemons, because a user-space process can not escape its memory space other than by calling syscalls, and there are no nasal daemons summoning syscalls).</span></p>
<p><span>C implementations are </span><em><span>not required to</span></em><span> but </span><em><span>may</span></em><span> define behaviors left undefined by the standard.</span>
<span>A C program written for a specific implementation may rely on undefined-in-the-standard but defined-in-the-implementation behavior.</span></p>
<p><span>Modern languages like </span><a href="https://doc.rust-lang.org/reference/behavior-considered-undefined.html"><span>Rust</span></a><span> and </span><a href="https://ziglang.org/documentation/0.10.1/#Undefined-Behavior"><span>Zig</span></a><span> re-use the </span>&ldquo;<span>undefined behavior</span>&rdquo;<span> term.</span>
<span>However, the intended semantics is subtly different.</span>
<span>A program exhibiting UB is </span><em><span>always</span></em><span> considered invalid.</span>
<span>Even if an alternative implementation of Rust defines some of Rust</span>&rsquo;<span>s UB, the programs hitting those behaviors would still be incorrect.</span></p>
<p><span>For this reason, I think it would be better to use a different term here.</span>
<span>I am not ready to suggest a specific wording, but a couple of reasonable options would be </span>&ldquo;<span>non-trapping programming error</span>&rdquo;<span> or </span>&ldquo;<span>invalid behavior</span>&rdquo;<span>.</span>
<span>The intended semantics being that any program execution containing illegal behavior is invalid under any implementation.</span></p>
<p><span>Curiously, C++ is ahead of the pack here, as it has an explicit notion of </span>&ldquo;<span>ill-formed, no diagnostic required</span>&rdquo;<span>.</span></p>
]]></content>
</entry>

<entry>
<title type="text">Rust Is a Scalable Language</title>
<link href="https://matklad.github.io/2023/03/28/rust-is-a-scalable-language.html" rel="alternate" type="text/html" title="Rust Is a Scalable Language" />
<published>2023-03-28T00:00:00+00:00</published>
<updated>2023-03-28T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/03/28/rust-is-a-scalable-language</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[In my last post about Zig and Rust, I mentioned that Rust is a scalable language.
Let me expand on this a bit.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/03/28/rust-is-a-scalable-language.html"><![CDATA[
    <h1>
    <a href="#Rust-Is-a-Scalable-Language"><span>Rust Is a Scalable Language</span> <time datetime="2023-03-28">Mar 28, 2023</time></a>
    </h1>
<p><span>In my last post about </span><a href="https://matklad.github.io/2023/03/26/zig-and-rust.html"><em><span>Zig and Rust</span></em></a><span>, I mentioned that Rust is a </span>&ldquo;<span>scalable language</span>&rdquo;<span>.</span>
<span>Let me expand on this a bit.</span></p>
<section id="Vertical-Scalability">

    <h2>
    <a href="#Vertical-Scalability"><span>Vertical Scalability</span> </a>
    </h2>
<p><span>Rust is vertically scalable, in that you can write all kinds of software in it.</span>
<span>You can write an advanced zero-alloc image compression library, build a web server exposing the library to the world as an HTTP SAAS, and cobble together a </span>&ldquo;<span>script</span>&rdquo;<span> for building, testing, and deploying it to wherever people deploy software these days.</span>
<span>And you would only need Rust </span>&mdash;<span> while it excels in the lowest half of the stack, it</span>&rsquo;<span>s pretty ok everywhere else too.</span></p>
</section>
<section id="Horizontal-Scalability">

    <h2>
    <a href="#Horizontal-Scalability"><span>Horizontal Scalability</span> </a>
    </h2>
<p><span>Rust is horizontally scalable, in that you can easily parallelize development of large software artifacts across many people and teams.</span>
<span>Rust itself moves with a breakneck speed, which is surprising for such a loosely coordinated and chronically understaffed open source project of this scale.</span>
<span>The relatively small community  managed to put together a comprehensive ecosystem of composable high-quality crates on a short notice.</span>
<span>Rust is so easy to compose reliably that even the stdlib itself does not shy from pulling dependencies from crates.io.</span></p>
<p><span>Steve Klabnik wrote about </span><a href="https://steveklabnik.com/writing/rusts-golden-rule"><em><span>Rust</span>&rsquo;<span>s Golden Rule</span></em></a><span>,</span>
<span>how function signatures are mandatory and authoritative and explicitly define the interface both for the callers of the function and for the function</span>&rsquo;<span>s body.</span>
<span>This thinking extends to other parts of the language.</span></p>
<p><span>My second most favorite feature of Rust (after safety) is its module system.</span>
<span>It has first-class support for the concept of a library.</span>
<span>A library is called a crate and is a tree of modules, a unit of compilation, and a principle visibility boundary.</span>
<span>Modules can contain circular dependencies, but libraries always form a directed acyclic graph.</span>
<span>There</span>&rsquo;<span>s no global namespace of symbols </span>&mdash;<span> libraries are anonymous, names only appear on dependency edges between two libraries, and are local to the downstream crate.</span></p>
<p><span>The benefits of this core compilation model are then greatly amplified by Cargo, which is not a generalized task runner, but rather a rigid specification for what is a package of Rust code:</span></p>
<ul>
<li>
<span>a (library) crate,</span>
</li>
<li>
<span>a manifest, which defines dependencies between packages in a declarative way, using semver,</span>
</li>
<li>
<span>an ecosystem-wide agreement on the semantics of dependency specification, and accompanied dependency resolution algorithm.</span>
</li>
</ul>
<p><span>Crucially, there</span>&rsquo;<span>s absolutely no way in Cargo to control the actual build process.</span>
<span>The </span><code>build.rs</code><span> file can be used to provide extra runtime inputs, but it</span>&rsquo;<span>s </span><code>cargo</code><span> who calls </span><code>rustc</code><span>.</span></p>
<p><span>Again, Cargo defines a rigid interface for a reusable piece of Rust code.</span>
<span>Both producers and consumers must abide by these rules, there is no way around them.</span>
<span>As a reward, they get a super-power of working together by working apart.</span>
<span>I don</span>&rsquo;<span>t need to ping dtolnay in Slack when I want to use serde-json because we implicitly pre-agreed to a shared golden rule.</span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">Zig And Rust</title>
<link href="https://matklad.github.io/2023/03/26/zig-and-rust.html" rel="alternate" type="text/html" title="Zig And Rust" />
<published>2023-03-26T00:00:00+00:00</published>
<updated>2023-03-26T00:00:00+00:00</updated>
<id>https://matklad.github.io/2023/03/26/zig-and-rust</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[This post will be a bit all over the place.
Several months ago, I wrote Hard Mode Rust, exploring an allocation-conscious style of programming.
In the ensuing discussion, @jamii name-dropped TigerBeetle, a reliable, distributed, fast, and small database written in Zig in a similar style, and, well, I now find myself writing Zig full-time, after more than seven years of Rust.
This post is a hand-wavy answer to the why? question.
It is emphatically not a balanced and thorough comparison of the two languages.
I haven't yet written my 100k lines of Zig to do that.
(if you are looking for a more general what the heck is Zig, I can recommend @jamii's post).
In fact, this post is going to be less about languages, and more about styles of writing software (but pre-existing knowledge of Rust and Zig would be very helpful).
Without further caveats, let's get started.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2023/03/26/zig-and-rust.html"><![CDATA[
    <h1>
    <a href="#Zig-And-Rust"><span>Zig And Rust</span> <time datetime="2023-03-26">Mar 26, 2023</time></a>
    </h1>
<p><span>This post will be a bit all over the place.</span>
<span>Several months ago, I wrote </span><a href="https://matklad.github.io/2022/10/06/hard-mode-rust.html"><em><span>Hard Mode Rust</span></em></a><span>, exploring an allocation-conscious style of programming.</span>
<span>In the ensuing discussion, </span><a href="https://github.com/Jamii"><span>@jamii</span></a><span> name-dropped </span><a href="https://tigerbeetle.com"><span>TigerBeetle</span></a><span>, a reliable, distributed, fast, and small database written in Zig in a similar style, and, well, I now find myself writing Zig full-time, after more than seven years of Rust.</span>
<span>This post is a hand-wavy answer to the </span>&ldquo;<span>why?</span>&rdquo;<span> question.</span>
<span>It is emphatically </span><em><span>not</span></em><span> a balanced and thorough comparison of the two languages.</span>
<span>I haven</span>&rsquo;<span>t yet written my </span><a href="https://matklad.github.io/2021/09/05/Rust100k.html"><span>100k lines of Zig</span></a><span> to do that.</span>
<span>(if you are looking for a more general </span>&ldquo;<span>what the heck is Zig</span>&rdquo;<span>, I can recommend </span><a href="https://www.scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/"><span>@jamii</span>&rsquo;<span>s post</span></a><span>).</span>
<span>In fact, this post is going to be less about languages, and more about styles of writing software (but pre-existing knowledge of Rust and Zig would be very helpful).</span>
<span>Without further caveats, let</span>&rsquo;<span>s get started.</span></p>
<section id="Reliable-Software">

    <h2>
    <a href="#Reliable-Software"><span>Reliable Software</span> </a>
    </h2>
<p><span>To the first approximation, we all strive to write bug-free programs.</span>
<span>But I think a closer look reveals that we don</span>&rsquo;<span>t actually care about programs being correct 100% of the time, at least in the majority of the domains.</span>
<span>Empirically, almost every program has bugs, and yet it somehow works out OK.</span>
<span>To pick one specific example, most programs use stack, but almost no programs understand what their stack usage is exactly, and how far they can go.</span>
<span>When we call </span><code>malloc</code><span>, we just hope that we have enough stack space for it, we almost never check.</span>
<span>Similarly, all Rust programs abort on OOM, and can</span>&rsquo;<span>t state their memory requirements up-front.</span>
<span>Certainly good enough, but not perfect.</span></p>
<p><span>The second approximation is that we strive to balance program usefulness with the effort to develop the program.</span>
<span>Bugs reduce usefulness a lot, and there are two styles of software engineering to deal with the:</span></p>
<p><em><span>Erlang style</span></em><span>, where we embrace failability of both hardware and software and explicitly design programs to be resilient to partial faults.</span></p>
<p><a href="https://www.sqlite.org/testing.html"><em><span>SQLite style</span></em></a><span>, where we overcome an unreliable environment at the cost of rigorous engineering.</span></p>
<p><span>rust-analyzer and TigerBeetle are perfect specimens of the two approaches, let me describe them.</span></p>
</section>
<section id="rust-analyzer">

    <h2>
    <a href="#rust-analyzer"><span>rust-analyzer</span> </a>
    </h2>
<p><a href="https://rust-analyzer.github.io"><span>rust-analyzer</span></a><span> is an LSP server for the Rust programming language.</span>
<span>By its nature, it</span>&rsquo;<span>s expansive.</span>
<span>Great developer tools usually have a feature for every niche use-case.</span>
<span>It also is a fast-moving open source project which has to play catch-up with the </span><code>rustc</code><span> compiler.</span>
<span>Finally, the nature of IDE dev tooling makes availability significantly more important than correctness.</span>
<span>An erroneous completion option would cause a smirk (if it is noticed at all), while the server crashing and all syntax highlighting turning off will be noticed immediately.</span></p>
<p><span>For this cluster of reasons, rust-analyzer is shifted far towards the </span>&ldquo;<span>embrace software imperfections</span>&rdquo;<span> side of the spectrum.</span>
<span>rust-analyzer is designed around having bugs.</span>
<span>All the various features are carefully compartmentalized at runtime, such that panicking code in just a single feature can</span>&rsquo;<span>t bring down the whole process.</span>
<span>Critically, almost no code has access to any mutable state, so usage of </span><code>catch_unwind</code><span> can</span>&rsquo;<span>t lead to a rotten state.</span></p>
<p><span>Development process </span><em><span>itself</span></em><span> is informed by this calculus.</span>
<span>For example, PRs with new features land when there</span>&rsquo;<span>s a reasonable certainty that the happy case works correctly.</span>
<span>If some weird incomplete code would cause the feature to crash, that</span>&rsquo;<span>s OK.</span>
<span>It might be even a benefit </span>&mdash;<span> fixing a well-reproducible bug in an isolated feature is a gateway drug to heavy contribution to rust-analyzer.</span>
<span>Our tight weekly release schedule (and the nightly release) help to get bug fixes out there faster.</span></p>
<p><span>Overall, the philosophy is to maximize provided value by focusing on the common case.</span>
<span>Edge cases become eventually correct over time.</span></p>
</section>
<section id="TigerBeetle">

    <h2>
    <a href="#TigerBeetle"><span>TigerBeetle</span> </a>
    </h2>
<p><span>TigerBeetle is the opposite of that.</span></p>
<p><span>It is a database, with domain model fixed at compile time (we currently do double-entry bookkeeping).</span>
<span>The database is distributed, meaning that there are six TigerBeetle replicas running on different geographically and operationally isolated machines, which together implement a replicated state machine.</span>
<span>That is, TigerBeetle replicas exchange messages to make sure every replica processes the same set of transactions, in the same order.</span>
<span>That</span>&rsquo;<span>s a surprisingly hard problem if you allow machines to fail (the whole point of using many machines for redundancy), so we use a smart </span><a href="https://pmg.csail.mit.edu/papers/vr-revisited.pdf"><span>consensus algorithm</span></a><span>  (non-byzantine) for this.</span>
<span>Traditionally, consensus algorithms assume reliable storage </span>&mdash;<span> data once written to disk can be always retrieved later.</span>
<span>In reality, storage is unreliable, nearly byzantine </span>&mdash;<span> a disk can return bogus data without signaling an error, and even a single such error can </span><a href="https://www.usenix.org/conference/fast18/presentation/alagappan"><span>break consensus</span></a><span>.</span>
<span>TigerBeetle combats that by allowing a replica to repair its local storage using data from other replicas.</span></p>
<p><span>On the engineering side of things, we are building a reliable, predictable system.</span>
<span>And predictable means </span><em><span>really</span></em><span> predictable.</span>
<span>Rather than reining in sources of non-determinism, we build the whole system from the ground up from a set of fully deterministic, hand crafted components.</span>
<span>Here are some of our unconventional choices (</span><a href="https://github.com/tigerbeetledb/tigerbeetle/blob/fe09404d465df46b2bdfc017633eff37b4ab2343/docs/DESIGN.md"><span>design doc</span></a><span>):</span></p>
<p><span>It</span>&rsquo;<span>s </span><a href="https://matklad.github.io/2022/10/06/hard-mode-rust.html"><span>hard mode</span></a><span>!</span>
<span>We allocate all the memory at a startup, and there</span>&rsquo;<span>s zero allocation after that.</span>
<span>This removes all the uncertainty about allocation.</span></p>
<p><span>The code is architected with brutal simplicity.</span>
<span>As a single example, we don</span>&rsquo;<span>t use JSON, or ProtoBuf, or Cap</span>&rsquo;<span>n</span>&rsquo;<span>Proto for serialization.</span>
<span>Rather, we just cast the bytes we received from the network to a desired type.</span>
<span>The motivation here is not so much performance, as reduction of the number of moving parts.</span>
<span>Parsing is hard, but, if you control both sides of the communication channel, you don</span>&rsquo;<span>t need to do it, you can send checksummed data as is.</span></p>
<p><span>We aggressively minimize all dependencies.</span>
<span>We know exactly the system calls our system is making, because all IO is our own code (on Linux, our main production platform, we don</span>&rsquo;<span>t link libc).</span></p>
<p><span>There</span>&rsquo;<span>s little abstraction between components </span>&mdash;<span> all parts of TigerBeetle work in concert.</span>
<span>For example, one of our core types, </span><a href="https://github.com/tigerbeetledb/tigerbeetle/blob/fe09404d465df46b2bdfc017633eff37b4ab2343/src/message_pool.zig#L64"><code>Message</code></a><span>, is used throughout the stack:</span></p>
<ul>
<li>
<span>network receives bytes from a TCP connection directly into a </span><code>Message</code>
</li>
<li>
<span>consensus processes and sends </span><code>Message</code><span>s</span>
</li>
<li>
<span>similarly, storage writes </span><code>Message</code><span>s to disk</span>
</li>
</ul>
<p><span>This naturally leads to very simple and fast code.</span>
<span>We don</span>&rsquo;<span>t need to do anything special to be zero copy </span>&mdash;<span> given that we allocate everything up-front, we simply don</span>&rsquo;<span>t have any extra memory to copy the data to!</span>
<span>(A separate issue is that, arguably, you just can</span>&rsquo;<span>t treat storage as a separate black box in a fault-tolerant distributed system, because storage is also faulty).</span></p>
<p><em><span>Everything</span></em><span> in TigerBeetle has an explicit upper-bound.</span>
<span>There</span>&rsquo;<span>s not a thing which is </span><em><span>just</span></em><span> an </span><code>u32</code><span> </span>&mdash;<span> all data is checked to meet specific numeric limits at the edges of the system.</span></p>
<p><span>This includes </span><code>Message</code><span>s.</span>
<span>We just upper-bound how many messages can be in-memory at the same time, and allocate precisely that amount of messages (</span><a href="https://github.com/tigerbeetledb/tigerbeetle/blob/53092098d69cc8facf94a2472bc79ca9d525a605/src/message_pool.zig#L16-L40"><span>source</span></a><span>).</span>
<span>Getting a new message from the message pool can</span>&rsquo;<span>t allocate and can</span>&rsquo;<span>t fail.</span></p>
<p><span>With all that strictness and explicitness about resources, of course we also fully externalize any IO, including time.</span>
<em><span>All</span></em><span> inputs are passed in explicitly, there</span>&rsquo;<span>s no ambient influences from the environment.</span>
<span>And that means that the bulk of our testing consists of trying all possible permutations of effects of the environment.</span>
<span>Deterministic randomized simulation is </span><a href="https://dl.acm.org/doi/10.1145/3158134"><span>very effective</span></a><span> at uncovering issues in real implementations of distributed systems.</span></p>
<p><span>What I am getting at is that TigerBeetle isn</span>&rsquo;<span>t really a normal </span>&ldquo;<span>program</span>&rdquo;<span> program.</span>
<span>It strictly is a finite state machine, explicitly coded as such.</span></p>
</section>
<section id="Back-From-The-Weeds">

    <h2>
    <a href="#Back-From-The-Weeds"><span>Back From The Weeds</span> </a>
    </h2>
<p><span>Oh, right, Rust and Zig, the topic of the post!</span></p>
<p><span>I find myself often returning to </span><a href="http://venge.net/graydon/talks/intro-talk.pdf"><span>the first Rust slide deck</span></a><span>.</span>
<span>A lot of core things are different (no longer Rust uses only the old ideas), but a lot is the same.</span>
<span>To be a bit snarky, while Rust </span>&ldquo;<span>is not for lone genius hackers</span>&rdquo;<span>, Zig </span>&hellip;<span> kinda is.</span>
<span>On more peaceable terms, while Rust is a language for building </span><em><span>modular</span></em><span> software, Zig is in some sense anti-modular.</span></p>
<p><span>It</span>&rsquo;<span>s appropriate to quote </span><a href="https://youtu.be/HgtRAbE1nBM?t=2359"><span>Bryan Cantrill</span></a><span> here:</span></p>

<figure class="blockquote">
<blockquote><p><span>I can write C that frees memory properly</span>&hellip;<span>that basically doesn</span>&rsquo;<span>t suffer from</span>
<span>memory corruption</span>&hellip;<span>I can do that, because I</span>&rsquo;<span>m controlling heaven and earth in</span>
<span>my software. It makes it very hard to compose software. Because even if you and</span>
<span>I both know how to write memory safe C, it</span>&rsquo;<span>s very hard for us to have an</span>
<span>interface boundary where we can agree about who does what.</span></p>
</blockquote>

</figure>
<p><span>That</span>&rsquo;<span>s the core of what Rust is doing: it provides you with a language to precisely express the contracts between components, such that components can be integrated in a machine-checkable way.</span></p>
<p><span>Zig doesn</span>&rsquo;<span>t do that. It isn</span>&rsquo;<span>t even memory safe. My first experience writing a non-trivial Zig program went like this:</span></p>

<figure class="blockquote">
<blockquote><p><span>ME: Oh wow! Do you mean I can finally </span><em><span>just</span></em><span> store a pointer to a struct</span>&rsquo;<span>s field in the struct itself?</span></p>
<p><span>30 seconds later</span></p>
<p><span>PROGRAM: Segmentation fault.</span></p>
</blockquote>

</figure>
<p><span>However!</span><br>
<span>Zig </span><em><span>is</span></em><span> a much smaller language than Rust.</span>
<span>Although you</span>&rsquo;<span>ll </span><em><span>have</span></em><span> to be able to keep the entirety of the program in your head, to control heaven and earth to not mess up resource management, doing that could be easier.</span></p>
<p><span>It</span>&rsquo;<span>s not true that rewriting a Rust program in Zig would make it simpler.</span>
<span>On the contrary, I expect the result to be significantly more complex (and segfaulty).</span>
<span>I noticed that a lot of Zig code written in </span>&ldquo;<span>let</span>&rsquo;<span>s replace </span><a href="https://doc.rust-lang.org/rust-by-example/scope/raii.html"><span>RAII</span></a><span> with </span><a href="https://ziglang.org/documentation/master/#defer"><span>defer</span></a>&rdquo;<span> style has resource-management bugs.</span></p>
<p><span>But it often is possible to architect the software such that there</span>&rsquo;<span>s little resource management to do (eg, allocating everything up-front, like TigerBeetle, or even at compile time, like many smaller embedded systems).</span>
<span>It</span>&rsquo;<span>s hard </span>&mdash;<span> simplicity is always hard.</span>
<span>But, if you go this  way, I feel like Zig can provide substantial benefits.</span></p>
<p><span>Zig has just a single feature, dynamically-typed comptime, which subsumes most of the special-cased Rust machinery.</span>
<span>It is definitely a tradeoff, instantiation-time errors are much worse for complex cases.</span>
<span>But a lot more of the cases are simple, because there</span>&rsquo;<span>s no need for programming in the language of types.</span>
<span>Zig is very spartan when it comes to the language.</span>
<span>There are no closures </span>&mdash;<span> if you want them, you</span>&rsquo;<span>ll have to pack a wide-pointer yourself.</span>
<span>Zig</span>&rsquo;<span>s expressiveness is aimed at producing just the right assembly, not at allowing maximally concise and abstract source code.</span>
<span>In the words of Andrew Kelley, Zig is a DSL for emitting machine code.</span></p>
<p><span>Zig strongly prefers explicit resource management.</span>
<span>A lot of Rust programs are web-servers.</span>
<span>Most web servers have a very specific execution pattern of processing multiple independent short-lived requests concurrently.</span>
<span>The most natural way to code this would be to give each request a dedicated bump allocator, which turns drops into no-ops and </span>&ldquo;<span>frees</span>&rdquo;<span> the memory at bulk after each request by resetting offset to zero.</span>
<span>This would be pretty efficient, and would provide per-request memory profiling and limiting out of the box.</span>
<span>I don</span>&rsquo;<span>t think any popular Rust frameworks do this </span>&mdash;<span> using the global allocator is convenient enough and creates a strong local optima.</span>
<span>Zig forces you to pass the allocator in, so you might as well think about the most appropriate one!</span></p>
<p><span>Similarly, the standard library is very conscious about allocation, more so than Rust</span>&rsquo;<span>s.</span>
<span>Collections are </span><em><span>not</span></em><span> parametrized by an allocator, like in C++ or (future) Rust.</span>
<span>Rather, an allocator is passed in explicitly to every method which actually needs to allocate.</span>
<span>This is </span><a href="https://matklad.github.io/2020/12/28/csdi.html"><em><span>Call Site Dependency Injection</span></em></a><span>, and it is more flexible.</span>
<span>For example in TigerBeetle we need a couple of hash maps.</span>
<span>These maps are sized at a startup time to hold just the right number of elements, and are never resized.</span>
<span>So we pass an allocator to </span><a href="https://github.com/tigerbeetledb/tigerbeetle/blob/53092098d69cc8facf94a2472bc79ca9d525a605/src/vsr/replica.zig#L540"><code>init</code></a><span> method, but we don</span>&rsquo;<span>t pass it to the </span><a href="https://github.com/tigerbeetledb/tigerbeetle/blob/53092098d69cc8facf94a2472bc79ca9d525a605/src/vsr/replica.zig#L758"><span>event loop</span></a><span>.</span>
<span>We get to both use the standard hash-map, and to feel confident that there</span>&rsquo;<span>s no way we can allocate in the actual event loop, because it doesn</span>&rsquo;<span>t have access to an allocator.</span></p>
</section>
<section id="Wishlist">

    <h2>
    <a href="#Wishlist"><span>Wishlist</span> </a>
    </h2>
<p><span>Finally, my wishlist for Zig.</span></p>
<p><em><span>First</span></em><span>, I think Zig</span>&rsquo;<span>s strength lies strictly in the realm of writing </span>&ldquo;<span>perfect</span>&rdquo;<span> systems software.</span>
<span>It is a relatively thin slice of the market, but it is important.</span>
<span>One of the problems with Rust is that we don</span>&rsquo;<span>t have a reliability-oriented high-level programming language with a good quality of implementation (modern ML, if you will).</span>
<span>This is a blessing for Rust, because it makes its niche bigger, increasing the amount of community momentum behind the language.</span>
<span>This is also a curse, because a bigger niche makes it harder to maintain focus.</span>
<span>For Zig, Rust already plays this role of </span>&ldquo;<span>modern ML</span>&rdquo;<span>, which creates bigger pressure to specialize.</span></p>
<p><em><span>Second</span></em><span>, my biggest worry about Zig is its semantics around aliasing, provenance, mutability and self-reference ball of problems.</span>
<span>I don</span>&rsquo;<span>t worry all that much about this creating </span>&ldquo;<span>iterator invalidation</span>&rdquo;<span> style of UB.</span>
<span>TigerBeetle runs in </span><code>-DReleaseSafe</code><span>, which mostly solves spatial memory safety, it doesn</span>&rsquo;<span>t really do dynamic memory allocation, which unasks the question about temporal memory safety,</span>
<span>and it has a very thorough fuzzer-driven test suite, which squashes the remaining bugs.</span>
<span>I do worry about the semantics of the language itself.</span>
<span>My current understanding is that, to correctly compile a C-like low-level language, one really needs to nail down semantics of pointers.</span>
<span>I am not sure </span>&ldquo;<span>portable assembly</span>&rdquo;<span> is really a thing: it is possible to create a compiler which does little optimization and </span>&ldquo;<span>works as expected</span>&rdquo;<span> most of the time, but I am doubtful that it</span>&rsquo;<span>s possible to correctly describe the behavior of such a compiler.</span>
<span>If you start asking questions about what are pointers, and what is memory, you end up in a fairly complicated land, where bytes are poison.</span>
<span>Rust tries to define that precisely, but writing code which abides by the Rust rules without a borrow-checker isn</span>&rsquo;<span>t really possible </span>&mdash;<span> the rules are too subtle.</span>
<span>Zig</span>&rsquo;<span>s implementation today is </span><em><span>very</span></em><span> fuzzy around potentially aliased pointers, copies of structs with interior-pointers and the like.</span>
<span>I wish that Zig had a clear answer to what the desired semantics is.</span></p>
<p><span id="ide"><em><span>Third</span></em></span><span>, IDE support.</span>
<span>I</span>&rsquo;<span>ve written about that before </span><a href="https://matklad.github.io/2023/02/10/how-a-zig-ide-could-work.html"><span>on this blog</span></a><span>.</span>
<span>As of today, developing Zig is quite pleasant </span>&mdash;<span> </span><a href="https://github.com/zigtools/zls"><span>the language server</span></a><span> is pretty spartan, but already is quite helpful, and for the rest, Zig is exceptionally greppable.</span>
<span>But, with the lazy compilation model and the absence of out-of-the-language meta programming, I feel like Zig could be more ambitious here.</span>
<span>To position itself well for the future in terms of IDE support, I think it would be nice if the compiler gets the basic data model for IDE use-case.</span>
<span>That is, there should be an API to create a persistent analyzer process, which ingests a stream of code edits, and produces a continuously updated model of the code without explicit compilation requests.</span>
<span>The model can be very simple, just </span>&ldquo;<span>give me an AST of this file at this point in time</span>&rdquo;<span> would do </span>&mdash;<span> all the fancy IDE features can be filled in later.</span>
<span>What matters is a shape of data flow through the compiler </span>&mdash;<span> not an edit-compile cycle, but rather a continuously updated view of the world.</span></p>
<p><em><span>Fourth</span></em><span>, one of the values of Zig which resonates with me a lot is a preference for low-dependency, self-contained processes.</span>
<span>Ideally, you get yourself a </span><code>./zig</code><span> binary, and go from there.</span>
<span>The preference, at this time of changes, is to bundle a particular version of </span><code>./zig</code><span> with a project, instead of using a system-wide </span><code>zig</code><span>.</span>
<span>There are two aspects that could be better.</span></p>
<p>&ldquo;<span>Getting yourself a Zig</span>&rdquo;<span> is a finicky problem, because it requires bootstrapping.</span>
<span>To do that, you need to run some code that will download the binary for your platform, but each platform has its own way to </span>&ldquo;<span>run code</span>&rdquo;<span>.</span>
<span>I wish that Zig provided a blessed set of scripts, </span><code>get_zig.sh</code><span>, </span><code>get_zig.bat</code><span>, etc (or maybe a small actually portable binary?), which projects could just vendor, so that the contribution experience becomes fully project-local and self-contained:</span></p>

<figure class="code-block">


<pre><code><span class="hl-title function_">$</span> ./get_zig.sh</code>
<code><span class="hl-title function_">$</span> ./zig build</code></pre>

</figure>
<p><span>Once you have </span><code>./zig</code><span>, you can use that to drive the </span><em><span>rest</span></em><span> of the automation.</span>
<span>You already can </span><code>./zig build</code><span> to drive the build, but there</span>&rsquo;<span>s more to software than just building.</span>
<span>There</span>&rsquo;<span>s always a long tail of small things which traditionally get solved with a pile of platform-dependent bash scripts.</span>
<span>I wish that Zig pushed the users harder towards specifying all that automation in Zig.</span>
<span>A picture is worth a thousand words, so</span></p>

<figure class="code-block">


<pre><code><span class="hl-comment"># BAD: dependency on the OS</span></code>
<code><span class="hl-title function_">$</span> ./scripts/deploy.sh --port 92</code>
<code><span class="hl-output"></span></code>
<code><span class="hl-comment"># OK: no dependency, but a mouthful to type</span></code>
<code><span class="hl-title function_">$</span> ./zig build task -- deploy --port 92</code>
<code><span class="hl-output"></span></code>
<code><span class="hl-comment"># Would be GREAT:</span></code>
<code><span class="hl-title function_">$</span> ./zig do deploy --port 92</code></pre>

</figure>
<p><span>Attempting to summarize,</span></p>
<ul>
<li>
<span>Rust is about compositional safety, it</span>&rsquo;<span>s a more scalable language than Scala.</span>
</li>
<li>
<span>Zig is about perfection.</span>
<span>It is a very sharp, dangerous, but, ultimately, more flexible tool.</span>
</li>
</ul>
<p><span>Discussion on </span><a href="https://old.reddit.com/r/Zig/comments/123jpia/blog_post_zig_and_rust/"><span>/r/Zig</span></a><span> and </span><a href="https://old.reddit.com/r/rust/comments/123jpry/blog_post_zig_and_rust/"><span>/r/rust</span></a><span>.</span></p>
</section>
]]></content>
</entry>

</feed>
